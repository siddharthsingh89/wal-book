<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title></title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-KVDQ13RXMY"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-KVDQ13RXMY');
</script>
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="preface"><a class="header" href="#preface">Preface</a></h1>
<p>Welcome to this book on <strong>Write-Ahead Logging (WAL)</strong> - a foundational concept in databases and modern storage systems.</p>
<p>Whether you are a software engineer, a database enthusiast, or a systems developer, understanding WAL is crucial for building <strong>robust, fault-tolerant, and high-performance systems</strong>.</p>
<hr />
<h2 id="why-this-book-exists"><a class="header" href="#why-this-book-exists">Why This Book Exists</a></h2>
<p>Modern applications generate massive volumes of data. With this comes the responsibility to <strong>store, modify, and retrieve it reliably</strong>, even in the face of unexpected crashes, power failures, or hardware errors.</p>
<p>Write-Ahead Logging is the mechanism that underpins <strong>durability</strong> in many of the world’s most widely used databases - from PostgreSQL and SQLite to RocksDB and LMDB. Despite its ubiquity, WAL is often glossed over in textbooks and online tutorials.</p>
<p>This book aims to <strong>demystify WAL</strong>:</p>
<ul>
<li>Explain its <strong>concepts and principles</strong> in a clear, step-by-step manner.</li>
<li>Show how it is <strong>implemented in real-world databases</strong>.</li>
<li>Provide <strong>hands-on examples</strong>, including a mini WAL implemented in Rust.</li>
<li>Explore <strong>advanced topics</strong>, like concurrency, recovery, and distributed systems.</li>
</ul>
<hr />
<h2 id="who-this-book-is-for"><a class="header" href="#who-this-book-is-for">Who This Book Is For</a></h2>
<p>This book is intended for:</p>
<ul>
<li><strong>Software engineers</strong> building systems where data integrity matters.</li>
<li><strong>Database enthusiasts</strong> curious about how durability is enforced.</li>
<li><strong>Students and learners</strong> looking to understand storage engines in depth.</li>
<li><strong>Rust developers</strong> interested in implementing low-level systems safely and efficiently.</li>
</ul>
<p>Prior knowledge of <strong>basic programming concepts</strong> is assumed. Familiarity with <strong>Rust</strong> will help with the hands-on sections, but the conceptual chapters are language-agnostic.</p>
<hr />
<h2 id="how-this-book-is-structured"><a class="header" href="#how-this-book-is-structured">How This Book Is Structured</a></h2>
<p>The book is divided into five parts:</p>
<ol>
<li>
<p><strong>Foundations of WAL</strong><br />
Introduces durability, the core principles of WAL, and the architecture behind it.</p>
</li>
<li>
<p><strong>Implementing WAL</strong><br />
Guides you through designing, building, and testing a minimal WAL, including crash recovery and concurrency considerations.</p>
</li>
<li>
<p><strong>WAL in Real Databases</strong><br />
Examines how PostgreSQL, SQLite, InnoDB, RocksDB, and LMDB implement WAL.</p>
</li>
<li>
<p><strong>WAL in Distributed Systems</strong><br />
Explores replication, consensus protocols like Raft, and how WAL integrates into distributed databases.</p>
</li>
<li>
<p><strong>Advanced Topics</strong><br />
Covers performance optimizations, debugging strategies, and insights into modern storage engines.</p>
</li>
</ol>
<hr />
<h2 id="how-to-use-this-book"><a class="header" href="#how-to-use-this-book">How to Use This Book</a></h2>
<ul>
<li><strong>Read sequentially:</strong> Each chapter builds on the previous.</li>
<li><strong>Try the examples:</strong> Code snippets and projects are included to reinforce learning.</li>
<li><strong>Experiment:</strong> The Rust project included allows you to tinker with WAL yourself.</li>
<li><strong>Reference:</strong> The later chapters can be used as a reference for WAL implementations in production databases.</li>
</ul>
<p>By the end of this book, you will have a <strong>strong conceptual understanding</strong> of WAL and practical experience in implementing it, giving you the confidence to reason about <strong>durability, recovery, and consistency</strong> in any database or storage system.</p>
<hr />
<h2 id="final-thoughts"><a class="header" href="#final-thoughts">Final Thoughts</a></h2>
<p>Durability is often invisible. You don’t notice it until it fails. By understanding WAL, you gain the knowledge to <strong>design systems that are resilient, reliable, and performant</strong>, ensuring that your data survives failures gracefully.</p>
<p>This book is your guide to unlocking the principles and practices behind one of the most important mechanisms in modern data systems. Let’s begin our journey into the world of Write-Ahead Logging.</p>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="part-1--foundations-of-write-ahead-logging"><a class="header" href="#part-1--foundations-of-write-ahead-logging">Part 1:  Foundations of Write-Ahead Logging</a></h1>
<p>Modern databases are built on one simple promise - <strong>your data should never be lost</strong>. Whether a system crashes, power fails, or a process is interrupted midway, users expect their data to be safe and recoverable. The journey to understanding this promise begins with the concept of <strong>durability</strong>, one of the most critical principles in data systems.</p>
<p>This part explores how databases achieve durability through <strong>Write-Ahead Logging (WAL)</strong> - a technique that ensures no change is ever lost, even in the face of failure. Before diving into implementation details, we’ll first build a solid conceptual foundation.</p>
<hr />
<h3 id="chapter-1-introduction-to-data-durability"><a class="header" href="#chapter-1-introduction-to-data-durability">Chapter 1: Introduction to Data Durability</a></h3>
<p>We start by understanding what <em>durability</em> really means - both in everyday systems and in the world of storage and databases. You’ll explore how applications rely on durable storage to preserve critical information and why this concept is the cornerstone of reliable systems.</p>
<h3 id="chapter-2-the-core-idea-of-write-ahead-logging"><a class="header" href="#chapter-2-the-core-idea-of-write-ahead-logging">Chapter 2: The Core Idea of Write-Ahead Logging</a></h3>
<p>Next, we uncover the key insight behind WAL: <strong>“Always write changes to the log before applying them to the database.”</strong>
This simple rule transforms how systems handle crashes, recovery, and consistency. You’ll see how WAL balances speed, safety, and correctness in data systems.</p>
<h3 id="chapter-3-wal-architecture-and-components"><a class="header" href="#chapter-3-wal-architecture-and-components">Chapter 3: WAL Architecture and Components</a></h3>
<p>Finally, we break down the internal architecture of a typical WAL implementation - from the <strong>log buffer</strong> and <strong>log files</strong> to <strong>flush</strong>, <strong>checkpoint</strong>, and <strong>recovery mechanisms</strong>. This chapter prepares you for hands-on exploration in the next parts, where you’ll build and experiment with your own WAL in Rust.</p>
<hr />
<p>By the end of Part 1, you’ll have a deep conceptual understanding of <strong>how and why databases protect data durability</strong>, setting the stage for implementation and optimization in later sections.</p>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="intro-to-data-durability"><a class="header" href="#intro-to-data-durability">Intro to Data Durability</a></h1>
<h2 id="the-data-behind-everyday-applications"><a class="header" href="#the-data-behind-everyday-applications">The Data Behind Everyday Applications</a></h2>
<p>Every day, we interact with dozens of applications that silently manage and store our data - from social media platforms and email clients to banking apps and cloud drives.
When you:</p>
<ul>
<li>Send a message on WhatsApp,</li>
<li>Save a photo to Google Photos,</li>
<li>Stream a playlist on Spotify, or</li>
<li>Check your balance on a banking app,</li>
</ul>
<p>you’re relying on a massive network of servers and databases to <strong>store and retrieve your data reliably</strong>.</p>
<p>This data isn’t trivial - it represents our:</p>
<ul>
<li><strong>Personal memories</strong> (photos, messages, notes),</li>
<li><strong>Financial records</strong> (transactions, bills, receipts),</li>
<li><strong>Business assets</strong> (customer data, analytics, and product information), and</li>
<li><strong>System states</strong> (user sessions, cache, logs, etc.).</li>
</ul>
<p>Losing this data can be catastrophic - imagine a bank losing transaction records or a social platform losing years of messages.
That’s why <strong>data durability</strong> is one of the most critical guarantees that storage systems and databases strive to achieve.</p>
<hr />
<h2 id="what-is-durability"><a class="header" href="#what-is-durability">What Is Durability?</a></h2>
<h3 id="general-definition"><a class="header" href="#general-definition"><strong>General Definition</strong></a></h3>
<p><strong>Durability</strong> refers to the <strong>ability of a system to preserve information even in the face of failures</strong> - such as crashes, power loss, or hardware damage.</p>
<p>In simple terms:</p>
<blockquote>
<p>Once data is written and confirmed as “saved,” it should <strong>never disappear</strong> unexpectedly.</p>
</blockquote>
<p>For example, if you write a document and click “Save,” you expect that file to still exist tomorrow, even if your computer restarts or the application crashes.</p>
<h3 id="importance-of-durability"><a class="header" href="#importance-of-durability"><strong>Importance of Durability</strong></a></h3>
<p>Durability is not just a technical property - it’s a <strong>promise of trust</strong> between the system and its users:</p>
<ul>
<li>It ensures that <strong>once a transaction is complete</strong>, it will remain intact.</li>
<li>It allows systems to <strong>recover from crashes</strong> without losing state.</li>
<li>It forms one of the key pillars of the <strong>ACID</strong> properties in databases (Atomicity, Consistency, Isolation, <strong>Durability</strong>).</li>
</ul>
<p>Without durability, all other guarantees collapse - a perfectly consistent and atomic system means little if it forgets its data after a reboot.</p>
<hr />
<h2 id="durability-in-storage-and-databases"><a class="header" href="#durability-in-storage-and-databases">Durability in Storage and Databases</a></h2>
<p>To understand durability in databases, we must look at how computers store data at different layers.</p>
<h3 id="volatile-vs-non-volatile-storage"><a class="header" href="#volatile-vs-non-volatile-storage"><strong>Volatile vs Non-Volatile Storage</strong></a></h3>
<div class="table-wrapper"><table><thead><tr><th>Type</th><th>Description</th><th>Example</th><th>Durability</th></tr></thead><tbody>
<tr><td><strong>Volatile Storage</strong></td><td>Loses data when power is lost</td><td>RAM (Random Access Memory)</td><td>Not durable</td></tr>
<tr><td><strong>Non-Volatile Storage</strong></td><td>Persists data even after power loss</td><td>SSDs, Hard Drives, Flash Storage</td><td>Durable</td></tr>
</tbody></table>
</div>
<p>When you run a database query or modify a record, the changes first happen in <strong>memory (RAM)</strong> for speed. But memory is volatile - if the machine crashes before changes reach disk, data is lost.</p>
<p>Therefore, databases must ensure that <strong>critical data is safely written to durable storage (disk or SSD)</strong> before confirming a successful operation.</p>
<hr />
<h1 id="how-computers-lose-data"><a class="header" href="#how-computers-lose-data">How Computers Lose Data</a></h1>
<p>Even the most powerful computers and databases are vulnerable to one simple truth:
<strong>hardware and software can fail.</strong></p>
<p>To understand why durability is so important - and how techniques like Write-Ahead Logging (WAL) were invented - we first need to see <em>how</em> computers actually lose data.</p>
<hr />
<h2 id="1-the-fragile-path-of-data"><a class="header" href="#1-the-fragile-path-of-data">1. The Fragile Path of Data</a></h2>
<p>When an application writes data - say, saving a user’s profile or recording a transaction - that data travels through several layers before reaching permanent storage:</p>
<pre><code>Application → Database Engine → OS Cache → Disk Controller → Disk/SSD
</code></pre>
<p>Each layer plays a role in speed and reliability, but also introduces <strong>points of failure</strong>.</p>
<p>Let’s see what can go wrong.</p>
<hr />
<h2 id="2-common-causes-of-data-loss"><a class="header" href="#2-common-causes-of-data-loss">2. Common Causes of Data Loss</a></h2>
<h3 id="a-power-failures"><a class="header" href="#a-power-failures"><strong>a. Power Failures</strong></a></h3>
<p>Power outages or system crashes can occur at any moment - even in the middle of a write.</p>
<p>If data is still in <strong>volatile memory (RAM)</strong> or <strong>OS buffers</strong>, it disappears instantly when power is lost.</p>
<blockquote>
<p>Example:
A database acknowledges a successful write, but the data never made it from cache to disk. After reboot, the record is gone.</p>
</blockquote>
<p>This is why true durability requires <strong>flushing writes to non-volatile storage</strong> and verifying that the data has been persisted.</p>
<hr />
<h3 id="b-software-crashes"><a class="header" href="#b-software-crashes"><strong>b. Software Crashes</strong></a></h3>
<p>Applications and databases are complex. A crash during write operations can leave files <strong>partially written</strong> or <strong>corrupted</strong>.</p>
<p>For example:</p>
<ul>
<li>A database might crash midway through updating an index.</li>
<li>File metadata may be inconsistent, leaving “dangling” data blocks.</li>
</ul>
<p>Such <strong>inconsistent states</strong> make recovery difficult - unless the system maintains an ordered log of what was being done (hint: WAL helps here).</p>
<hr />
<h3 id="c-hardware-failures"><a class="header" href="#c-hardware-failures"><strong>c. Hardware Failures</strong></a></h3>
<p>Disks and SSDs, despite their reliability, <strong>wear out</strong> over time.</p>
<ul>
<li>Hard disks can develop <strong>bad sectors</strong> or <strong>head crashes</strong>.</li>
<li>SSDs have <strong>limited write cycles</strong> and can suffer from controller failures.</li>
</ul>
<p>If data is stored on a single disk with no replication, its loss may be <strong>irrecoverable</strong>.</p>
<p>Modern systems counter this with:</p>
<ul>
<li>RAID configurations,</li>
<li>Replication to multiple nodes, and</li>
<li>Checksums to detect bit rot or silent corruption.</li>
</ul>
<hr />
<h3 id="d-operating-system-or-file-system-errors"><a class="header" href="#d-operating-system-or-file-system-errors"><strong>d. Operating System or File System Errors</strong></a></h3>
<p>Even if hardware works fine, <strong>file systems</strong> (like NTFS, ext4, or APFS) can introduce risk:</p>
<ul>
<li>The OS might reorder or batch writes for performance.</li>
<li>Filesystem journaling might not complete before a crash.</li>
<li>Metadata (like directory entries) may get corrupted.</li>
</ul>
<p>When this happens, data might physically exist on disk but be <strong>logically inaccessible</strong>.</p>
<hr />
<h3 id="e-human-error"><a class="header" href="#e-human-error"><strong>e. Human Error</strong></a></h3>
<p>Not all failures are mechanical.
Developers and administrators also delete, overwrite, or misconfigure systems accidentally.</p>
<p>Examples:</p>
<ul>
<li>Running <code>rm -rf /data</code> on the wrong server.</li>
<li>Misapplying a database migration.</li>
<li>Restoring from an outdated backup.</li>
</ul>
<p>Human mistakes are among the <strong>most frequent causes</strong> of data loss - and durability mechanisms can only help if changes are logged before they’re lost.</p>
<hr />
<h2 id="3-data-corruption-the-silent-enemy"><a class="header" href="#3-data-corruption-the-silent-enemy">3. Data Corruption: The Silent Enemy</a></h2>
<p>Sometimes, data doesn’t vanish - it <strong>changes silently</strong>.
Corruption can happen due to:</p>
<ul>
<li>Bit flips caused by cosmic radiation or faulty RAM,</li>
<li>Disk write interruptions,</li>
<li>Transmission errors across hardware buses.</li>
</ul>
<p>The danger is that these errors might <strong>go undetected for months</strong> until a read operation fails.
Databases therefore use <strong>checksums</strong> and <strong>redundant copies</strong> to detect and repair corruption early.</p>
<hr />
<h2 id="how-durability-mechanisms-protect-against-loss"><a class="header" href="#how-durability-mechanisms-protect-against-loss">How Durability Mechanisms Protect Against Loss</a></h2>
<p>Let’s revisit the durability techniques introduced earlier - now through the lens of failure scenarios:</p>
<div class="table-wrapper"><table><thead><tr><th>Failure Type</th><th>Without Durability</th><th>With WAL/Durable Design</th></tr></thead><tbody>
<tr><td>Power Loss</td><td>Incomplete writes lost</td><td>Changes logged before crash; can replay after reboot</td></tr>
<tr><td>Crash During Update</td><td>Corrupted data</td><td>Atomic recovery using log</td></tr>
<tr><td>Disk Failure</td><td>Data gone</td><td>Replication or backup recovers state</td></tr>
<tr><td>Software Bug</td><td>Inconsistent state</td><td>Logs enable rollback or reapply</td></tr>
<tr><td>Human Error</td><td>Permanent deletion</td><td>Point-in-time recovery via logs</td></tr>
</tbody></table>
</div>
<hr />
<h2 id="the-limits-of-durability"><a class="header" href="#the-limits-of-durability">The Limits of Durability</a></h2>
<p>No system can promise <strong>absolute</strong> durability - disks fail, data centers burn, and bugs slip through.
However, systems aim for <strong>practical durability</strong>, often expressed as:</p>
<blockquote>
<p><em>“The probability of data loss is less than 1 in 10¹⁵ writes.”</em></p>
</blockquote>
<p>In practice, this means data is:</p>
<ul>
<li>Written to <strong>durable media</strong> (disk/SSD),</li>
<li>Logged before application,</li>
<li>Replicated across machines or regions,</li>
<li>Verified with checksums, and</li>
<li>Periodically backed up.</li>
</ul>
<p>These layers together form a <strong>defense-in-depth</strong> strategy against data loss.</p>
<hr />
<h2 id="setting-the-stage-for-write-ahead-logging"><a class="header" href="#setting-the-stage-for-write-ahead-logging">Setting the Stage for Write-Ahead Logging</a></h2>
<p>Write-Ahead Logging was designed as a <strong>structured response</strong> to the chaos of system failures.
It ensures that <strong>every change is recorded in a durable log before modifying the main data</strong>, allowing systems to recover exactly where they left off.</p>
<p>In essence:</p>
<blockquote>
<p><em>If computers can lose data at any moment, WAL ensures they can always find their way back.</em></p>
</blockquote>
<p>In the next chapter, we’ll dive into <strong>the core idea and key principles of Write-Ahead Logging</strong> - how it works, why it’s fast despite being durable, and how databases like PostgreSQL, SQLite, and RocksDB rely on it every second.</p>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-2-the-core-idea-of-write-ahead-logging-1"><a class="header" href="#chapter-2-the-core-idea-of-write-ahead-logging-1">Chapter 2: The Core Idea of Write-Ahead Logging</a></h1>
<p>When you use a database - whether you’re transferring money in an app, editing a document, or saving a game - you expect your data to stay safe, <strong>no matter what</strong>.
Even if the power goes out, your laptop crashes, or your phone dies midway, you want your data to come back intact when you restart.</p>
<p>But how does that actually happen?</p>
<p>That’s where <strong>Write-Ahead Logging (WAL)</strong> comes in - a clever technique that databases use to ensure that data isn’t lost or corrupted even in the face of failures.
Let’s understand the <em>core idea</em> behind it step by step.</p>
<hr />
<h2 id="1-the-problem-wal-solves"><a class="header" href="#1-the-problem-wal-solves">1. The Problem WAL Solves</a></h2>
<p>Imagine you’re building a banking app.
A user transfers ₹1000 from <strong>Account A</strong> to <strong>Account B</strong>.</p>
<p>You write some code like this:</p>
<pre><code class="language-sql">UPDATE accounts SET balance = balance - 1000 WHERE id = 'A';
UPDATE accounts SET balance = balance + 1000 WHERE id = 'B';
COMMIT;
</code></pre>
<p>Looks fine, right?</p>
<p>Now imagine the power goes out <strong>after</strong> the first line executes but <strong>before</strong> the second one does.
Account A has lost ₹1000, but Account B hasn’t received it yet.
Your database is now in an inconsistent state.</p>
<p>This kind of partial update is what <strong>WAL</strong> is designed to prevent.</p>
<p>It ensures that even if a crash happens halfway through an operation, the database can <strong>recover to a consistent state</strong> - either completing the transaction fully or rolling it back entirely.</p>
<hr />
<h2 id="2-the-naive-approach-and-its-limitations"><a class="header" href="#2-the-naive-approach-and-its-limitations">2. The Naive Approach and Its Limitations</a></h2>
<p>In theory, we could just <strong>write all changes directly to disk immediately</strong> to ensure they are durable.</p>
<p>But there’s a problem - disk I/O is <strong>slow</strong> compared to memory operations.</p>
<p>If every update required rewriting data files on disk synchronously, performance would tank.
To make matters worse, if the system crashes during that disk write, the data file could become partially updated (corrupted).</p>
<p>So we need something faster and safer - something that records <em>what we intend to do</em> before doing it.</p>
<hr />
<h2 id="3-the-key-idea-log-before-you-write"><a class="header" href="#3-the-key-idea-log-before-you-write">3. The Key Idea: Log Before You Write</a></h2>
<p>Here’s the golden rule of WAL:</p>
<blockquote>
<p><strong>Always write the intent of a change (to a log) before applying the change to the main data.</strong></p>
</blockquote>
<p>This means whenever the database wants to modify data, it follows this simple three-step process:</p>
<ol>
<li><strong>Create a log record</strong> that describes the change.
Example: “At time T, subtract 1000 from A, add 1000 to B.”</li>
<li><strong>Append that record to a log file</strong> on disk - <em>sequentially</em> (this is fast).</li>
<li><strong>Apply the change to the actual data file</strong> - possibly later, in batches.</li>
</ol>
<p>If a crash happens right after step 2 but before step 3, the log file still contains the record of what was intended.
When the system restarts, it can <strong>replay</strong> the log to restore the correct state.</p>
<p>Here’s a simple pseudo-code version:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn transfer(a: &amp;mut Account, b: &amp;mut Account, amount: i64) {
    // Step 1: create log entry
    let log_entry = format!("TRANSFER {} {} {}", a.id, b.id, amount);

    // Step 2: write log first (flush to disk)
    write_to_log(log_entry);

    // Step 3: apply to data
    a.balance -= amount;
    b.balance += amount;

    // Step 4: mark log as committed
    mark_log_committed();
}
<span class="boring">}</span></code></pre></pre>
<p>Notice that <strong>we never update the data until the log is safely on disk</strong>.</p>
<p>That’s the essence of “write-ahead.”</p>
<hr />
<h2 id="4-the-log-as-a-source-of-truth"><a class="header" href="#4-the-log-as-a-source-of-truth">4. The Log as a Source of Truth</a></h2>
<p>The WAL is typically a <strong>sequential file</strong> - an append-only record of every change the database plans to make.</p>
<p>Each record (or “log entry”) contains enough information to redo or undo an operation.</p>
<p>Example structure:</p>
<div class="table-wrapper"><table><thead><tr><th>Log Sequence Number (LSN)</th><th>Operation</th><th>Before Value</th><th>After Value</th><th>Page ID</th></tr></thead><tbody>
<tr><td>101</td><td>UPDATE</td><td>balance=5000</td><td>balance=4000</td><td>A</td></tr>
<tr><td>102</td><td>UPDATE</td><td>balance=2000</td><td>balance=3000</td><td>B</td></tr>
</tbody></table>
</div>
<p>Why is this efficient?</p>
<ul>
<li>Appending to a log file is a <strong>sequential write</strong> - disks and SSDs handle it extremely fast.</li>
<li>Updating the data file directly involves <strong>random I/O</strong>, which is much slower.</li>
<li>This means WAL gives you <strong>durability with high performance</strong>.</li>
</ul>
<p>In many databases (like PostgreSQL and SQLite), the log file is stored separately - often in a directory like <code>pg_wal/</code> or <code>journal/</code>.</p>
<hr />
<h2 id="5-crash-recovery-using-wal"><a class="header" href="#5-crash-recovery-using-wal">5. Crash Recovery Using WAL</a></h2>
<p>So what happens if the system crashes?</p>
<p>On restart, the database checks two things:</p>
<ol>
<li><strong>The log</strong> - what changes were intended or committed?</li>
<li><strong>The data files</strong> - what’s the current state on disk?</li>
</ol>
<p>It then performs two operations:</p>
<ul>
<li><strong>Redo:</strong> Reapply committed changes that weren’t fully written to the data files.</li>
<li><strong>Undo:</strong> Roll back uncommitted changes that were partially applied.</li>
</ul>
<p>Example:</p>
<p>Let’s say the last few WAL entries were:</p>
<pre><code>LSN 201: START TRANSACTION
LSN 202: UPDATE A -1000
LSN 203: UPDATE B +1000
LSN 204: COMMIT
</code></pre>
<p>If the system crashed before the data was flushed to disk, the WAL replay will read up to <code>LSN 204</code> (the COMMIT) and <strong>redo</strong> the operations to make sure both updates appear.</p>
<p>If it crashed before <code>COMMIT</code>, WAL ensures both are undone - maintaining atomicity.</p>
<hr />
<h2 id="6-benefits-of-wal"><a class="header" href="#6-benefits-of-wal">6. Benefits of WAL</a></h2>
<p>WAL gives databases several powerful advantages:</p>
<ul>
<li><strong>Durability</strong> – Once a log is written, data can always be recovered.</li>
<li><strong>Atomicity</strong> – Transactions either complete fully or not at all.</li>
<li><strong>Performance</strong> – Sequential writes are faster than random writes.</li>
<li><strong>Recovery</strong> – Easy crash recovery using redo/undo.</li>
<li><strong>Replication</strong> – Logs can be shipped to replicas for high availability.</li>
<li><strong>Checkpoints</strong> – Data files can be periodically synced with logs for faster startup.</li>
</ul>
<p>This is why almost every modern database - PostgreSQL, MySQL (InnoDB), SQLite, LevelDB, RocksDB, and more - uses some form of WAL.</p>
<hr />
<h2 id="7-real-world-examples"><a class="header" href="#7-real-world-examples">7. Real-world Examples</a></h2>
<p>Here’s how different systems implement WAL:</p>
<ul>
<li>
<p><strong>PostgreSQL:</strong>
Uses WAL segments (usually 16 MB files) stored in <code>pg_wal/</code>.
You can stream these logs to replicas using <strong>WAL shipping</strong>.
<a href="https://www.postgresql.org/docs/current/wal-intro.html">PostgreSQL WAL Docs →</a></p>
</li>
<li>
<p><strong>SQLite:</strong>
Uses a WAL journal file (<code>.wal</code>) for atomic commits.
You can even inspect it with tools like <code>sqlite3 mydb.db ".recover"</code>.
<a href="https://www.sqlite.org/wal.html">SQLite WAL Mode →</a></p>
</li>
<li>
<p><strong>RocksDB / LevelDB:</strong>
Use a write-ahead log (<code>MANIFEST</code> or <code>.log</code>) to record changes to key-value stores.
It helps rebuild the database after crashes without full re-compaction.</p>
</li>
</ul>
<p>Even modern file systems like <strong>ext4</strong> and <strong>NTFS</strong> internally use journaling - which is a form of WAL at the file-system level.</p>
<hr />
<h2 id="8-summary-and-key-takeaways"><a class="header" href="#8-summary-and-key-takeaways">8. Summary and Key Takeaways</a></h2>
<ul>
<li><strong>WAL’s core principle:</strong> <em>Log every change before applying it.</em></li>
<li>This ensures <strong>durability</strong>, <strong>atomicity</strong>, and <strong>fast recovery</strong>.</li>
<li>Logs are <strong>append-only</strong>, making writes efficient.</li>
<li>During recovery, WAL replays committed transactions and rolls back incomplete ones.</li>
<li>It’s the foundation for advanced database features like replication, checkpoints, and crash recovery.</li>
</ul>
<hr />
<blockquote>
<p><strong>Coming Next:</strong>
In the next chapter, we’ll open up the hood and look at the <strong>architecture of WAL</strong> - the actual components, buffer management, checkpoints, and how databases implement them efficiently.</p>
</blockquote>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-3-wal-architecture-and-components-1"><a class="header" href="#chapter-3-wal-architecture-and-components-1">Chapter 3: WAL Architecture and Components</a></h1>
<p>In the previous chapter, we understood the <strong>core idea</strong> of Write-Ahead Logging (WAL):</p>
<blockquote>
<p>“Always record changes in a log before applying them to the main data.”</p>
</blockquote>
<p>Now, let’s go one layer deeper and see <strong>how WAL is actually structured</strong> inside a database.
Think of this chapter as looking “under the hood” - we’ll explore how WAL is organized, the key components that make it work, and the different types used across systems.</p>
<hr />
<h2 id="1-what-happens-inside-a-wal-system"><a class="header" href="#1-what-happens-inside-a-wal-system">1. What Happens Inside a WAL System</a></h2>
<p>When a database receives an update (say, <code>UPDATE accounts SET balance = balance - 100;</code>),
it doesn’t immediately rewrite the whole data file. Instead, it follows this sequence:</p>
<ol>
<li><strong>Prepare a log record</strong> that describes what’s about to change.</li>
<li><strong>Append</strong> that log record to the WAL file (sequential write).</li>
<li><strong>Acknowledge</strong> the commit after the log is safely flushed to disk.</li>
<li><strong>Apply</strong> the actual data change later in the background (during checkpoints).</li>
</ol>
<p>This is the essential workflow across almost all WAL-based systems.</p>
<hr />
<h2 id="2-types-of-write-ahead-logs"><a class="header" href="#2-types-of-write-ahead-logs">2. Types of Write-Ahead Logs</a></h2>
<p>Different systems design WAL differently depending on their recovery model, performance target, and data model.
Broadly, we can categorize WAL implementations into <strong>three major types</strong>:</p>
<hr />
<h3 id="21-physical-wal"><a class="header" href="#21-physical-wal"><strong>2.1 Physical WAL</strong></a></h3>
<p>This logs <strong>exact bytes</strong> of data pages that changed.</p>
<ul>
<li>Used by systems like <strong>PostgreSQL</strong> and <strong>SQLite</strong>.</li>
<li>Log entries describe <em>which block</em> changed and <em>what bytes</em> were modified.</li>
<li>Recovery simply replays these changes in order.</li>
</ul>
<h4 id="example-pseudo-code"><a class="header" href="#example-pseudo-code">Example (Pseudo-code)</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Simplified physical WAL record structure
struct PhysicalWalRecord {
    lsn: u64,              // Log sequence number
    page_id: u64,          // Which page (block) changed
    offset: u32,           // Byte offset within page
    before: Vec&lt;u8&gt;,       // Optional old bytes (for undo)
    after: Vec&lt;u8&gt;,        // New bytes to apply
}

// Writing a physical WAL entry
fn log_page_update(page_id: u64, offset: u32, new_data: &amp;[u8]) {
    let record = PhysicalWalRecord {
        lsn: next_lsn(),
        page_id,
        offset,
        before: vec![],  // optional
        after: new_data.to_vec(),
    };
    wal_append(record);
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Advantages:</strong></p>
<ul>
<li>Simple to implement and fast for redo.</li>
<li>Works directly on binary pages.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Large log volume (writes entire page deltas).</li>
<li>Harder to make portable between versions or replicas.</li>
</ul>
<hr />
<h3 id="22-logical-wal"><a class="header" href="#22-logical-wal"><strong>2.2 Logical WAL</strong></a></h3>
<p>Logs the <strong>intent</strong> of the change, not the bytes.
Instead of “write bytes X–Y on page 123”, it logs “increment balance by 100”.</p>
<ul>
<li>Used by systems like <strong>MySQL binlog (row or statement-based)</strong> or <strong>logical replication</strong> in PostgreSQL.</li>
<li>Useful for replication because it’s <em>database-independent</em>.</li>
</ul>
<h4 id="example-pseudo-code-1"><a class="header" href="#example-pseudo-code-1">Example (Pseudo-code)</a></h4>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Logical WAL record (SQL-level)
struct LogicalWalRecord {
    lsn: u64,
    table: String,
    operation: String,     // "UPDATE", "INSERT", etc.
    primary_key: u64,
    old_values: Option&lt;HashMap&lt;String, String&gt;&gt;,
    new_values: HashMap&lt;String, String&gt;,
}

fn log_update(pk: u64, old_balance: i64, new_balance: i64) {
    let record = LogicalWalRecord {
        lsn: next_lsn(),
        table: "accounts".into(),
        operation: "UPDATE".into(),
        primary_key: pk,
        old_values: Some(hashmap!{"balance" =&gt; old_balance.to_string()}),
        new_values: hashmap!{"balance" =&gt; new_balance.to_string()},
    };
    wal_append(record);
}
<span class="boring">}</span></code></pre></pre>
<p><strong>Advantages:</strong></p>
<ul>
<li>Compact, portable, and perfect for replication.</li>
<li>Easier to interpret for analytics or CDC (Change Data Capture).</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Slower recovery (must interpret higher-level operations).</li>
<li>Requires schema awareness.</li>
</ul>
<hr />
<h3 id="23-hybrid-wal"><a class="header" href="#23-hybrid-wal"><strong>2.3 Hybrid WAL</strong></a></h3>
<p>Many modern systems combine both:</p>
<ul>
<li>Physical WAL for <strong>fast recovery</strong>,</li>
<li>Logical WAL for <strong>replication or auditing</strong>.</li>
</ul>
<p><strong>Example:</strong> PostgreSQL uses physical WAL for crash recovery and allows exporting <em>logical replication streams</em> derived from it.</p>
<hr />
<h2 id="3-key-components-of-wal"><a class="header" href="#3-key-components-of-wal">3. Key Components of WAL</a></h2>
<p>Let’s break down the moving parts inside a WAL system.
Each piece has a clear role in ensuring durability and consistency.</p>
<hr />
<h3 id="31-log-sequence-number-lsn"><a class="header" href="#31-log-sequence-number-lsn"><strong>3.1 Log Sequence Number (LSN)</strong></a></h3>
<p>Every WAL record gets a <strong>monotonic number</strong> - the <strong>LSN</strong>.
It uniquely identifies a log position and ensures recovery happens in the correct order.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>static mut GLOBAL_LSN: u64 = 0;

fn next_lsn() -&gt; u64 {
    unsafe {
        GLOBAL_LSN += 1;
        GLOBAL_LSN
    }
}
<span class="boring">}</span></code></pre></pre>
<p>LSNs are like version numbers for database changes.
During recovery, the system knows “I’ve replayed logs up to LSN = 1050”.</p>
<hr />
<h3 id="32-wal-buffer"><a class="header" href="#32-wal-buffer"><strong>3.2 WAL Buffer</strong></a></h3>
<p>Instead of writing each log record directly to disk, databases keep a <strong>WAL buffer</strong> in memory.</p>
<ul>
<li>Multiple log records are grouped together (batching).</li>
<li>Written to disk periodically or on commit.</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>struct WalBuffer {
    entries: Vec&lt;WalRecord&gt;,
}

impl WalBuffer {
    fn append(&amp;mut self, rec: WalRecord) {
        self.entries.push(rec);
        if self.entries.len() &gt; 1000 {
            self.flush_to_disk();
        }
    }

    fn flush_to_disk(&amp;mut self) {
        // Sequential write
        write_all("wal.log", &amp;self.entries);
        self.entries.clear();
    }
}
<span class="boring">}</span></code></pre></pre>
<p>This gives both <strong>high throughput</strong> and <strong>durability</strong>.</p>
<hr />
<h3 id="33-wal-file-manager"><a class="header" href="#33-wal-file-manager"><strong>3.3 WAL File Manager</strong></a></h3>
<p>Handles log file creation, rotation, and deletion.</p>
<ul>
<li>Large systems split WAL into segments (like <code>00000001.wal</code>, <code>00000002.wal</code>).</li>
<li>Older logs are archived or removed after checkpoints.</li>
</ul>
<pre><code class="language-bash">postgresql/
├── data/
│   ├── base/
│   ├── pg_wal/
│   │   ├── 00000001000000000000000A
│   │   ├── 00000001000000000000000B
│   │   └── ...
</code></pre>
<hr />
<h3 id="34-checkpoints"><a class="header" href="#34-checkpoints"><strong>3.4 Checkpoints</strong></a></h3>
<p>A <strong>checkpoint</strong> is when all in-memory changes are written to disk,
and the WAL up to that point is no longer needed for recovery.</p>
<ul>
<li>Reduces WAL size.</li>
<li>Makes recovery faster (start from the last checkpoint).</li>
</ul>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn checkpoint() {
    flush_dirty_pages_to_disk();
    mark_checkpoint_lsn(current_lsn());
    truncate_old_wal_files();
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h3 id="35-wal-writer-background-process"><a class="header" href="#35-wal-writer-background-process"><strong>3.5 WAL Writer (Background Process)</strong></a></h3>
<p>In systems like PostgreSQL:</p>
<ul>
<li>A background process (<code>walwriter</code>) constantly flushes log buffers to disk.</li>
<li>Ensures commit latency stays low even under heavy load.</li>
</ul>
<p>This decouples <strong>user transactions</strong> from <strong>disk I/O</strong>.</p>
<hr />
<h3 id="36-wal-replay-recovery"><a class="header" href="#36-wal-replay-recovery"><strong>3.6 WAL Replay (Recovery)</strong></a></h3>
<p>When the system restarts after a crash:</p>
<ol>
<li>Identify the last checkpoint.</li>
<li>Replay all WAL entries <strong>after</strong> it.</li>
<li>Apply redo operations until the end of log.</li>
</ol>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn recover_from_wal() {
    let logs = read_wal_since_checkpoint();
    for record in logs {
        apply_record(record);
    }
    println!("Recovery complete");
}
<span class="boring">}</span></code></pre></pre>
<p>This guarantees that all committed transactions are redone,
and uncommitted ones are ignored (thanks to transaction IDs).</p>
<hr />
<h2 id="4-putting-it-all-together"><a class="header" href="#4-putting-it-all-together">4. Putting It All Together</a></h2>
<p>Let’s visualize a complete WAL cycle:</p>
<pre><code>WAL Write Sequence:

App
 │
 │ Write log record
 ▼
WAL_Buffer
 │
 │ Flush on commit
 ▼
Disk_Log
 │
 │ Commit acknowledged (durable)
 ▼
Data_File
 │
 │ Apply during checkpoint
 ▼
Background redo (if crash occurs)
</code></pre>
<pre><code>Step | Component      | Action
-----|----------------|---------------------------
1    | App            | Write log record
2    | WAL_Buffer     | Append to in-memory log buffer
3    | Disk_Log       | Flush log to disk (commit)
4    | App            | Commit acknowledged (durable)
5    | Data_File      | Apply changes during checkpoint
6    | Recovery       | Background redo if needed
</code></pre>
<p>This separation of <strong>logging</strong> (fast sequential writes) and <strong>data flushing</strong> (slow random writes)
is the key to WAL’s efficiency and reliability.</p>
<hr />
<h2 id="5-summary"><a class="header" href="#5-summary">5. Summary</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Component</th><th>Purpose</th><th>Example</th></tr></thead><tbody>
<tr><td><strong>LSN</strong></td><td>Identifies log order</td><td><code>000000010000000A</code></td></tr>
<tr><td><strong>WAL Buffer</strong></td><td>Temporary memory for batching logs</td><td><code>wal_buffer.append(record)</code></td></tr>
<tr><td><strong>WAL File</strong></td><td>Durable append-only log on disk</td><td><code>pg_wal/000000010000000A</code></td></tr>
<tr><td><strong>Checkpoint</strong></td><td>Flushes dirty pages, truncates old WAL</td><td><code>checkpoint()</code></td></tr>
<tr><td><strong>Replay/Recovery</strong></td><td>Re-applies committed changes after crash</td><td><code>recover_from_wal()</code></td></tr>
</tbody></table>
</div>
<hr />
<h3 id="key-takeaways"><a class="header" href="#key-takeaways">Key Takeaways</a></h3>
<ul>
<li>WAL is the <strong>heart of durability</strong> - every change is first made durable through the log.</li>
<li>There are <strong>three main types</strong>: physical, logical, and hybrid.</li>
<li>Core components (LSN, buffer, files, checkpoint, replay) work together to ensure <strong>crash recovery and consistency</strong>.</li>
<li>WAL is not just for databases - file systems, message queues, and even compilers use the same principle.</li>
</ul>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h2 id="part-ii-implementing-wal"><a class="header" href="#part-ii-implementing-wal">Part II: Implementing WAL</a></h2>
<p>In <strong>Part I</strong>, we explored the fundamental ideas behind <strong>durability</strong> and the <strong>write-ahead logging (WAL)</strong> mechanism-why it exists, how it ensures data safety, and the architectural components that make it work.
Now, it’s time to move from understanding <strong>what</strong> WAL is to learning <strong>how to build it</strong>.</p>
<p>In this part, we’ll gradually construct a minimal yet functional WAL system, exploring the techniques that make it reliable, efficient, and recoverable. You’ll see how databases ensure no committed data is ever lost-even if the system crashes halfway through a write-and how logs evolve, compact, and cooperate with concurrent operations.</p>
<p>Here’s what lies ahead:</p>
<ul>
<li>
<p><strong>Chapter 4 – Designing a Minimal WAL:</strong>
We start small-designing a simple, append-only WAL that records changes safely before applying them. You’ll learn the key data structures, file formats, and write-order guarantees needed for correctness.
Based on a request from a reader, there is another version of this chapter with code snippets in C#.</p>
</li>
<li>
<p><strong>Chapter 5 – Crash Recovery:</strong>
Systems fail. This chapter shows how to bring them back to a consistent state by replaying log entries, ensuring durability and atomicity even after unexpected crashes.</p>
</li>
<li>
<p><strong>Chapter 6 – Checkpointing and Log Compaction:</strong>
As logs grow, we must prune them efficiently. You’ll learn how checkpointing creates a balance between performance and space, and how compaction reclaims disk without compromising safety.</p>
</li>
<li>
<p><strong>Chapter 7 – Concurrency and WAL:</strong>
Modern systems handle many transactions in parallel. Here, we explore synchronization, ordering, and how concurrent writes and flushes are coordinated in multi-threaded environments.</p>
</li>
<li>
<p><strong>Chapter 8 – Building a WAL in Rust:</strong>
Finally, theory meets practice. We’ll build a working WAL module in <strong>Rust</strong>, focusing on correctness, performance, and clean abstractions-turning all concepts into tangible code.</p>
</li>
</ul>
<p>By the end of Part II, you’ll not only understand how a WAL works internally but also have a working implementation you can extend into a small database or storage engine of your own.</p>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-4-designing-a-minimal-wal"><a class="header" href="#chapter-4-designing-a-minimal-wal">Chapter 4: Designing a Minimal WAL</a></h1>
<p>In this chapter, we’ll <strong>build a minimal Write-Ahead Log (WAL)</strong> — the foundation of durability in databases.
You’ll see how a WAL works <em>internally</em>, one step at a time, and by the end you’ll have a working implementation in <strong>Rust</strong> that can recover from crashes.</p>
<hr />
<h2 id="step-1-setting-the-goal"><a class="header" href="#step-1-setting-the-goal">Step 1: Setting the Goal</a></h2>
<p>Our minimal WAL will:</p>
<ol>
<li>Accept a series of “operations” (like inserts or updates).</li>
<li>Write each operation to a <strong>log file</strong> before applying it.</li>
<li>Ensure that once written and flushed, the data will <strong>survive a crash</strong>.</li>
<li>Allow recovery by <strong>replaying</strong> the log.</li>
</ol>
<hr />
<h3 id="the-big-picture"><a class="header" href="#the-big-picture">The Big Picture</a></h3>
<pre><code> ┌──────────────┐        ┌──────────────┐
 │  Operation   │        │   Database   │
 │ (e.g. update)│        │   State      │
 └──────┬───────┘        └──────┬───────┘
        │                        │
        │                        │
        ▼                        │
   Write to WAL File              │
        │                        │
        ▼                        │
   Flush to Disk (Durable)        │
        │                        │
        └────────► Apply to DB ◄──┘
</code></pre>
<p>The rule is simple:</p>
<blockquote>
<p><strong>Never modify data before logging it.</strong></p>
</blockquote>
<hr />
<h2 id="step-2-designing-the-log-format"><a class="header" href="#step-2-designing-the-log-format">Step 2: Designing the Log Format</a></h2>
<p>We’ll define a simple binary format for each record.</p>
<pre><code>+------------+-------------------+
| 4 bytes    | variable length   |
| length (u32) | payload bytes   |
+------------+-------------------+
</code></pre>
<p>Each entry has:</p>
<ul>
<li>A <strong>length prefix</strong> - so we know how much data to read.</li>
<li>A <strong>payload</strong> - the actual operation or change.</li>
</ul>
<p>If the system crashes mid-write, we can detect incomplete entries using this prefix.</p>
<hr />
<h3 id="example"><a class="header" href="#example">Example</a></h3>
<p>If we log two updates:</p>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Bytes Written</th></tr></thead><tbody>
<tr><td><code>set x=1</code></td><td><code>[06][73 65 74 20 78 3d 31]</code></td></tr>
<tr><td><code>set y=2</code></td><td><code>[06][73 65 74 20 79 3d 32]</code></td></tr>
</tbody></table>
</div>
<p>Visually:</p>
<pre><code>┌──────────────────────────────────────────────┐
│ 0x06 set x=1 | 0x06 set y=2                  │
└──────────────────────────────────────────────┘
</code></pre>
<hr />
<h2 id="step-3-creating-the-wal-file"><a class="header" href="#step-3-creating-the-wal-file">Step 3: Creating the WAL File</a></h2>
<p>We start with the file management code:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use std::fs::{File, OpenOptions};
use std::io::{Write, Read, Seek, SeekFrom};
use std::path::Path;

pub struct Wal {
    file: File,
}

impl Wal {
    pub fn open&lt;P: AsRef&lt;Path&gt;&gt;(path: P) -&gt; std::io::Result&lt;Self&gt; {
        let file = OpenOptions::new()
            .create(true)
            .append(true)
            .read(true)
            .open(path)?;
        Ok(Self { file })
    }
}
<span class="boring">}</span></code></pre></pre>
<p>Now we have an <strong>append-only file</strong> ready to store our operations.</p>
<blockquote>
<p>You’ve created the backbone of your WAL file system.</p>
</blockquote>
<hr />
<h2 id="step-4-writing-and-flushing-entries"><a class="header" href="#step-4-writing-and-flushing-entries">Step 4: Writing and Flushing Entries</a></h2>
<p>Let’s write entries in a way that ensures <strong>durability</strong>.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl Wal {
    pub fn append(&amp;mut self, data: &amp;[u8]) -&gt; std::io::Result&lt;()&gt; {
        let len = data.len() as u32;
        self.file.write_all(&amp;len.to_le_bytes())?;
        self.file.write_all(data)?;
        self.file.flush()?; // Ensures durability
        Ok(())
    }
}
<span class="boring">}</span></code></pre></pre>
<h3 id="the-process"><a class="header" href="#the-process">The Process</a></h3>
<ol>
<li>Convert the length to bytes (<code>u32 → [u8; 4]</code>)</li>
<li>Write the length prefix.</li>
<li>Write the payload.</li>
<li>Flush to ensure data reaches disk.</li>
</ol>
<hr />
<h3 id="on-disk-after-two-writes"><a class="header" href="#on-disk-after-two-writes">On Disk After Two Writes</a></h3>
<pre><code>Offset →
0        4        10       14
│        │        │        │
▼        ▼        ▼        ▼
[06][set x=1][06][set y=2]
</code></pre>
<p>Each <code>[06]</code> = 6-byte payload length prefix.</p>
<blockquote>
<p>If we crash right after writing <code>set y=2</code> but before flushing,
only the first record will be replayed — because the second might not be fully on disk.</p>
</blockquote>
<hr />
<h2 id="step-5-reading-and-replaying-the-wal"><a class="header" href="#step-5-reading-and-replaying-the-wal">Step 5: Reading and Replaying the WAL</a></h2>
<p>Now we add the <strong>replay mechanism</strong>, which reads the log after a crash.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl Wal {
    pub fn replay&lt;P: AsRef&lt;Path&gt;&gt;(path: P) -&gt; std::io::Result&lt;Vec&lt;Vec&lt;u8&gt;&gt;&gt; {
        let mut file = File::open(path)?;
        let mut entries = Vec::new();

        loop {
            let mut len_buf = [0u8; 4];
            if file.read_exact(&amp;mut len_buf).is_err() {
                break; // EOF or partial record
            }
            let len = u32::from_le_bytes(len_buf);
            let mut data = vec![0u8; len as usize];
            if file.read_exact(&amp;mut data).is_err() {
                break; // incomplete write
            }
            entries.push(data);
        }

        Ok(entries)
    }
}
<span class="boring">}</span></code></pre></pre>
<hr />
<h3 id="what-replay-does"><a class="header" href="#what-replay-does">What Replay Does</a></h3>
<pre><code>┌────────────────────────────────────────┐
│ WAL File: [len][payload][len][payload] │
└────────────────────────────────────────┘
           │
           ▼
   Read entry by entry
           │
           ▼
   Apply or print each recovered record
</code></pre>
<p>If a crash cut off an entry halfway, the replay function simply stops reading — keeping recovery <strong>safe and idempotent</strong>.</p>
<hr />
<h2 id="step-6-testing-it-all-together"><a class="header" href="#step-6-testing-it-all-together">Step 6: Testing It All Together</a></h2>
<pre><pre class="playground"><code class="language-rust">fn main() -&gt; std::io::Result&lt;()&gt; {
    let path = "wal.log";

    // Write
    let mut wal = Wal::open(path)?;
    wal.append(b"insert key1=value1")?;
    wal.append(b"update key1=value2")?;

    // Simulate crash (just reopen)
    let recovered = Wal::replay(path)?;
    for entry in recovered {
        println!("Replayed: {}", String::from_utf8_lossy(&amp;entry));
    }

    Ok(())
}</code></pre></pre>
<p>Output:</p>
<pre><code>Replayed: insert key1=value1
Replayed: update key1=value2
</code></pre>
<hr />
<h3 id="visualization-full-lifecycle"><a class="header" href="#visualization-full-lifecycle">Visualization: Full Lifecycle</a></h3>
<pre><code>Write → Flush → Crash → Replay

┌──────────────┐      ┌──────────────┐
│ append(data) │ ---&gt; │ flush()      │
└──────────────┘      └──────────────┘
        │                    │
        │ Crash occurs!      │
        ▼                    ▼
┌──────────────────────────────────────┐
│ WAL file on disk survives crash      │
│ Replay reads all complete entries    │
└──────────────────────────────────────┘
</code></pre>
<blockquote>
<p><strong>Congratulations!</strong>
You’ve implemented a crash-safe, minimal write-ahead log that can recover all committed data.</p>
</blockquote>
<hr />
<h2 id="step-7-reflection-and-next-steps"><a class="header" href="#step-7-reflection-and-next-steps">Step 7: Reflection and Next Steps</a></h2>
<p>You now have:</p>
<ul>
<li>A simple append-only log format.</li>
<li>Durable writes with flushing.</li>
<li>Replay logic to restore data.</li>
<li>A practical understanding of what <em>“write-ahead”</em> really means.</li>
</ul>
<p>But this version isn’t perfect — it grows forever and flushes every write, which is slow.</p>
<h3 id="coming-up-next"><a class="header" href="#coming-up-next">Coming Up Next</a></h3>
<p>In the next chapters, we’ll:</p>
<ul>
<li><strong>Handle crashes more efficiently</strong> (Chapter 5 - <em>Crash Recovery</em>).</li>
<li><strong>Compact logs</strong> to free disk space (Chapter 6 - <em>Checkpointing</em>).</li>
<li><strong>Support concurrent writes safely</strong> (Chapter 7 - <em>Concurrency</em>).</li>
<li><strong>Build a complete Rust WAL library</strong> (Chapter 8 - <em>Final Implementation</em>).</li>
</ul>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-4-designing-a-minimal-wal-c-version"><a class="header" href="#chapter-4-designing-a-minimal-wal-c-version">Chapter 4: Designing a Minimal WAL (C# Version)</a></h1>
<p>I got a request to write the chapter in C#. So, I rewrote this using LLMs. If needed, will add a Java version as well later. If you understand the concept explained in the previous
Rust version, you can skip this chapter as this is just the same chapater copy pasted with C# code.</p>
<p>In this tutorial, we’ll build a <strong>minimal Write-Ahead Log (WAL)</strong> in <strong>C#</strong>, step by step.</p>
<p>By the end, you’ll have:</p>
<ul>
<li>A durable log file that survives crashes.</li>
<li>Append and replay functionality.</li>
<li>A foundation for recovery, checkpoints, and concurrency in later chapters.</li>
</ul>
<hr />
<h2 id="step-1-understanding-the-goal"><a class="header" href="#step-1-understanding-the-goal">Step 1: Understanding the Goal</a></h2>
<p>A Write-Ahead Log ensures that <strong>every change is recorded to disk before being applied</strong>.</p>
<p>Think of it as your safety net:</p>
<pre><code>┌──────────────┐        ┌──────────────┐
│  Operation   │        │   Database   │
│ (e.g. update)│        │   State      │
└──────┬───────┘        └──────┬───────┘
        │                        │
        ▼                        │
   Write to WAL File              │
        │                        │
        ▼                        │
   Flush to Disk (Durable)        │
        │                        │
        └────────► Apply to DB ◄──┘
</code></pre>
<p>Rule:</p>
<blockquote>
<p><strong>No modification happens unless it’s safely logged first.</strong></p>
</blockquote>
<hr />
<h2 id="step-2-log-record-format"><a class="header" href="#step-2-log-record-format">Step 2: Log Record Format</a></h2>
<p>We’ll store each record with a <strong>4-byte length prefix</strong> followed by the actual payload bytes.</p>
<pre><code>+------------+-------------------+
| 4 bytes    | variable length   |
| length (u32) | payload bytes   |
+------------+-------------------+
</code></pre>
<p>This allows us to detect incomplete entries after a crash.</p>
<hr />
<h3 id="example-1"><a class="header" href="#example-1">Example</a></h3>
<p>If we write two entries: <code>set x=1</code> and <code>set y=2</code>,
the file might look like this (in conceptual view):</p>
<pre><code>┌──────────────────────────────────────────────┐
│ [06][set x=1][06][set y=2]                  │
└──────────────────────────────────────────────┘
</code></pre>
<hr />
<h2 id="step-3-implementing-the-wal-class"><a class="header" href="#step-3-implementing-the-wal-class">Step 3: Implementing the WAL Class</a></h2>
<p>Let’s start with the class definition and basic setup.</p>
<pre><code class="language-csharp">using System;
using System.Collections.Generic;
using System.IO;
using System.Text;

public class WriteAheadLog : IDisposable
{
    private readonly FileStream _fileStream;
    private readonly BinaryWriter _writer;

    public string FilePath { get; }

    public WriteAheadLog(string path)
    {
        FilePath = path;
        _fileStream = new FileStream(
            path,
            FileMode.OpenOrCreate,
            FileAccess.ReadWrite,
            FileShare.Read,
            4096,
            FileOptions.WriteThrough // ensures OS-level durability
        );
        _fileStream.Seek(0, SeekOrigin.End); // append mode
        _writer = new BinaryWriter(_fileStream, Encoding.UTF8, leaveOpen: true);
    }

    public void Dispose()
    {
        _writer?.Dispose();
        _fileStream?.Dispose();
    }
}
</code></pre>
<blockquote>
<p>You’ve built the foundation:
an append-only, durable file with OS-level write-through mode.</p>
</blockquote>
<hr />
<h2 id="step-4-appending-entries"><a class="header" href="#step-4-appending-entries">Step 4: Appending Entries</a></h2>
<p>Now we add the <code>Append()</code> method.</p>
<pre><code class="language-csharp">public void Append(string data)
{
    var bytes = Encoding.UTF8.GetBytes(data);
    var length = (uint)bytes.Length;

    // Write length prefix
    _writer.Write(length);

    // Write payload
    _writer.Write(bytes);

    // Flush ensures data hits disk
    _writer.Flush();
    _fileStream.Flush(true); // flush to physical media
}
</code></pre>
<p>Each append guarantees:</p>
<ol>
<li>Length prefix and payload are written atomically.</li>
<li>Flush ensures durability.</li>
</ol>
<hr />
<h3 id="file-layout-after-two-appends"><a class="header" href="#file-layout-after-two-appends">File Layout After Two Appends</a></h3>
<pre><code>Offset →
0        4        10       14
│        │        │        │
▼        ▼        ▼        ▼
[06][set x=1][06][set y=2]
</code></pre>
<blockquote>
<p><strong>Now you have a WAL that guarantees data won’t be lost even if the app crashes.</strong></p>
</blockquote>
<hr />
<h2 id="step-5-replaying-the-wal"><a class="header" href="#step-5-replaying-the-wal">Step 5: Replaying the WAL</a></h2>
<p>Recovery works by reading each complete entry from the file.</p>
<pre><code class="language-csharp">public static IEnumerable&lt;string&gt; Replay(string path)
{
    var results = new List&lt;string&gt;();
    if (!File.Exists(path))
        return results;

    using var fs = new FileStream(path, FileMode.Open, FileAccess.Read, FileShare.Read);
    using var reader = new BinaryReader(fs, Encoding.UTF8);

    while (fs.Position &lt; fs.Length)
    {
        try
        {
            uint len = reader.ReadUInt32();
            byte[] data = reader.ReadBytes((int)len);

            // if data is incomplete, stop replay
            if (data.Length &lt; len)
                break;

            results.Add(Encoding.UTF8.GetString(data));
        }
        catch (EndOfStreamException)
        {
            break; // crashed mid-write
        }
    }

    return results;
}
</code></pre>
<hr />
<h3 id="recovery-visualization"><a class="header" href="#recovery-visualization">Recovery Visualization</a></h3>
<pre><code>┌────────────────────────────────────────┐
│ WAL File: [len][payload][len][payload] │
└────────────────────────────────────────┘
           │
           ▼
   Read entry by entry
           │
           ▼
   Replay operations in order
</code></pre>
<p>If a crash occurred mid-write, the partial record at the end is safely ignored.</p>
<blockquote>
<p>You’ve implemented <strong>crash-safe recovery</strong> logic.</p>
</blockquote>
<hr />
<h2 id="step-6-testing-it-all-together-1"><a class="header" href="#step-6-testing-it-all-together-1">Step 6: Testing It All Together</a></h2>
<p>Let’s test everything in a simple <code>Main()</code>:</p>
<pre><code class="language-csharp">public static void Main()
{
    const string path = "wal.log";

    // Step 1: Write some entries
    using (var wal = new WriteAheadLog(path))
    {
        wal.Append("insert key1=value1");
        wal.Append("update key1=value2");
    }

    // Step 2: Simulate crash → reopen and replay
    var recovered = WriteAheadLog.Replay(path);
    foreach (var entry in recovered)
    {
        Console.WriteLine($"Replayed: {entry}");
    }
}
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Replayed: insert key1=value1
Replayed: update key1=value2
</code></pre>
<hr />
<h2 id="step-7-visualizing-the-full-lifecycle"><a class="header" href="#step-7-visualizing-the-full-lifecycle">Step 7: Visualizing the Full Lifecycle</a></h2>
<pre><code>Write → Flush → Crash → Replay

┌──────────────┐      ┌──────────────┐
│ wal.Append() │ ---&gt; │ Flush to Disk│
└──────────────┘      └──────────────┘
        │                    │
        │  Crash occurs!     │
        ▼                    ▼
┌──────────────────────────────────────┐
│ WAL file on disk survives crash      │
│ Replay reads all complete entries    │
└──────────────────────────────────────┘
</code></pre>
<blockquote>
<p><strong>At this point:</strong>
You’ve built a fully functional minimal WAL in <strong>C#</strong> — durable, replay-safe, and extendable.</p>
</blockquote>
<hr />
<h2 id="step-8-reflection-and-next-steps"><a class="header" href="#step-8-reflection-and-next-steps">Step 8: Reflection and Next Steps</a></h2>
<p>What you’ve accomplished:</p>
<ul>
<li>Built a minimal append-only WAL file.</li>
<li>Achieved durability using <code>Flush()</code> and <code>FileOptions.WriteThrough</code>.</li>
<li>Implemented safe crash recovery via replay.</li>
</ul>
<h3 id="whats-next"><a class="header" href="#whats-next">What’s Next</a></h3>
<p>In the coming chapters, we’ll:</p>
<ul>
<li><strong>Chapter 5:</strong> Implement structured crash recovery and consistency checks.</li>
<li><strong>Chapter 6:</strong> Add checkpointing and log compaction to manage file growth.</li>
<li><strong>Chapter 7:</strong> Handle concurrent writes safely.</li>
<li><strong>Chapter 8:</strong> Build a full-featured WAL in Rust (and compare with this C# version).</li>
</ul>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-5-crash-recovery"><a class="header" href="#chapter-5-crash-recovery">Chapter 5: Crash Recovery</a></h1>
<p>When you first implement a Write-Ahead Log, you might think the hard part is done - after all, every change is recorded safely before it’s applied to data. But the <em>real</em> test of a WAL comes when the system crashes.</p>
<p>This chapter is all about recovery: how a database uses the WAL to restore itself to a consistent state after a crash. You’ll see how to reconstruct committed transactions, discard incomplete ones, and make sure no data is lost - all automatically, on restart.</p>
<hr />
<h2 id="step-1-why-crash-recovery-matters"><a class="header" href="#step-1-why-crash-recovery-matters">Step 1: Why Crash Recovery Matters</a></h2>
<p>Imagine your database is running smoothly, applying transactions and appending log records. Then suddenly the system crashes. When it comes back online, you have two kinds of persistent data:</p>
<ol>
<li><strong>Data files</strong> – may not include recent writes (since they’re buffered in memory).</li>
<li><strong>WAL file</strong> – includes a complete, durable record of all operations.</li>
</ol>
<p>That WAL is your lifeline. It remembers what the database <em>intended</em> to do, even if it didn’t finish doing it. Crash recovery’s job is to make sure those intentions are faithfully completed - or safely undone.</p>
<hr />
<h2 id="step-2-recovery-goals"><a class="header" href="#step-2-recovery-goals">Step 2: Recovery Goals</a></h2>
<p>Crash recovery must satisfy two of the four core database guarantees:</p>
<ul>
<li><strong>Durability:</strong> Committed transactions must survive crashes.</li>
<li><strong>Atomicity:</strong> Uncommitted transactions must be rolled back completely.</li>
</ul>
<p>To achieve this, our recovery logic will:</p>
<ol>
<li><strong>Redo</strong> the effects of committed transactions that weren’t fully written to disk.</li>
<li><strong>Undo</strong> the effects of incomplete or aborted transactions.</li>
</ol>
<p>You can think of recovery as a replay and rewind system - replay all good work, rewind anything half-done.</p>
<hr />
<h2 id="step-3-what-survives-a-crash"><a class="header" href="#step-3-what-survives-a-crash">Step 3: What Survives a Crash</a></h2>
<p>Let’s recap what persists across a crash:</p>
<ul>
<li>The <strong>WAL file</strong> on disk, flushed up to the last <code>fsync</code>.</li>
<li>The <strong>data files</strong>, which may be out of date.</li>
</ul>
<p>Everything else - the in-memory cache, dirty page map, and active transaction table - is gone.</p>
<p>When the system restarts, the recovery process will rebuild that lost in-memory state <em>by reading the WAL itself</em>.</p>
<hr />
<h2 id="step-4-the-recovery-process-overview"><a class="header" href="#step-4-the-recovery-process-overview">Step 4: The Recovery Process Overview</a></h2>
<p>We’ll use a simplified version of the ARIES algorithm, with three stages:</p>
<ol>
<li><strong>Analysis:</strong> Figure out which transactions were active and which pages might be dirty at crash time.</li>
<li><strong>Redo:</strong> Reapply changes for committed transactions to ensure durability.</li>
<li><strong>Undo:</strong> Roll back incomplete transactions to ensure atomicity.</li>
</ol>
<p>Here’s the pseudocode for our recovery:</p>
<pre><code class="language-text">recover():
    txTable = {}
    dirtyPages = {}

    # Phase 1: Analysis
    for record in WAL:
        update txTable based on record type
        mark dirty pages

    # Phase 2: Redo
    for record in WAL:
        if record.page.LSN &lt; record.LSN and record.txn is committed:
            apply(record)

    # Phase 3: Undo
    for record in reversed(WAL):
        if record.txn is uncommitted:
            undo(record)
</code></pre>
<hr />
<h2 id="step-5-the-analysis-phase"><a class="header" href="#step-5-the-analysis-phase">Step 5: The Analysis Phase</a></h2>
<p>The first step is scanning the WAL from the beginning to rebuild our <strong>transaction table</strong> and <strong>dirty page table</strong>.</p>
<p>Whenever you see:</p>
<ul>
<li><code>BEGIN_TXN</code> → Add transaction as <em>in-progress</em></li>
<li><code>COMMIT_TXN</code> → Mark as <em>committed</em></li>
<li><code>ABORT_TXN</code> → Mark as <em>aborted</em></li>
<li><code>UPDATE</code> → Record that the page is dirty and associate it with that transaction</li>
</ul>
<p>By the end of this scan, you know exactly which transactions were incomplete when the system crashed.</p>
<p><strong>Example:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Txn ID</th><th>State</th><th>Last LSN</th><th>Dirty Pages</th></tr></thead><tbody>
<tr><td>1</td><td>committed</td><td>152</td><td>[P3, P5]</td></tr>
<tr><td>2</td><td>in-progress</td><td>177</td><td>[P8]</td></tr>
</tbody></table>
</div>
<p>Now we’re ready to restore data to a consistent state.</p>
<hr />
<h2 id="step-6-redo-phase"><a class="header" href="#step-6-redo-phase">Step 6: Redo Phase</a></h2>
<p>Redo ensures that <em>all committed transactions</em> are reflected in the data files.</p>
<p>For each WAL record:</p>
<ol>
<li>Read the page from disk.</li>
<li>Check the page’s stored <code>pageLSN</code> (the last log record applied to it).</li>
<li>If <code>pageLSN &lt; record.LSN</code>, it means this record wasn’t applied yet - so apply the update now.</li>
<li>Update the <code>pageLSN</code> to the record’s <code>LSN</code>.</li>
</ol>
<p>This process is <strong>idempotent</strong> - if you crash again during recovery, redoing the same records won’t double-apply changes.</p>
<hr />
<h3 id="example-2"><a class="header" href="#example-2">Example</a></h3>
<p>Let’s say we have the following WAL log:</p>
<div class="table-wrapper"><table><thead><tr><th>LSN</th><th>Txn</th><th>Type</th><th>Page</th><th>Before</th><th>After</th></tr></thead><tbody>
<tr><td>100</td><td>1</td><td>BEGIN_TXN</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td>110</td><td>1</td><td>UPDATE</td><td>P3</td><td>A</td><td>B</td></tr>
<tr><td>120</td><td>2</td><td>BEGIN_TXN</td><td>-</td><td>-</td><td>-</td></tr>
<tr><td>130</td><td>2</td><td>UPDATE</td><td>P5</td><td>X</td><td>Y</td></tr>
<tr><td>140</td><td>1</td><td>COMMIT_TXN</td><td>-</td><td>-</td><td>-</td></tr>
</tbody></table>
</div>
<p>Suppose a crash occurs right after record 130 (Txn 2’s update), before 140 is fully flushed.</p>
<p>On restart:</p>
<ul>
<li>Analysis: Txn 1 = committed, Txn 2 = in-progress.</li>
<li>Redo: Apply Txn 1’s update to P3 (if not already applied).</li>
<li>Undo: Roll back Txn 2’s change on P5.</li>
</ul>
<p>The end state is consistent:</p>
<pre><code>P3 = B  (committed)
P5 = X  (rolled back)
</code></pre>
<hr />
<h2 id="step-7-undo-phase"><a class="header" href="#step-7-undo-phase">Step 7: Undo Phase</a></h2>
<p>The undo phase is the cleanup crew.
We scan the WAL backward and reverse any changes made by uncommitted transactions.</p>
<p>For each record:</p>
<ul>
<li>If it belongs to an in-progress transaction, apply its <em>before-image</em> to restore the old value.</li>
<li>Optionally, write a <strong>compensation log record (CLR)</strong> indicating that this undo action has been logged. (Our minimal design can skip this for simplicity.)</li>
</ul>
<p>When all incomplete transactions are undone, atomicity is guaranteed.</p>
<hr />
<h2 id="step-8-checkpoints-optional-optimization"><a class="header" href="#step-8-checkpoints-optional-optimization">Step 8: Checkpoints (Optional Optimization)</a></h2>
<p>Reading the entire WAL on every restart works, but it can be slow.
To speed up recovery, databases write <strong>checkpoints</strong> periodically.</p>
<p>A checkpoint records:</p>
<ul>
<li>The list of active transactions</li>
<li>The list of dirty pages</li>
<li>The last flushed LSN</li>
</ul>
<p>When the system restarts, recovery can begin from the last checkpoint instead of from the start of the log.</p>
<p>Here’s what a minimal checkpoint record might look like:</p>
<pre><code class="language-json">{
  "type": "CHECKPOINT",
  "active_txns": [2, 3],
  "dirty_pages": ["P8", "P9"],
  "last_lsn": 150
}
</code></pre>
<hr />
<h2 id="step-9-implementing-recovery-in-code-c-example"><a class="header" href="#step-9-implementing-recovery-in-code-c-example">Step 9: Implementing Recovery in Code (C# Example)</a></h2>
<p>Below is a minimal conceptual version of the recovery logic.
It assumes you already have a <code>WALRecord</code> struct and a way to read pages from disk.</p>
<pre><code class="language-csharp">public class RecoveryManager
{
    private readonly IPageStore _pageStore;
    private readonly IEnumerable&lt;WALRecord&gt; _log;

    public RecoveryManager(IPageStore pageStore, IEnumerable&lt;WALRecord&gt; log)
    {
        _pageStore = pageStore;
        _log = log;
    }

    public void Recover()
    {
        var txState = new Dictionary&lt;long, string&gt;(); // txnId -&gt; state

        // Phase 1: Analysis
        foreach (var rec in _log)
        {
            switch (rec.Type)
            {
                case WALRecordType.Begin:
                    txState[rec.TxnId] = "active";
                    break;
                case WALRecordType.Commit:
                    txState[rec.TxnId] = "committed";
                    break;
                case WALRecordType.Abort:
                    txState[rec.TxnId] = "aborted";
                    break;
            }
        }

        // Phase 2: Redo
        foreach (var rec in _log)
        {
            if (rec.Type == WALRecordType.Update &amp;&amp;
                txState.TryGetValue(rec.TxnId, out var state) &amp;&amp;
                state == "committed")
            {
                var page = _pageStore.ReadPage(rec.PageId);
                if (page.LSN &lt; rec.LSN)
                {
                    page.Apply(rec.AfterImage);
                    page.LSN = rec.LSN;
                    _pageStore.WritePage(page);
                }
            }
        }

        // Phase 3: Undo
        foreach (var rec in _log.Reverse())
        {
            if (rec.Type == WALRecordType.Update &amp;&amp;
                txState.TryGetValue(rec.TxnId, out var state) &amp;&amp;
                state == "active")
            {
                var page = _pageStore.ReadPage(rec.PageId);
                page.Apply(rec.BeforeImage);
                _pageStore.WritePage(page);
            }
        }

        Console.WriteLine("Recovery complete!");
    }
}
</code></pre>
<p>This implementation is minimal but captures the essence:</p>
<ul>
<li>Replays committed work.</li>
<li>Reverses incomplete transactions.</li>
<li>Keeps idempotence through <code>page.LSN</code> checks.</li>
</ul>
<hr />
<h2 id="step-10-wrapping-up"><a class="header" href="#step-10-wrapping-up">Step 10: Wrapping Up</a></h2>
<p>Crash recovery completes the story that began with Write-Ahead Logging.
Together, they guarantee that your database can crash at <em>any</em> point and still come back consistent.</p>
<p>By now, your system:</p>
<ul>
<li>Writes intent before data (WAL).</li>
<li>Replays or rolls back intent after crashes (Recovery).</li>
</ul>
<p>In the next chapter, we’ll focus on <strong>checkpointing and log truncation</strong> - the final step in keeping your WAL lean, your recovery fast, and your storage healthy.</p>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-6-checkpointing-and-log-compaction"><a class="header" href="#chapter-6-checkpointing-and-log-compaction">Chapter 6: Checkpointing and Log Compaction</a></h1>
<p>By now, you’ve built a database that can <strong>survive crashes</strong> using a Write-Ahead Log and <strong>recover</strong> correctly.
But if you let your system run long enough, your WAL file will just keep growing - indefinitely.</p>
<p>Every transaction, every update, every byte of history lives on in that log, even after it’s no longer needed.
Soon, you’ll have gigabytes of old, irrelevant log records sitting on disk.</p>
<p>That’s where <strong>checkpointing</strong> and <strong>log compaction</strong> come in.</p>
<hr />
<h2 id="step-1-the-problem---infinite-logs"><a class="header" href="#step-1-the-problem---infinite-logs">Step 1: The Problem - Infinite Logs</a></h2>
<p>Imagine your WAL has been running for hours:</p>
<pre><code>LSN 100: BEGIN_TXN 1
LSN 110: UPDATE page 3
LSN 120: COMMIT_TXN 1
...
LSN 10,000,000: UPDATE page 8
</code></pre>
<p>The system is fine, but recovery is getting slower - because every crash means replaying millions of records.
Even though most of those updates have long been persisted to disk, we’re still keeping them “just in case.”</p>
<p>We need a way to:</p>
<ol>
<li>Persist a consistent snapshot of the system state.</li>
<li>Discard old WAL entries that are no longer required for recovery.</li>
</ol>
<p>That’s exactly what a <strong>checkpoint</strong> does.</p>
<hr />
<h2 id="step-2-what-is-a-checkpoint"><a class="header" href="#step-2-what-is-a-checkpoint">Step 2: What Is a Checkpoint?</a></h2>
<p>A <strong>checkpoint</strong> is a point in time where we guarantee:</p>
<blockquote>
<p>“All updates up to this LSN are reflected on disk.”</p>
</blockquote>
<p>When a checkpoint is taken, we flush all dirty pages to disk, record which transactions are still active, and write a <em>checkpoint record</em> to the WAL.</p>
<p>Later, if a crash happens, recovery can skip all log records before the last checkpoint - because we know the data files already contain those effects.</p>
<hr />
<h2 id="step-3-checkpoint-contents"><a class="header" href="#step-3-checkpoint-contents">Step 3: Checkpoint Contents</a></h2>
<p>A checkpoint record usually includes:</p>
<ul>
<li><strong>Last LSN flushed to disk</strong></li>
<li><strong>List of active transactions</strong></li>
<li><strong>Dirty page table</strong> (pages modified but not yet flushed)</li>
</ul>
<p>In our minimal WAL, we’ll store a simple JSON structure like:</p>
<pre><code class="language-json">{
  "type": "CHECKPOINT",
  "active_txns": [3, 4],
  "dirty_pages": [7, 9],
  "last_lsn": 900
}
</code></pre>
<p>This record marks a safe recovery starting point.
When recovery begins, it finds the last checkpoint and starts replaying from there instead of from LSN 0.</p>
<hr />
<h2 id="step-4-how-to-take-a-checkpoint"><a class="header" href="#step-4-how-to-take-a-checkpoint">Step 4: How to Take a Checkpoint</a></h2>
<p>To take a checkpoint safely:</p>
<ol>
<li><strong>Pause new writes</strong> (briefly or via a lightweight lock).</li>
<li><strong>Flush all dirty pages</strong> to disk.</li>
<li><strong>Record current active transactions and their last LSNs.</strong></li>
<li><strong>Append a checkpoint record</strong> to the WAL.</li>
<li><strong>Flush the WAL</strong> to ensure durability.</li>
<li><strong>Resume normal operations.</strong></li>
</ol>
<p>Here’s the pseudocode:</p>
<pre><code class="language-text">take_checkpoint():
    lock system
    flush all dirty pages
    record active_txns and last_lsn
    append CHECKPOINT record to WAL
    fsync WAL
    unlock system
</code></pre>
<hr />
<h2 id="step-5-log-compaction-truncation"><a class="header" href="#step-5-log-compaction-truncation">Step 5: Log Compaction (Truncation)</a></h2>
<p>Once we’ve written a checkpoint and ensured all earlier updates are persisted,
<strong>older WAL entries become redundant</strong>. We can safely delete or truncate them.</p>
<p>We call this <strong>log compaction</strong> or <strong>log truncation</strong>.</p>
<ul>
<li>The log can be truncated up to the last checkpoint’s LSN.</li>
<li>Recovery will never need those earlier records again.</li>
</ul>
<p>This keeps our log file size bounded and ensures faster recovery.</p>
<hr />
<h2 id="step-6-example-timeline"><a class="header" href="#step-6-example-timeline">Step 6: Example Timeline</a></h2>
<p>Let’s visualize how checkpointing fits into the life of a running database:</p>
<pre><code>Time → →
| Txn A Updates | Txn A Commits | Txn B Starts | Checkpoint | Txn B Updates | Crash |
|----------------|---------------|---------------|-------------|---------------|-------|

During recovery:
  - Start from last checkpoint
  - Redo only log records after checkpoint
  - Undo incomplete Txn B if necessary
</code></pre>
<p>If we didn’t have checkpoints, recovery would have to scan the entire log from the beginning - much slower.</p>
<hr />
<h2 id="step-7-checkpointing-in-code-rust-example"><a class="header" href="#step-7-checkpointing-in-code-rust-example">Step 7: Checkpointing in Code (Rust Example)</a></h2>
<p>Here’s a simple demonstration of checkpoint creation and log truncation logic,
extending the recovery manager from the previous chapter.</p>
<pre><pre class="playground"><code class="language-rust">use std::collections::{HashMap, HashSet};
use std::fs::{File, OpenOptions};
use std::io::{Seek, SeekFrom, Write};

type Lsn = u64;
type TxnId = u64;
type PageId = u64;

#[derive(Clone, Debug)]
struct CheckpointRecord {
    last_lsn: Lsn,
    active_txns: HashSet&lt;TxnId&gt;,
    dirty_pages: HashSet&lt;PageId&gt;,
}

impl CheckpointRecord {
    fn serialize(&amp;self) -&gt; String {
        format!(
            "{{\"type\":\"CHECKPOINT\",\"last_lsn\":{},\"active_txns\":{:?},\"dirty_pages\":{:?}}}",
            self.last_lsn, self.active_txns, self.dirty_pages
        )
    }
}

/// A minimal WAL manager that supports writing, checkpointing, and truncation.
struct WalManager {
    wal_path: String,
    next_lsn: Lsn,
    active_txns: HashSet&lt;TxnId&gt;,
    dirty_pages: HashSet&lt;PageId&gt;,
}

impl WalManager {
    fn new(wal_path: &amp;str) -&gt; Self {
        Self {
            wal_path: wal_path.to_string(),
            next_lsn: 1,
            active_txns: HashSet::new(),
            dirty_pages: HashSet::new(),
        }
    }

    fn append(&amp;mut self, record: &amp;str) {
        let mut file = OpenOptions::new()
            .create(true)
            .append(true)
            .open(&amp;self.wal_path)
            .expect("Failed to open WAL file");

        writeln!(file, "{}", record).expect("Write WAL failed");
        file.flush().unwrap();
        self.next_lsn += 1;
    }

    fn take_checkpoint(&amp;mut self) -&gt; CheckpointRecord {
        // Simulate flushing all dirty pages
        println!("Flushing {} dirty pages to disk...", self.dirty_pages.len());

        // Create checkpoint record
        let chk = CheckpointRecord {
            last_lsn: self.next_lsn,
            active_txns: self.active_txns.clone(),
            dirty_pages: self.dirty_pages.clone(),
        };

        // Write checkpoint to WAL
        let mut file = OpenOptions::new()
            .create(true)
            .append(true)
            .open(&amp;self.wal_path)
            .unwrap();

        writeln!(file, "{}", chk.serialize()).unwrap();
        file.flush().unwrap();

        println!("Checkpoint created at LSN {}", chk.last_lsn);
        chk
    }

    fn truncate_log(&amp;mut self, checkpoint: &amp;CheckpointRecord) {
        // For simplicity, we just rewrite WAL file starting from checkpoint.
        // In real DBs, you'd keep the tail after checkpoint and delete older data.
        println!("Truncating WAL up to LSN {}", checkpoint.last_lsn);

        let mut file = File::create(&amp;self.wal_path).unwrap();
        writeln!(file, "{}", checkpoint.serialize()).unwrap();
        file.flush().unwrap();
    }
}

fn main() {
    let mut wal = WalManager::new("wal.log");
    wal.active_txns.insert(1);
    wal.dirty_pages.insert(3);
    wal.dirty_pages.insert(5);

    wal.append("{\"type\":\"BEGIN_TXN\",\"txn\":1}");
    wal.append("{\"type\":\"UPDATE\",\"txn\":1,\"page\":3}");
    wal.append("{\"type\":\"COMMIT_TXN\",\"txn\":1}");

    let chk = wal.take_checkpoint();
    wal.truncate_log(&amp;chk);
}</code></pre></pre>
<h3 id="whats-happening"><a class="header" href="#whats-happening">What’s happening:</a></h3>
<ul>
<li>We log a few records.</li>
<li>We flush dirty pages and write a checkpoint record.</li>
<li>We truncate the WAL, keeping only the checkpoint forward.</li>
</ul>
<p>This simple mechanism ensures your WAL file stays compact and recovery starts quickly.</p>
<hr />
<h2 id="step-8-checkpoint-frequency"><a class="header" href="#step-8-checkpoint-frequency">Step 8: Checkpoint Frequency</a></h2>
<p>How often should you checkpoint?
It’s a trade-off:</p>
<div class="table-wrapper"><table><thead><tr><th>Frequency</th><th>Pros</th><th>Cons</th></tr></thead><tbody>
<tr><td>Frequent (every few seconds)</td><td>Faster recovery</td><td>Higher runtime overhead</td></tr>
<tr><td>Infrequent (every few minutes)</td><td>Lower overhead</td><td>Longer recovery time</td></tr>
</tbody></table>
</div>
<p>Many databases use a hybrid strategy - <strong>periodic checkpoints</strong> plus <strong>event-based checkpoints</strong> when the log grows beyond a certain threshold.</p>
<hr />
<h2 id="step-9-putting-it-all-together"><a class="header" href="#step-9-putting-it-all-together">Step 9: Putting It All Together</a></h2>
<p>At this point, your database engine now supports:</p>
<ul>
<li><strong>Write-Ahead Logging</strong> - ensuring durability.</li>
<li><strong>Crash Recovery</strong> - guaranteeing atomicity.</li>
<li><strong>Checkpointing and Compaction</strong> - keeping the system lean and fast.</li>
</ul>
<p>These three pillars form the foundation of any reliable storage engine.</p>
<hr />
<h2 id="step-10-next-steps"><a class="header" href="#step-10-next-steps">Step 10: Next Steps</a></h2>
<p>Now that we’ve built durability and recovery, we can explore:</p>
<ul>
<li><strong>Concurrency Control</strong> - locks, latches, and isolation.</li>
<li><strong>Buffer Management</strong> - page replacement and pinning.</li>
<li><strong>Replication</strong> - applying WAL across nodes for fault tolerance.</li>
</ul>
<p>But even at this stage, you’ve implemented something profound - a <strong>recoverable transactional storage system</strong>, built from first principles.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-7---concurrency-and-wal"><a class="header" href="#chapter-7---concurrency-and-wal">Chapter 7 - Concurrency and WAL</a></h1>
<p>Concurrency is where correctness and performance collide. In a single-threaded toy WAL you can append records safely, but real systems must handle many concurrent transactions, background flushers, checkpointing, and possibly replication - all touching the WAL. This chapter explains practical, production-proven patterns and shows C# examples so you can experiment and reason about trade-offs.</p>
<hr />
<h2 id="71-core-concurrency-goals-for-a-wal"><a class="header" href="#71-core-concurrency-goals-for-a-wal">7.1 Core concurrency goals for a WAL</a></h2>
<p>When multiple threads/processes interact with the WAL we want to ensure:</p>
<ol>
<li><strong>Atomicity of appends</strong> - a single log record must be written atomically (no interleaving bytes from different writers).</li>
<li><strong>Ordering</strong> - the on-disk order of records must match the logical order of operations (or at least the order required to preserve correctness).</li>
<li><strong>Durability semantics</strong> - when a transaction commits, the WAL strategy must ensure its durability guarantees (e.g., durable after fsync, or "probably durable" if async).</li>
<li><strong>Performance</strong> - high throughput and acceptable latency under concurrency.</li>
</ol>
<p>These goals pull in opposite directions: stronger durability (fsync every commit) harms throughput; batching improves throughput but increases commit latency.</p>
<hr />
<h2 id="72-common-production-approaches-what-other-databases-do"><a class="header" href="#72-common-production-approaches-what-other-databases-do">7.2 Common production approaches (what other databases do)</a></h2>
<p>Short survey - patterns used by real DBs:</p>
<ul>
<li>
<p><strong>Single-writer + WAL writer thread</strong></p>
<ul>
<li><em>PostgreSQL</em> uses a WAL writer and a dedicated background process that helps flush WAL segments; the WAL insertion is serialized (though multiple backends can prepare WAL records, actual fsync/commit ordering is coordinated). Postgres implements <strong>group commit</strong> to batch multiple commits into a single fsync.</li>
</ul>
</li>
<li>
<p><strong>Group commit</strong></p>
<ul>
<li>Many DBs (Postgres, MySQL InnoDB via <code>innodb_flush_log_at_trx_commit</code> variations) accumulate commit requests and do a single fsync for several transactions - trading a bound on latency for much higher throughput.</li>
</ul>
</li>
<li>
<p><strong>Buffered/appender thread</strong></p>
<ul>
<li>A single thread accepts append requests (from other worker threads via a queue) and writes to disk, ensuring ordering and atomic writes while avoiding per-thread file locking cost.</li>
</ul>
</li>
<li>
<p><strong>Doublewrite / safe page writes</strong></p>
<ul>
<li><em>InnoDB</em> uses a doublewrite buffer to avoid torn page problems on partial page writes.</li>
</ul>
</li>
<li>
<p><strong>Force vs lazy flush</strong></p>
<ul>
<li>Some systems allow synchronous durability (<code>fsync</code> at commit) or relaxed modes where commits are acknowledged before <code>fsync</code> (risking data loss on crash).</li>
</ul>
</li>
<li>
<p><strong>Direct I/O / O_DSYNC / Write barriers</strong></p>
<ul>
<li>Low-level options to control caching and ordering for stronger guarantees.</li>
</ul>
</li>
<li>
<p><strong>Checksumming and LSNs</strong></p>
<ul>
<li>Records include LSNs and checksums; pages store page LSNs for idempotent Redo.</li>
</ul>
</li>
<li>
<p><strong>Segmented WAL + log shipping</strong></p>
<ul>
<li>WAL is layered into segments to allow safe rotation and replication (Postgres, Oracle).</li>
</ul>
</li>
<li>
<p><strong>LSM vs B-Tree differences</strong></p>
<ul>
<li>LSM-based systems (RocksDB) write WAL entries and keep memtables in memory; compaction and memtable flush strategies interact with WAL flushing.</li>
</ul>
</li>
</ul>
<p>We’ll use simplified patterns inspired by these techniques.</p>
<hr />
<h2 id="73-approach-1---simple-mutex-guarded-append-safe--easy"><a class="header" href="#73-approach-1---simple-mutex-guarded-append-safe--easy">7.3 Approach 1 - Simple mutex-guarded append (safe &amp; easy)</a></h2>
<p>This is the simplest correct approach: a single lock around file writes. It’s easy to reason about and safe across threads, but it serializes writers and will limit throughput.</p>
<pre><code class="language-csharp">// SimpleWAL.cs
using System;
using System.IO;
using System.Text;
using System.Threading;

public class SimpleWAL : IDisposable
{
    private readonly FileStream _fs;
    private readonly object _writeLock = new object();

    public SimpleWAL(string path)
    {
        // FileOptions.WriteThrough reduces caching, but platform behavior varies.
        _fs = new FileStream(path, FileMode.Append, FileAccess.Write, FileShare.Read, 4096, FileOptions.WriteThrough);
    }

    // Append a single log entry and optionally flush to disk (durable if flush==true).
    public void Append(string jsonRecord, bool flush = true)
    {
        var bytes = Encoding.UTF8.GetBytes(jsonRecord + "\n");
        lock (_writeLock)
        {
            _fs.Write(bytes, 0, bytes.Length);
            if (flush)
            {
                // Ensure persistence. On modern .NET, Flush(true) requests OS-level flush.
                _fs.Flush(true); // flush metadata &amp; data to disk (may throw on unsupported platforms)
            }
        }
    }

    public void Dispose()
    {
        _fs?.Dispose();
    }
}
</code></pre>
<p><strong>When to use:</strong> small systems, low concurrency, or as a reference implementation.
<strong>Pros:</strong> trivial correctness.
<strong>Cons:</strong> blocks all writers on IO latency.</p>
<hr />
<h2 id="74-approach-2---buffering--background-flusher-higher-throughput-tuned-durability"><a class="header" href="#74-approach-2---buffering--background-flusher-higher-throughput-tuned-durability">7.4 Approach 2 - Buffering + background flusher (higher throughput, tuned durability)</a></h2>
<p>A more common production pattern: worker threads enqueue log entries into an in-memory queue; a single background thread drains the queue to disk (preserving order) and performs periodic or batched <code>fsync</code>s. This gives you high throughput and allows you to implement <strong>group commit</strong> cheaply.</p>
<p>Key design points:</p>
<ul>
<li>Writers don't wait on disk I/O (reduced latency).</li>
<li>Periodic flush or batch-on-commit allows many transactions to share one expensive <code>fsync</code>.</li>
<li>You must decide when a caller is allowed to consider a transaction "committed" (before or after fsync).</li>
</ul>
<p>Example:</p>
<pre><code class="language-csharp">// BufferedWAL.cs
using System;
using System.Collections.Concurrent;
using System.IO;
using System.Text;
using System.Threading;
using System.Threading.Tasks;

public class BufferedWAL : IDisposable
{
    private readonly BlockingCollection&lt;WalEntry&gt; _queue = new BlockingCollection&lt;WalEntry&gt;(new ConcurrentQueue&lt;WalEntry&gt;());
    private readonly FileStream _fs;
    private readonly Thread _flusherThread;
    private readonly TimeSpan _flushInterval;
    private volatile bool _running = true;

    public BufferedWAL(string path, TimeSpan flushInterval)
    {
        _fs = new FileStream(path, FileMode.Append, FileAccess.Write, FileShare.Read, 4096, FileOptions.None);
        _flushInterval = flushInterval;
        _flusherThread = new Thread(FlusherLoop) { IsBackground = true };
        _flusherThread.Start();
    }

    public void Enqueue(string record, TaskCompletionSource&lt;bool&gt; tcs = null)
    {
        _queue.Add(new WalEntry { Record = record, Tcs = tcs });
    }

    // Call this for "fire-and-forget". For durability-on-commit you'd provide a TCS and await it.
    public void Append(string jsonRecord)
    {
        Enqueue(jsonRecord);
    }

    // Caller can await this TCS to know when the record has been flushed to disk.
    public Task CommitAsync(string jsonRecord)
    {
        var tcs = new TaskCompletionSource&lt;bool&gt;(TaskCreationOptions.RunContinuationsAsynchronously);
        Enqueue(jsonRecord, tcs);
        return tcs.Task;
    }

    private void FlusherLoop()
    {
        var sw = System.Diagnostics.Stopwatch.StartNew();
        var sb = new StringBuilder();
        while (_running)
        {
            WalEntry entry;
            // Block for a short time to gather entries.
            if (_queue.TryTake(out entry, 50))
            {
                sb.AppendLine(entry.Record);
                // drain any additional ready entries
                while (_queue.TryTake(out entry))
                {
                    sb.AppendLine(entry.Record);
                }
                // write batch
                var bytes = Encoding.UTF8.GetBytes(sb.ToString());
                _fs.Write(bytes, 0, bytes.Length);
                _fs.Flush(true); // flush to disk (durability)
                // mark committed entries
                // (we would mark all enqueued TCS as completed; for brevity handle single entry)
                // In a real impl, track list of TCS in the batch and set them true.
                sb.Clear();
            }
            else
            {
                // Periodic flush in case small amount of data remains
                if (sw.Elapsed &gt;= _flushInterval)
                {
                    // nothing to flush here in this simplified example
                    sw.Restart();
                }
            }
        }
    }

    public void Dispose()
    {
        _running = false;
        _queue.CompleteAdding();
        _flusherThread.Join();
        _fs?.Dispose();
    }

    class WalEntry
    {
        public string Record;
        public TaskCompletionSource&lt;bool&gt; Tcs;
    }
}
</code></pre>
<p><strong>Durability note:</strong> The code above calls <code>_fs.Flush(true)</code> inside the background thread. If you want the caller to see a commit as durable only after fsync, use <code>CommitAsync</code> and complete the <code>TaskCompletionSource</code> only after flush completes. If you allow callers to proceed before flush, you adopt asynchronous durability (faster but riskier).</p>
<p><strong>When to use:</strong> medium to high concurrency workloads, where batching yields better throughput.</p>
<hr />
<h2 id="75-approach-3---group-commit-how-postgres-and-others-batch-fsyncs"><a class="header" href="#75-approach-3---group-commit-how-postgres-and-others-batch-fsyncs">7.5 Approach 3 - Group commit (how Postgres and others batch fsyncs)</a></h2>
<p><strong>Group commit</strong> is the strategy of batching multiple transaction commits into a single <code>fsync</code> call. The orchestrator collects transactions that want to commit, writes all related WAL records (or ensures they are in the buffer), then performs one <code>fsync</code>. All waiting transactions are then acknowledged as durable.</p>
<p>This dramatically increases throughput because <code>fsync</code> is expensive.</p>
<p>A tiny simulation (conceptual) of group commit in C#:</p>
<pre><code class="language-csharp">// GroupCommitWAL.cs (conceptual)
using System;
using System.Collections.Generic;
using System.IO;
using System.Text;
using System.Threading;

public class GroupCommitWAL : IDisposable
{
    private readonly FileStream _fs;
    private readonly object _lock = new object();
    private readonly List&lt;CommitRequest&gt; _pending = new List&lt;CommitRequest&gt;();
    private readonly Timer _timer;

    public GroupCommitWAL(string path, TimeSpan commitWindow)
    {
        _fs = new FileStream(path, FileMode.Append, FileAccess.Write, FileShare.Read, 4096, FileOptions.None);
        // Timer ticks to trigger grouped commits periodically
        _timer = new Timer(_ =&gt; FlushGroup(), null, commitWindow, commitWindow);
    }

    public void Commit(string walRecord)
    {
        var req = new CommitRequest { Record = walRecord, Completed = new ManualResetEventSlim(false) };
        lock (_lock)
        {
            _pending.Add(req);
        }
        // Wait until group flush completes (durable commit)
        req.Completed.Wait();
    }

    private void FlushGroup()
    {
        List&lt;CommitRequest&gt; toFlush;
        lock (_lock)
        {
            if (_pending.Count == 0) return;
            toFlush = new List&lt;CommitRequest&gt;(_pending);
            _pending.Clear();
        }

        // Write all records in one batch
        var sb = new StringBuilder();
        foreach (var r in toFlush) sb.AppendLine(r.Record);
        var bytes = Encoding.UTF8.GetBytes(sb.ToString());
        lock (_fs) // file write must be atomic wrt other file operations
        {
            _fs.Write(bytes, 0, bytes.Length);
            _fs.Flush(true); // one fsync for many commits
        }

        // Signal all waiting commits that their work is durable
        foreach (var r in toFlush) r.Completed.Set();
    }

    public void Dispose()
    {
        _timer.Dispose();
        FlushGroup();
        _fs.Dispose();
    }

    private class CommitRequest
    {
        public string Record;
        public ManualResetEventSlim Completed;
    }
}
</code></pre>
<p><strong>Real systems:</strong> Postgres uses a similar idea but with finer control: backends call <code>XLogInsert</code> to add WAL records, then to commit they wait for the WAL to be flushed (or use async commit options). The WAL writer coordinates the flush and wakes the waiting backends.</p>
<p><strong>Pros:</strong> much higher throughput under many small transactions.
<strong>Cons:</strong> increases worst-case commit latency by up to the group window; more complex coordination.</p>
<hr />
<h2 id="76-durability-modes-and-configuration-knobs"><a class="header" href="#76-durability-modes-and-configuration-knobs">7.6 Durability modes and configuration knobs</a></h2>
<p>Databases expose knobs controlling durability behavior; common choices:</p>
<ul>
<li><strong>Synchronous commit</strong> (force WAL to disk before returning commit) - safest; highest latency.</li>
<li><strong>Asynchronous commit</strong> (ack before fsync) - fastest; risk of losing recent commits on crash.</li>
<li><strong>Group commit window</strong> - time to wait to aggregate commit requests.</li>
<li><strong>Periodic flush</strong> - flush WAL every X ms for a compromise.</li>
<li><strong>O_DSYNC / O_DIRECT / fdatasync</strong> - lower-level flags to control caching and metadata flushes.</li>
</ul>
<p><strong>Examples in real DBs:</strong></p>
<ul>
<li><em>Postgres</em>: <code>synchronous_commit</code> can be on/off/local, and Postgres does group commit internally.</li>
<li><em>InnoDB (MySQL)</em>: <code>innodb_flush_log_at_trx_commit</code> = 1 (fsync per commit), 2 (flush to OS buffer but not fsync), 0 (flush every second).</li>
<li><em>SQLite</em>: can run in WAL mode (append-only WAL file) or rollback journal; durability also depends on <code>PRAGMA synchronous</code>.</li>
</ul>
<hr />
<h2 id="77-handling-tornwritten-partial-pages--atomicity"><a class="header" href="#77-handling-tornwritten-partial-pages--atomicity">7.7 Handling torn/written partial pages &amp; atomicity</a></h2>
<p>Disk writes may be torn (part of a page persisted). Production systems guard against partial writes:</p>
<ul>
<li><strong>Doublewrite buffer</strong> (MySQL InnoDB): write pages first to a contiguous doublewrite area, then to final locations. If a crash causes torn writes, pages can be recovered from the doublewrite area.</li>
<li><strong>Checksums + page LSN</strong>: pages include LSN and checksum; corrupted pages detected during recovery and skipped/rewritten.</li>
<li><strong>Atomic append for WAL</strong>: appending small WAL records is typically atomic from the file system perspective if the writes are smaller than the atomic write size - but you should not rely on this; instead, rely on WAL + checksums and redo logic.</li>
</ul>
<hr />
<h2 id="78-concurrency-and-recovery-interactions"><a class="header" href="#78-concurrency-and-recovery-interactions">7.8 Concurrency and recovery interactions</a></h2>
<p>Concurrency complicates recovery: while WAL is append-only, workers may reorder actions in memory. Two important invariants:</p>
<ul>
<li><strong>Write-Ahead Principle:</strong> WAL record for a change must be durable before the change is made durable in the data file (or before transaction is committed). This is why WAL flush ordering is critical.</li>
<li><strong>Page LSN checks during redo:</strong> When reapplying log entries during recovery, databases compare the record LSN against the page’s stored LSN (in-page metadata) to decide whether to reapply - this makes redo idempotent.</li>
</ul>
<p>When using background flushers, group commit, or batching, ensure that the code that acknowledges commit to the application only does that after the WAL has been persisted according to the chosen durability level.</p>
<hr />
<h2 id="79-advanced-optimizations-overview"><a class="header" href="#79-advanced-optimizations-overview">7.9 Advanced optimizations (overview)</a></h2>
<ul>
<li><strong>Batch &amp; vectorized writes</strong>: accumulating many small records into a bigger write reduces syscall overhead.</li>
<li><strong>Per-thread WAL buffers merged by a writer</strong>: Threads append into thread-local buffers, then merged in order by a single writer with sequence numbers.</li>
<li><strong>LSN allocation vs persistence ordering</strong>: Assign LSNs in-memory quickly, but ensure the physical disk order reflects necessary constraints (LSNs are logical; recovery relies on LSN semantics).</li>
<li><strong>Partitioned WAL</strong>: split WAL into segments (log files) to rotate, compress, and ship for replication (e.g., Postgres WAL segments).</li>
<li><strong>Direct I/O &amp; aligned writes</strong>: reduce kernel cache effects and torn writes, but increases complexity (alignment, buffering).</li>
</ul>
<hr />
<h2 id="710-full-illustrative-sample---wal-writer-with-background-flusher--commit-acknowledgements-c"><a class="header" href="#710-full-illustrative-sample---wal-writer-with-background-flusher--commit-acknowledgements-c">7.10 Full illustrative sample - WAL writer with background flusher + commit acknowledgements (C#)</a></h2>
<p>Below is a more fleshed-out sample: workers submit WAL records with a <code>Task</code> they can await for durability. The background flusher groups pending records and performs one <code>Flush</code> (fsync) for the group. This simulates group commit in a simple way.</p>
<pre><code class="language-csharp">// GroupedBufferedWAL.cs
using System;
using System.Collections.Concurrent;
using System.Collections.Generic;
using System.IO;
using System.Text;
using System.Threading;
using System.Threading.Tasks;

public class GroupedBufferedWAL : IDisposable
{
    private readonly FileStream _fs;
    private readonly BlockingCollection&lt;WalRequest&gt; _queue = new BlockingCollection&lt;WalRequest&gt;(new ConcurrentQueue&lt;WalRequest&gt;());
    private readonly Thread _flusher;
    private readonly int _maxBatchSize;
    private readonly TimeSpan _maxWait;

    public GroupedBufferedWAL(string path, int maxBatchSize = 64, TimeSpan? maxWait = null)
    {
        _fs = new FileStream(path, FileMode.Append, FileAccess.Write, FileShare.Read, 8192, FileOptions.None);
        _maxBatchSize = Math.Max(1, maxBatchSize);
        _maxWait = maxWait ?? TimeSpan.FromMilliseconds(10);

        _flusher = new Thread(FlushLoop) { IsBackground = true };
        _flusher.Start();
    }

    // Returns a Task that completes when the record is flushed (durable).
    public Task AppendAsync(string record)
    {
        var req = new WalRequest { Record = record, Tcs = new TaskCompletionSource&lt;bool&gt;(TaskCreationOptions.RunContinuationsAsynchronously) };
        _queue.Add(req);
        return req.Tcs.Task;
    }

    private void FlushLoop()
    {
        var batch = new List&lt;WalRequest&gt;(_maxBatchSize);
        while (!_queue.IsCompleted)
        {
            try
            {
                WalRequest first;
                if (!_queue.TryTake(out first, (int)_maxWait.TotalMilliseconds))
                {
                    // timed out - if we have something in batch, flush it
                    if (batch.Count &gt; 0)
                    {
                        DoFlush(batch);
                        batch.Clear();
                    }
                    continue;
                }

                batch.Add(first);

                // collect up to maxBatchSize quickly
                while (batch.Count &lt; _maxBatchSize &amp;&amp; _queue.TryTake(out var r))
                {
                    batch.Add(r);
                }

                DoFlush(batch);
                batch.Clear();
            }
            catch (Exception ex)
            {
                // in production you'd have robust error handling and retry logic
                Console.Error.WriteLine("FlushLoop error: " + ex);
            }
        }

        // flush remaining
        if (batch.Count &gt; 0) DoFlush(batch);
    }

    private void DoFlush(List&lt;WalRequest&gt; batch)
    {
        // Build a single write for the whole batch
        var sb = new StringBuilder();
        foreach (var r in batch) sb.AppendLine(r.Record);
        var bytes = Encoding.UTF8.GetBytes(sb.ToString());

        lock (_fs) // serialize file writes with the file handle
        {
            _fs.Write(bytes, 0, bytes.Length);
            _fs.Flush(true); // ask OS to persist to stable storage
        }

        // mark all tasks as completed (durable)
        foreach (var r in batch) r.Tcs.TrySetResult(true);
    }

    public void Dispose()
    {
        _queue.CompleteAdding();
        _flusher.Join();
        _fs.Dispose();
    }

    private class WalRequest
    {
        public string Record;
        public TaskCompletionSource&lt;bool&gt; Tcs;
    }
}
</code></pre>
<p><strong>How to use:</strong></p>
<pre><code class="language-csharp">var wal = new GroupedBufferedWAL("wal.log", maxBatchSize: 128, maxWait: TimeSpan.FromMilliseconds(5));

async Task Worker(int id)
{
    for (int i = 0; i &lt; 10; i++)
    {
        string rec = $"{{\"lsn\":null,\"txn\":{id},\"seq\":{i},\"payload\":\"data\"}}";
        await wal.AppendAsync(rec); // returns when durable (fsynced) as implemented
        Console.WriteLine($"Worker {id} committed {i}");
    }
}

var tasks = new List&lt;Task&gt;();
for (int w = 0; w &lt; 8; w++) tasks.Add(Worker(w));
Task.WaitAll(tasks.ToArray());
wal.Dispose();
</code></pre>
<p>This yields high throughput and provides a per-transaction durability <code>Task</code> that resolves after the group <code>fsync</code>.</p>
<hr />
<h2 id="711-trade-offs-recap"><a class="header" href="#711-trade-offs-recap">7.11 Trade-offs recap</a></h2>
<ul>
<li><strong>Lock-per-append</strong>: simple, correct, low concurrency. Good for small systems.</li>
<li><strong>Buffered flusher</strong>: better throughput, supports multiple writers without blocking on IO, but requires careful handling to provide correctness when callers need synchronous durability.</li>
<li><strong>Group commit</strong>: excellent throughput for many small commits; increases commit latency up to the group window.</li>
<li><strong>Doublewrite / checksums</strong>: required when page-level atomicity is a concern.</li>
<li><strong>Direct I/O / OS flags</strong>: can increase durability semantics but complicate code (alignment, buffering).</li>
<li><strong>Crash correctness guarantee</strong>: must preserve WAL ordering, ensure write-ahead property, and implement predictable recovery semantics (redo/undo with page LSN checks).</li>
</ul>
<hr />
<h2 id="712-practical-recommendations"><a class="header" href="#712-practical-recommendations">7.12 Practical recommendations</a></h2>
<ul>
<li>Start simple: use a mutex-guarded append for correctness early in development.</li>
<li>If you need throughput, move to an async buffered writer - implement <code>AppendAsync</code> that gives callers a <code>Task</code> when their entry is durable.</li>
<li>Implement <strong>group commit</strong> semantics if many small transactions dominate your workload.</li>
<li>Make durability configurable (sync vs async) so you can tune for latency vs throughput.</li>
<li>Add checksums and LSNs to WAL records and store page LSNs to make redo idempotent.</li>
<li>Test crash scenarios: simulate crashes at many points and validate recovery.</li>
</ul>
<hr />
<h2 id="713-summary"><a class="header" href="#713-summary">7.13 Summary</a></h2>
<p>Concurrency is the "real world" requirement for any WAL-backed storage engine. The key is deciding the right mix of guarantees and performance for your use case:</p>
<ul>
<li>If correctness and strong durability are critical, prefer synchronous commits and conservative flushes.</li>
<li>If throughput with acceptable bounded durability is acceptable, use group commit or periodic flush.</li>
<li>Mirror patterns used by mature systems (group commit in Postgres, InnoDB's doublewrite and flush controls, RocksDB's WAL + memtable flushing) to avoid common pitfalls.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-8--building-a-wal-in-rust"><a class="header" href="#chapter-8--building-a-wal-in-rust">Chapter 8 — Building a WAL in Rust</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="part-iii-wal-in-real-databases"><a class="header" href="#part-iii-wal-in-real-databases">Part III: WAL in Real Databases</a></h1>
<p>In the previous parts, we explored the <em>why</em> and <em>how</em> of Write-Ahead Logging (WAL).
We learned about durability, crash recovery, and even built our own miniature WAL system from scratch.
Now, it’s time to step into the real world.</p>
<p>Modern databases, from relational powerhouses like <strong>PostgreSQL</strong> and <strong>MySQL (InnoDB)</strong> to lightweight engines like <strong>SQLite</strong> and <strong>LMDB</strong>, all rely on WAL-or its close cousins-to ensure data consistency and crash recovery. But they each do it differently, balancing trade-offs in performance, durability, and concurrency.</p>
<p>In this part, we’ll take a guided tour through how real databases implement WAL. We’ll explore their internal mechanisms, design choices, and the reasoning behind them. Each section focuses on one system and connects theory to practice.</p>
<hr />
<h3 id="41-postgresql-wal"><a class="header" href="#41-postgresql-wal">4.1 PostgreSQL WAL</a></h3>
<p>We start with <strong>PostgreSQL</strong>, one of the most educational open-source systems when it comes to WAL design. Its implementation closely follows textbook principles but adds layers of sophistication-like checkpoints, timeline files, and streaming replication. We’ll see how PostgreSQL uses its WAL not only for recovery but also as a foundation for replication and point-in-time recovery.</p>
<hr />
<h3 id="42-sqlite-wal-mode"><a class="header" href="#42-sqlite-wal-mode">4.2 SQLite WAL Mode</a></h3>
<p>Next, we turn to <strong>SQLite</strong>, which takes a very different approach. Its “WAL mode” was introduced to solve concurrency issues in single-file databases. We’ll see how it replaces rollback journals with a simple yet powerful append-only WAL file, making reads and writes coexist peacefully in a lightweight environment-perfect for embedded and mobile systems.</p>
<hr />
<h3 id="43-innodb-redo-and-undo-logs"><a class="header" href="#43-innodb-redo-and-undo-logs">4.3 InnoDB Redo and Undo Logs</a></h3>
<p>MySQL’s <strong>InnoDB</strong> engine adds complexity with <em>two</em> kinds of logs-<strong>redo</strong> and <strong>undo</strong>. Together, they form a hybrid system balancing transactional atomicity and durability. We’ll dissect how the redo log ensures durability while the undo log maintains transactional isolation, and why InnoDB’s buffer pool makes its WAL design unique among relational engines.</p>
<hr />
<h3 id="44-rocksdb-wal"><a class="header" href="#44-rocksdb-wal">4.4 RocksDB WAL</a></h3>
<p><strong>RocksDB</strong>, a high-performance key-value store from Meta (Facebook), uses WAL to achieve consistency in a Log-Structured Merge (LSM) tree. Its WAL plays a critical role in bridging in-memory memtables and on-disk SSTables. We’ll look at how it handles high write throughput, compression, and replication in modern data-intensive systems.</p>
<hr />
<h3 id="45-lmdb-and-embedded-systems"><a class="header" href="#45-lmdb-and-embedded-systems">4.5 LMDB and Embedded Systems</a></h3>
<p>Finally, we’ll explore <strong>LMDB</strong> (Lightning Memory-Mapped Database), which takes an unconventional approach by relying on copy-on-write B-trees instead of a traditional WAL. This section helps us see that durability doesn’t always require a separate log file-the underlying principle of "write-before-overwrite" can manifest in different forms depending on the system’s constraints and goals.</p>
<hr />
<p>By the end of this part, you’ll have a deeper appreciation for how different databases interpret and implement WAL principles to fit their architectures. You’ll see the common threads-and the creative deviations-that make each system unique.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-9--postgresql-wal"><a class="header" href="#chapter-9--postgresql-wal">Chapter 9 : PostgreSQL WAL</a></h1>
<h3 id="inside-postgresqls-write-ahead-log-design-implementation-and-recovery"><a class="header" href="#inside-postgresqls-write-ahead-log-design-implementation-and-recovery"><em>Inside PostgreSQL’s Write-Ahead Log: Design, Implementation, and Recovery</em></a></h3>
<hr />
<h2 id="1-introduction---wal-at-the-heart-of-postgresql"><a class="header" href="#1-introduction---wal-at-the-heart-of-postgresql">1. Introduction - WAL at the Heart of PostgreSQL</a></h2>
<p>When PostgreSQL promises that a committed transaction will never vanish-even after a crash-it’s the <strong>Write-Ahead Log (WAL)</strong> that makes that promise possible.
Every change in PostgreSQL passes through this append-only log before it ever touches the main data files.</p>
<p>In the PostgreSQL source tree, the WAL subsystem lives primarily under
<code>src/backend/access/transam/</code> and headers under <code>src/include/access/</code>.
If you ever browse the repo (<a href="https://github.com/postgres/postgres">github.com/postgres/postgres</a>), files like
<code>xlog.c</code>, <code>xlogrecovery.c</code>, and <code>walwriter.c</code> are the main entry points.</p>
<p>Official documentation:</p>
<ul>
<li><a href="https://www.postgresql.org/docs/current/wal-intro.html">WAL Introduction</a></li>
<li><a href="https://www.postgresql.org/docs/current/wal-internals.html">WAL Internals</a></li>
<li><a href="https://www.postgresql.org/docs/current/continuous-archiving.html">Continuous Archiving and PITR</a></li>
</ul>
<hr />
<h2 id="2-architecture-overview"><a class="header" href="#2-architecture-overview">2. Architecture Overview</a></h2>
<p>PostgreSQL uses a <strong>multiprocess</strong> model-each client connection runs in its own process. WAL management therefore depends on shared memory and a few cooperating background processes:</p>
<div class="table-wrapper"><table><thead><tr><th>Process</th><th>Responsibility</th></tr></thead><tbody>
<tr><td><strong>Backend</strong></td><td>Executes queries and inserts WAL records for each change.</td></tr>
<tr><td><strong>WAL Writer</strong></td><td>Periodically flushes WAL buffers to disk, offloading I/O from backends.</td></tr>
<tr><td><strong>Background Writer</strong></td><td>Writes dirty data pages from shared buffers to data files.</td></tr>
<tr><td><strong>Checkpointer</strong></td><td>Creates checkpoints, ensuring pages up to a certain LSN are on disk.</td></tr>
<tr><td><strong>Archiver / WAL Sender</strong></td><td>Ships WAL segments to replicas or archives.</td></tr>
</tbody></table>
</div>
<p>The high-level flow looks like this:</p>
<pre><code>Client SQL  → Backend modifies buffers
             → XLogInsert() creates WAL record
             → WAL buffer appended
             → WAL Writer flushes to segment file
             → Checkpoint ensures data pages persist
</code></pre>
<p>All WAL-related files live under <code>pg_wal/</code> in the data directory. Each file (a <em>segment</em>) is 16 MB by default and named like <code>00000001000000000000000A</code>.</p>
<hr />
<h2 id="3-wal-record-structure--lsns"><a class="header" href="#3-wal-record-structure--lsns">3. WAL Record Structure &amp; LSNs</a></h2>
<p>Every operation that changes data generates one or more <strong>WAL records</strong>.
The core structure, defined in <code>xlogrecord.h</code>, contains:</p>
<pre><code class="language-c">typedef struct XLogRecord {
    uint32  xl_tot_len;   /* total record length */
    TransactionId xl_xid; /* transaction id */
    XLogRecPtr xl_prev;   /* pointer to previous record */
    uint8   xl_info;      /* record type flags */
    RmgrId  xl_rmid;      /* resource manager id */
    /* followed by record-specific data */
} XLogRecord;
</code></pre>
<p>PostgreSQL assigns each byte in the WAL a <strong>Log Sequence Number (LSN)</strong> - a 64-bit pointer displayed as two hex parts (<code>0/16B3C1F0</code>).
Every data page stores the LSN of its last update in its header.
During recovery, WAL records are applied only if their LSN &gt; page LSN, guaranteeing idempotent replay.</p>
<p>LSNs are returned by SQL functions like:</p>
<pre><code class="language-sql">SELECT pg_current_wal_lsn();
SELECT pg_last_wal_replay_lsn();
</code></pre>
<hr />
<h2 id="4-inserting-and-flushing-wal"><a class="header" href="#4-inserting-and-flushing-wal">4. Inserting and Flushing WAL</a></h2>
<p>When a backend modifies a page, it calls <code>XLogInsert()</code> to write a WAL record into a circular buffer in shared memory (the <em>WAL buffers</em>).
Space reservation and LSN assignment are protected by lightweight locks to serialize writers.</p>
<p>Simplified flow (adapted from <code>xlog.c</code>):</p>
<pre><code class="language-c">/* Pseudocode illustration */
XLogRecPtr XLogInsert(rmid, info, data)
{
    acquire(WALInsertLock);
    LSN start = InsertPos;
    InsertPos += sizeof(XLogRecord) + data_length;
    release(WALInsertLock);

    fill_record_header(...);
    copy_payload_into_wal_buffer(...);
    return start;
}
</code></pre>
<p>Backends don’t usually write WAL to disk themselves.
Instead, they signal that their commit’s LSN must be flushed, and either the <strong>WAL Writer</strong> or one backend performing a commit will call <code>XLogFlush()</code>:</p>
<pre><code class="language-c">void XLogFlush(LSN target)
{
    if (WalWriterFlushPos &gt;= target)
        return;
    acquire(WALFlushLock);
    write_wal_buffers_to_disk(up_to=target);
    fsync(wal_segment_file);
    WalWriterFlushPos = target;
    release(WALFlushLock);
}
</code></pre>
<p>This batching amortizes costly <code>fsync()</code> calls and allows many commits to share the same disk flush.</p>
<hr />
<h2 id="5-checkpointing-and-background-processes"><a class="header" href="#5-checkpointing-and-background-processes">5. Checkpointing and Background Processes</a></h2>
<p>A <strong>checkpoint</strong> marks a moment when all changes up to a given LSN are safely on disk.
During a checkpoint, PostgreSQL:</p>
<ol>
<li>Requests the WAL Writer to flush WAL up to the checkpoint LSN.</li>
<li>Forces all dirty buffers whose page LSN ≤ that LSN to disk.</li>
<li>Writes a <em>checkpoint record</em> into WAL describing the redo start point.</li>
<li>Updates <code>pg_control</code> with the new checkpoint pointer.</li>
</ol>
<p>Simplified flow (modeled after <code>checkpointer.c</code>):</p>
<pre><code class="language-c">CheckPoint()
{
    LSN redo = ComputeRedoStart();
    RequestXLogFlush(redo);
    FlushDirtyBuffers(redo);
    XLogInsert(RM_XLOG_ID, XLOG_CHECKPOINT_ONLINE, ...);
    XLogFlush(...);
    UpdateControlFile(redo);
}
</code></pre>
<p>Checkpoints keep recovery time bounded but introduce heavy I/O bursts, so PostgreSQL spreads them over time using <code>checkpoint_completion_target</code>.</p>
<p>Old segments before the checkpoint are either archived (via <code>archive_command</code>) or recycled for reuse, depending on configuration.</p>
<hr />
<h2 id="6-recovery-and-replay"><a class="header" href="#6-recovery-and-replay">6. Recovery and Replay</a></h2>
<p>On startup, PostgreSQL inspects <code>pg_control</code> to find the last checkpoint.
If the previous shutdown wasn’t clean, it enters <strong>crash recovery</strong> mode.</p>
<p>Recovery algorithm (simplified from <code>xlogrecovery.c</code>):</p>
<ol>
<li>
<p>Read the checkpoint record; get its <em>redo LSN</em>.</p>
</li>
<li>
<p>From that point, sequentially read WAL records.</p>
</li>
<li>
<p>For each record:</p>
<ul>
<li>Identify the resource manager (<code>rmgr</code>) responsible.</li>
<li>If the target page’s LSN &lt; record LSN, apply the redo function.</li>
<li>Update the page’s LSN.</li>
</ul>
</li>
<li>
<p>Stop at the end of WAL or at a requested PITR target.</p>
</li>
</ol>
<p>Each record type has a dedicated redo callback (see <code>src/backend/access/rmgr/</code> for modules like <code>heapam</code>, <code>btree</code>, <code>gin</code>, etc.) implementing <code>rm_redo()</code>.</p>
<p>During replay, PostgreSQL can branch timelines: when a standby is promoted, it starts a new timeline file (<code>00000002.history</code>) recording ancestry-vital for point-in-time recovery and replication consistency.</p>
<hr />
<h2 id="7-concurrency-and-design-trade-offs"><a class="header" href="#7-concurrency-and-design-trade-offs">7. Concurrency and Design Trade-offs</a></h2>
<p>PostgreSQL’s WAL design balances <em>simplicity</em> with <em>safety</em>:</p>
<ul>
<li><strong>Append-only design</strong> makes inserts fast and serializable with simple locks.</li>
<li><strong>WAL Writer batching</strong> reduces fsync overhead.</li>
<li><strong>Full-page writes</strong> (entire page images logged when first modified after a checkpoint) prevent torn-page corruption.</li>
<li><strong>Checkpoints</strong> trade off runtime I/O vs. recovery duration.</li>
<li><strong>Multiprocess architecture</strong> avoids complex thread safety issues but relies heavily on shared memory coordination.</li>
</ul>
<h3 id="observed-trade-offs"><a class="header" href="#observed-trade-offs">Observed trade-offs</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Goal</th><th>Challenge</th><th>Mitigation</th></tr></thead><tbody>
<tr><td>Low latency commits</td><td>fsync cost</td><td>Group commits via WAL Writer</td></tr>
<tr><td>Quick recovery</td><td>Frequent checkpoints</td><td>Spread checkpoint I/O</td></tr>
<tr><td>Small WAL size</td><td>full_page_writes expansion</td><td>Optional compression / tuning</td></tr>
<tr><td>Replication lag</td><td>WAL shipping delay</td><td>Asynchronous streaming / slots</td></tr>
</tbody></table>
</div>
<hr />
<h2 id="8-configuration-tuning--monitoring"><a class="header" href="#8-configuration-tuning--monitoring">8. Configuration, Tuning &amp; Monitoring</a></h2>
<p>PostgreSQL exposes many WAL-related parameters in <code>postgresql.conf</code> (<a href="https://www.postgresql.org/docs/current/runtime-config-wal.html">docs</a>):</p>
<div class="table-wrapper"><table><thead><tr><th>Setting</th><th>Description</th></tr></thead><tbody>
<tr><td><code>wal_level</code></td><td>Controls WAL detail: <code>minimal</code>, <code>replica</code>, <code>logical</code>.</td></tr>
<tr><td><code>synchronous_commit</code></td><td>Determines when commit waits for WAL flush.</td></tr>
<tr><td><code>checkpoint_timeout</code></td><td>Maximum time between checkpoints.</td></tr>
<tr><td><code>checkpoint_completion_target</code></td><td>Fraction of interval used to spread writes.</td></tr>
<tr><td><code>max_wal_size</code> / <code>min_wal_size</code></td><td>Controls recycling/archiving thresholds.</td></tr>
<tr><td><code>archive_mode</code> / <code>archive_command</code></td><td>Enables continuous archiving.</td></tr>
</tbody></table>
</div>
<p>Monitoring views:</p>
<pre><code class="language-sql">SELECT * FROM pg_stat_bgwriter;
SELECT * FROM pg_stat_wal;
</code></pre>
<hr />
<h2 id="9-summary--takeaways"><a class="header" href="#9-summary--takeaways">9. Summary &amp; Takeaways</a></h2>
<p>PostgreSQL’s WAL system embodies the purest form of write-ahead logging found in any production database:</p>
<ul>
<li><strong>Strict ordering:</strong> Every modification is logged before it reaches disk.</li>
<li><strong>Cooperative background processes:</strong> WAL Writer, Checkpointer, and Archiver work in concert.</li>
<li><strong>LSN-based recovery:</strong> Pages and WAL are stitched together by LSNs for deterministic replay.</li>
<li><strong>Unified mechanism:</strong> The same WAL supports crash recovery, replication, and point-in-time restoration.</li>
</ul>
<p>For readers exploring the codebase:</p>
<ul>
<li><code>src/backend/access/transam/xlog.c</code> – core WAL insert &amp; flush logic</li>
<li><code>walwriter.c</code> – background writer loop</li>
<li><code>xlogrecovery.c</code> – recovery and redo routines</li>
<li><code>rmgr.c</code> + <code>src/backend/access/rmgr/</code> – resource-manager redo handlers</li>
</ul>
<hr />
<h2 id="10-reflection-and-comparison-template"><a class="header" href="#10-reflection-and-comparison-template">10. Reflection and Comparison Template</a></h2>
<p>This section acts as a <strong>bridge</strong> for upcoming chapters (SQLite, InnoDB, RocksDB, LMDB).
You can reuse the same structure to compare how each system:</p>
<ol>
<li>Defines log records and sequence numbers</li>
<li>Handles concurrency and flushing</li>
<li>Implements checkpoints or compaction</li>
<li>Replays logs during recovery</li>
<li>Balances performance vs. durability</li>
</ol>
<p>For PostgreSQL, the guiding philosophy is clarity over cleverness-every change flows through a linear, append-only stream, and every byte written can be traced by its LSN.</p>
<hr />
<p><strong>Further Reading</strong></p>
<ul>
<li><a href="https://www.postgresql.org/docs/current/wal-internals.html">PostgreSQL WAL Internals (official)</a></li>
<li><a href="https://github.com/postgres/postgres/blob/master/src/backend/access/transam/xlog.c">PostgreSQL Source: <code>xlog.c</code></a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-10-sqlite-wal-mode"><a class="header" href="#chapter-10-sqlite-wal-mode">Chapter 10: SQLite WAL Mode</a></h1>
<h2 id="1-introduction"><a class="header" href="#1-introduction"><strong>1. Introduction</strong></a></h2>
<p>SQLite is one of the most widely used embedded databases in the world. It powers everything from mobile applications and web browsers to IoT devices and operating systems. One of its most remarkable features is its simplicity-just a single file acts as a complete database.
But simplicity comes with a price: concurrency and performance challenges.</p>
<p>In earlier versions, SQLite used a <strong>rollback journal</strong> mechanism to ensure atomicity and durability. However, this model required exclusive file locking during commits, severely limiting write concurrency.</p>
<p>To solve this, SQLite introduced <strong>Write-Ahead Logging (WAL) mode</strong> in version <strong>3.7.0 (2010)</strong>. WAL mode fundamentally changed how SQLite handles transactions, offering better concurrency, crash recovery, and performance under multi-reader workloads.</p>
<blockquote>
<p>Official Docs: <a href="https://www.sqlite.org/wal.html">SQLite WAL Mode Overview</a></p>
</blockquote>
<hr />
<h2 id="2-the-core-idea"><a class="header" href="#2-the-core-idea"><strong>2. The Core Idea</strong></a></h2>
<p>In <strong>WAL mode</strong>, writes are no longer made directly to the main database file (<code>.db</code>).
Instead, they are <strong>appended</strong> to a separate file - the <strong>WAL file</strong> (<code>.db-wal</code>).</p>
<ul>
<li>The WAL file stores the new versions of modified pages.</li>
<li>Readers continue to access the old database file until a checkpoint merges the WAL contents back into it.</li>
<li>This eliminates the need for readers to block writers.</li>
</ul>
<p>SQLite’s WAL file acts as a <strong>rolling append-only log of page changes</strong>, maintaining durability and crash safety.</p>
<pre><code>┌──────────────┐     ┌─────────────┐
│ main.db file │     │  main.db-wal│
└──────────────┘     └─────────────┘
       ↑                     ↑
       │                     │
    checkpoint ←────── append writes
</code></pre>
<hr />
<h2 id="3-how-wal-mode-works"><a class="header" href="#3-how-wal-mode-works"><strong>3. How WAL Mode Works</strong></a></h2>
<h3 id="31-normal-journal-mode-vs-wal-mode"><a class="header" href="#31-normal-journal-mode-vs-wal-mode"><strong>3.1 Normal Journal Mode vs WAL Mode</strong></a></h3>
<div class="table-wrapper"><table><thead><tr><th>Aspect</th><th>Rollback Journal</th><th>WAL Mode</th></tr></thead><tbody>
<tr><td>Write behavior</td><td>Copy old pages to rollback journal, then write new pages</td><td>Append modified pages to WAL file</td></tr>
<tr><td>Reader-writer concurrency</td><td>Readers block writers</td><td>Readers and writers can proceed concurrently</td></tr>
<tr><td>Crash recovery</td><td>Rollback uncommitted changes</td><td>Replay committed changes from WAL</td></tr>
<tr><td>Checkpointing</td><td>Not needed (implicit)</td><td>Required to merge WAL into DB</td></tr>
</tbody></table>
</div>
<hr />
<h3 id="32-wal-file-structure"><a class="header" href="#32-wal-file-structure"><strong>3.2 WAL File Structure</strong></a></h3>
<p>Each WAL file is composed of <strong>frames</strong>, each representing a modified page.</p>
<p><strong>Structure:</strong></p>
<pre><code>+-----------------------------------------------------------+
| WAL Header (32 bytes)                                     |
+-----------------------------------------------------------+
| Frame #1 Header (24 bytes) | Frame #1 Page Data (4096 B)  |
+-----------------------------------------------------------+
| Frame #2 Header (24 bytes) | Frame #2 Page Data (4096 B)  |
+-----------------------------------------------------------+
| ...                                                         
+-----------------------------------------------------------+
</code></pre>
<p>Each frame includes:</p>
<ul>
<li><strong>Page number</strong></li>
<li><strong>Commit record number (or LSN)</strong></li>
<li><strong>Checksum</strong></li>
<li><strong>The page image</strong></li>
</ul>
<p>SQLite always appends new frames to the WAL file until a <strong>checkpoint</strong> occurs.</p>
<hr />
<h3 id="33-checkpointing"><a class="header" href="#33-checkpointing"><strong>3.3 Checkpointing</strong></a></h3>
<p>Checkpointing merges changes from the WAL file into the main database.
There are three types:</p>
<div class="table-wrapper"><table><thead><tr><th>Type</th><th>Description</th></tr></thead><tbody>
<tr><td><strong>Passive</strong></td><td>Merge only if readers are not active</td></tr>
<tr><td><strong>Full</strong></td><td>Waits for readers to finish before checkpointing</td></tr>
<tr><td><strong>Restart</strong></td><td>Like full, but also resets WAL</td></tr>
<tr><td><strong>Truncate</strong></td><td>Like restart, but truncates WAL to zero bytes</td></tr>
</tbody></table>
</div>
<p>You can manually trigger one using SQL:</p>
<pre><code class="language-sql">PRAGMA wal_checkpoint(FULL);
</code></pre>
<p>Or configure automatic checkpoints:</p>
<pre><code class="language-sql">PRAGMA wal_autocheckpoint = 1000;
</code></pre>
<hr />
<h2 id="4-code-example---enabling-wal-mode"><a class="header" href="#4-code-example---enabling-wal-mode"><strong>4. Code Example - Enabling WAL Mode</strong></a></h2>
<p>Let’s enable WAL mode and observe its behavior:</p>
<pre><code class="language-sql">-- Enable WAL mode
PRAGMA journal_mode = WAL;

-- Create a table
CREATE TABLE users(id INTEGER PRIMARY KEY, name TEXT);

-- Insert some data
INSERT INTO users(name) VALUES ('Alice');
INSERT INTO users(name) VALUES ('Bob');

-- Check current WAL mode
PRAGMA journal_mode;
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>wal
</code></pre>
<p>Now, check your directory - you’ll see a new file:</p>
<pre><code>mydb.db-wal
</code></pre>
<p>This file will grow as you write, then shrink after a checkpoint.</p>
<hr />
<h2 id="5-c-code-snippet-opening-sqlite-in-wal-mode"><a class="header" href="#5-c-code-snippet-opening-sqlite-in-wal-mode"><strong>5. C Code Snippet: Opening SQLite in WAL Mode</strong></a></h2>
<p>From SQLite’s own <a href="https://github.com/sqlite/sqlite/blob/master/src/wal.c">test fixture code</a>:</p>
<pre><code class="language-c">sqlite3 *db;
int rc = sqlite3_open("test.db", &amp;db);
if (rc == SQLITE_OK) {
    sqlite3_exec(db, "PRAGMA journal_mode=WAL;", NULL, NULL, NULL);
}
</code></pre>
<p>SQLite internally calls <code>sqlite3PagerWalSupported()</code> and then opens a <strong>WAL object</strong> using the <code>sqlite3WalOpen()</code> function, which lives in <code>wal.c</code>.</p>
<p>Simplified from the SQLite source:</p>
<pre><code class="language-c">int sqlite3WalOpen(
  sqlite3_vfs *pVfs,
  const char *zDb,
  sqlite3_file *pDbFd,
  int syncFlags,
  int *pExists,
  sqlite3_wal **ppWal
){
  // Open or create a .db-wal file
  // Initialize WAL header and frame buffer
  // Prepare locks for writers and readers
}
</code></pre>
<hr />
<h2 id="6-concurrency-in-wal-mode"><a class="header" href="#6-concurrency-in-wal-mode"><strong>6. Concurrency in WAL Mode</strong></a></h2>
<p>SQLite implements <strong>multi-reader, single-writer</strong> concurrency.</p>
<ul>
<li>Multiple readers can read from the database while the writer appends to WAL.</li>
<li>Writers only need to acquire an <strong>exclusive WAL lock</strong> when committing.</li>
<li>Readers use a snapshot view based on the WAL header.</li>
</ul>
<p>Internally, this works through <strong>shared memory (<code>.db-shm</code>)</strong> that tracks:</p>
<ul>
<li>Readers’ positions (read marks)</li>
<li>The last committed frame</li>
<li>The writer’s status</li>
</ul>
<pre><code>main.db
main.db-wal
main.db-shm
</code></pre>
<p><code>main.db-shm</code> helps coordinate access among multiple processes.</p>
<hr />
<h2 id="7-recovery-process"><a class="header" href="#7-recovery-process"><strong>7. Recovery Process</strong></a></h2>
<p>On startup:</p>
<ol>
<li>SQLite checks if <code>.db-wal</code> exists.</li>
<li>If yes, it validates the WAL header.</li>
<li>Replays all committed frames (based on the last commit LSN).</li>
<li>Applies changes to the main database.</li>
<li>Truncates the WAL after recovery.</li>
</ol>
<pre><code class="language-c">// Simplified replay logic
while (next_frame &lt;= last_commit_frame) {
    apply_page_to_db(wal_get_page(frame.page_no));
}
</code></pre>
<p>This ensures <strong>atomic commit</strong> and <strong>durability</strong> - even if the crash happened mid-transaction.</p>
<hr />
<h2 id="8-performance-and-tradeoffs"><a class="header" href="#8-performance-and-tradeoffs"><strong>8. Performance and Tradeoffs</strong></a></h2>
<p><strong>Advantages:</strong></p>
<ul>
<li>Readers and writers don’t block each other.</li>
<li>Writes are sequential (append-only).</li>
<li>Excellent for read-heavy workloads.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Slightly higher disk space usage (WAL + main file).</li>
<li>Checkpointing introduces periodic I/O bursts.</li>
<li>Limited scalability on concurrent writes (still single-writer).</li>
</ul>
<hr />
<h2 id="9-wal-mode-configuration-and-monitoring"><a class="header" href="#9-wal-mode-configuration-and-monitoring"><strong>9. WAL Mode Configuration and Monitoring</strong></a></h2>
<pre><code class="language-sql">-- View WAL file size
PRAGMA wal_checkpoint;

-- Force a manual checkpoint
PRAGMA wal_checkpoint(TRUNCATE);

-- Configure auto-checkpoint
PRAGMA wal_autocheckpoint = 1000;

-- Check if WAL is enabled
PRAGMA journal_mode;
</code></pre>
<hr />
<h2 id="10-experiment---visualizing-wal-in-action"><a class="header" href="#10-experiment---visualizing-wal-in-action"><strong>10. Experiment - Visualizing WAL in Action</strong></a></h2>
<p>Try this Python script to observe WAL behavior live:</p>
<pre><code class="language-python">import sqlite3, os, time

db = sqlite3.connect("wal_test.db")
cur = db.cursor()
cur.execute("PRAGMA journal_mode=WAL;")
cur.execute("CREATE TABLE IF NOT EXISTS test(id INTEGER PRIMARY KEY, value TEXT)")

for i in range(5):
    cur.execute("INSERT INTO test(value) VALUES (?)", (f"value-{i}",))
    db.commit()
    print("Inserted:", i)
    print("WAL size:", os.path.getsize("wal_test.db-wal"))
    time.sleep(1)
</code></pre>
<p>You’ll see the WAL file grow, then shrink when checkpointed.</p>
<hr />
<h2 id="11-design-insights"><a class="header" href="#11-design-insights"><strong>11. Design Insights</strong></a></h2>
<p>SQLite’s WAL design reflects minimalism:</p>
<ul>
<li><strong>Single-writer concurrency</strong> suits embedded environments.</li>
<li><strong>Append-only writes</strong> minimize random I/O.</li>
<li><strong>Auto-checkpointing</strong> keeps WAL manageable.</li>
<li><strong>Crash recovery</strong> is simple: replay the log.</li>
</ul>
<p>Its architecture inspired lightweight WAL systems in other embedded and distributed databases.</p>
<hr />
<h2 id="12-references"><a class="header" href="#12-references"><strong>12. References</strong></a></h2>
<ul>
<li><a href="https://www.sqlite.org/wal.html">SQLite WAL Documentation</a></li>
<li><a href="https://github.com/sqlite/sqlite/blob/master/src/wal.c">SQLite Source Code (<code>wal.c</code>)</a></li>
<li><a href="https://www.sqlite.org/checkpoint.html">SQLite Checkpointing Details</a></li>
<li><a href="http://aosabook.org/en/sqlite.html">SQLite Internals – The Architecture of Open Source Applications</a></li>
</ul>
<hr />
<h2 id="13-summary"><a class="header" href="#13-summary"><strong>13. Summary</strong></a></h2>
<p>SQLite’s <strong>WAL mode</strong> turns a single-file embedded database into a high-performance, crash-safe system capable of serving multiple readers and a single writer concurrently.
Its design is elegant - a minimal implementation of <strong>write-ahead logging</strong> tuned for simplicity and embedded environments.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-11-innodb-redo-and-undo-logging"><a class="header" href="#chapter-11-innodb-redo-and-undo-logging">Chapter 11: InnoDB Redo and Undo Logging</a></h1>
<hr />
<h2 id="1-introduction-1"><a class="header" href="#1-introduction-1"><strong>1. Introduction</strong></a></h2>
<p>The <strong>InnoDB storage engine</strong> (the default for MySQL) implements full <strong>ACID</strong> guarantees - Atomicity, Consistency, Isolation, and Durability.</p>
<p>At its core, InnoDB achieves <strong>durability and atomicity</strong> through two powerful mechanisms:</p>
<ol>
<li><strong>Redo Log</strong> - for durability and crash recovery (similar to WAL)</li>
<li><strong>Undo Log</strong> - for atomic rollback and MVCC (Multi-Version Concurrency Control)</li>
</ol>
<p>These two logs form the backbone of InnoDB’s storage and recovery system.</p>
<blockquote>
<p>Reference: <a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-redo-log.html">MySQL InnoDB Redo Log Documentation</a></p>
</blockquote>
<hr />
<h2 id="2-the-dual-log-architecture"><a class="header" href="#2-the-dual-log-architecture"><strong>2. The Dual-Log Architecture</strong></a></h2>
<p>Unlike SQLite (single WAL) or PostgreSQL (single WAL + hint bits), InnoDB splits logging responsibilities:</p>
<div class="table-wrapper"><table><thead><tr><th>Log Type</th><th>Purpose</th><th>Stored In</th></tr></thead><tbody>
<tr><td><strong>Redo Log</strong></td><td>Replays committed changes after crash</td><td><code>ib_logfile0</code>, <code>ib_logfile1</code> (circular)</td></tr>
<tr><td><strong>Undo Log</strong></td><td>Rolls back uncommitted transactions &amp; supports MVCC</td><td>Inside <code>ibdata1</code> or undo tablespaces</td></tr>
</tbody></table>
</div>
<p><strong>Analogy:</strong></p>
<ul>
<li>Redo log → “Replay what was done”</li>
<li>Undo log → “Undo what wasn’t committed”</li>
</ul>
<p>This dual design gives InnoDB the ability to <strong>recover from crashes</strong> while also <strong>supporting consistent reads</strong> without blocking writers.</p>
<hr />
<h2 id="3-the-innodb-transaction-lifecycle"><a class="header" href="#3-the-innodb-transaction-lifecycle"><strong>3. The InnoDB Transaction Lifecycle</strong></a></h2>
<p>When a transaction modifies a record, the following steps occur:</p>
<pre><code>+----------------------------------------------------+
| 1. Modify page in buffer pool                      |
| 2. Record change in redo log (write-ahead)         |
| 3. Record original version in undo log             |
| 4. Commit transaction                              |
| 5. Flush redo log to disk (fsync)                  |
| 6. Later: flush dirty pages to tablespace (.ibd)   |
+----------------------------------------------------+
</code></pre>
<p>So, InnoDB writes twice before the actual data hit the disk:</p>
<ul>
<li><strong>Redo Log</strong> ensures durability.</li>
<li><strong>Undo Log</strong> ensures atomicity and rollback capability.</li>
</ul>
<hr />
<h2 id="4-redo-log-internals"><a class="header" href="#4-redo-log-internals"><strong>4. Redo Log Internals</strong></a></h2>
<h3 id="41-files-and-format"><a class="header" href="#41-files-and-format"><strong>4.1 Files and Format</strong></a></h3>
<p>The redo log is stored in the <code>innodb_log_group_home_dir</code>, typically:</p>
<pre><code>ib_logfile0
ib_logfile1
</code></pre>
<p>As of MySQL 8.0, these are stored in the <strong>redo log directory</strong> and form a <strong>circular log sequence</strong>.</p>
<p>Each file contains <em>log records</em> describing low-level modifications (like updating a page or index).</p>
<h3 id="42-write-ahead-principle"><a class="header" href="#42-write-ahead-principle"><strong>4.2 Write-Ahead Principle</strong></a></h3>
<p>Just like PostgreSQL and SQLite WAL:</p>
<blockquote>
<p>InnoDB <strong>always writes redo logs before flushing data pages</strong> to disk.</p>
</blockquote>
<p>This guarantees that on a crash, InnoDB can recover committed transactions by replaying redo logs.</p>
<p><strong>Config parameters:</strong></p>
<pre><code class="language-sql">SHOW VARIABLES LIKE 'innodb_flush_log_at_trx_commit';
</code></pre>
<div class="table-wrapper"><table><thead><tr><th>Value</th><th>Behavior</th></tr></thead><tbody>
<tr><td>0</td><td>Write and flush every second</td></tr>
<tr><td>1</td><td>(Default) Flush on every commit</td></tr>
<tr><td>2</td><td>Write at commit, flush every second</td></tr>
</tbody></table>
</div>
<hr />
<h3 id="43-redo-log-record-example"><a class="header" href="#43-redo-log-record-example"><strong>4.3 Redo Log Record Example</strong></a></h3>
<p>A redo record doesn’t store “SQL,” it stores <strong>physical changes</strong> - like “modify bytes in page X at offset Y.”</p>
<p>Simplified example (conceptually):</p>
<pre><code class="language-text">LSN: 108490
Type: Update
Space ID: 3
Page No: 204
Offset: 0x4F
Before: 0x0002
After: 0x0003
</code></pre>
<p>Each record is identified by a <strong>Log Sequence Number (LSN)</strong> - a monotonically increasing byte offset.</p>
<hr />
<h3 id="44-checkpointing"><a class="header" href="#44-checkpointing"><strong>4.4 Checkpointing</strong></a></h3>
<p>InnoDB periodically writes a <strong>checkpoint</strong> - the point up to which all changes have been flushed to disk.</p>
<ul>
<li>Prevents replaying the entire redo log during recovery.</li>
<li>Maintains <code>min_lsn</code> (oldest log needed for recovery).</li>
<li>Implemented via the <strong>log checkpoint thread</strong>.</li>
</ul>
<pre><code>LSN progression:
 |-----------|-----------|-----------|
   flushed     checkpoint   new writes
</code></pre>
<p>Checkpoint info is stored in the <strong>InnoDB system tablespace header</strong>.</p>
<hr />
<h2 id="5-undo-log-internals"><a class="header" href="#5-undo-log-internals"><strong>5. Undo Log Internals</strong></a></h2>
<h3 id="51-purpose"><a class="header" href="#51-purpose"><strong>5.1 Purpose</strong></a></h3>
<p>Undo logs are the <strong>mirror</strong> of redo logs - they store how to <strong>undo</strong> modifications.</p>
<ul>
<li>On rollback → undo logs reverse changes.</li>
<li>On consistent reads → they reconstruct old versions of rows (MVCC).</li>
</ul>
<hr />
<h3 id="52-structure"><a class="header" href="#52-structure"><strong>5.2 Structure</strong></a></h3>
<p>Undo logs are stored in <strong>undo segments</strong> within <strong>rollback segments</strong>, inside undo tablespaces:</p>
<pre><code>Undo Tablespace
 └── Rollback Segments
      └── Undo Segments
           ├── Insert Undo
           └── Update Undo
</code></pre>
<p>Each undo record links to:</p>
<ul>
<li>Transaction ID</li>
<li>Previous version pointer</li>
<li>Modified columns</li>
</ul>
<hr />
<h3 id="53-undo-record-example"><a class="header" href="#53-undo-record-example"><strong>5.3 Undo Record Example</strong></a></h3>
<p>Conceptually:</p>
<pre><code class="language-text">Undo Record:
{
  trx_id: 505,
  table_id: 13,
  row_id: 600,
  undo_type: UPDATE,
  before_image: { col1=‘Alice’, col2=42 },
  prev_undo_ptr: 0x0001A3F0
}
</code></pre>
<p>Undo logs form a <strong>linked list of versions</strong>, allowing readers to traverse back in time.</p>
<hr />
<h3 id="54-undo-and-mvcc"><a class="header" href="#54-undo-and-mvcc"><strong>5.4 Undo and MVCC</strong></a></h3>
<p>When a reader starts a transaction, it gets a <strong>consistent snapshot</strong> of the database.
If another transaction updates a row afterward, InnoDB uses undo logs to reconstruct the old version.</p>
<p>This allows <strong>non-blocking reads</strong> (snapshot isolation).</p>
<pre><code>Current Row → Undo #2 → Undo #1 → Original
</code></pre>
<p>Readers pick the appropriate version based on the transaction’s snapshot view.</p>
<hr />
<h2 id="6-recovery-process"><a class="header" href="#6-recovery-process"><strong>6. Recovery Process</strong></a></h2>
<p>On startup after a crash:</p>
<ol>
<li><strong>Redo Phase</strong> – Replay committed changes (using redo logs).</li>
<li><strong>Undo Phase</strong> – Roll back uncommitted transactions (using undo logs).</li>
</ol>
<p>This is very similar to <strong>ARIES</strong> recovery algorithm used in many databases.</p>
<pre><code class="language-text">Crash Recovery Steps:
1. Read checkpoint LSN.
2. Replay redo records &gt;= checkpoint LSN.
3. For uncommitted trx, follow undo logs backward.
</code></pre>
<hr />
<h2 id="7-visualization-of-interaction"><a class="header" href="#7-visualization-of-interaction"><strong>7. Visualization of Interaction</strong></a></h2>
<pre><code>                ┌────────────────────────────────────┐
                │          User Transaction          │
                └────────────────────────────────────┘
                          │
                          ▼
                  ┌─────────────┐
                  │ Buffer Pool │
                  └─────────────┘
                       │
        ┌──────────────┼────────────────────┐
        ▼                                      ▼
┌──────────────┐                    ┌────────────────┐
│   Redo Log   │                    │   Undo Log     │
│ (Durability) │                    │ (Atomicity)    │
└──────────────┘                    └────────────────┘
</code></pre>
<hr />
<h2 id="8-simplified-example-in-c"><a class="header" href="#8-simplified-example-in-c"><strong>8. Simplified Example in C++</strong></a></h2>
<p>Let’s model InnoDB’s idea of redo/undo pairs conceptually.</p>
<pre><code class="language-cpp">#include &lt;iostream&gt;
#include &lt;stack&gt;
#include &lt;string&gt;

struct LogRecord {
    std::string action;
    std::string before;
    std::string after;
};

int main() {
    std::stack&lt;LogRecord&gt; undoLog;
    std::vector&lt;LogRecord&gt; redoLog;

    // Update operation
    LogRecord record = {"UPDATE users SET age=30", "age=29", "age=30"};
    redoLog.push_back(record);
    undoLog.push(record);

    // Commit: flush redo log
    std::cout &lt;&lt; "Flushing redo log to disk..." &lt;&lt; std::endl;

    // Rollback: use undo log
    if (false) { // simulate rollback
        auto undo = undoLog.top();
        std::cout &lt;&lt; "Undoing: " &lt;&lt; undo.before &lt;&lt; std::endl;
    }
}
</code></pre>
<p>This simplified code shows:</p>
<ul>
<li>Redo = persist future changes.</li>
<li>Undo = restore old state if rollback is needed.</li>
</ul>
<hr />
<h2 id="9-monitoring-and-configuration"><a class="header" href="#9-monitoring-and-configuration"><strong>9. Monitoring and Configuration</strong></a></h2>
<p>Useful commands for administrators:</p>
<pre><code class="language-sql">-- View redo log size
SHOW VARIABLES LIKE 'innodb_log_file_size';

-- View undo tablespaces
SELECT * FROM INFORMATION_SCHEMA.INNODB_TABLESPACES WHERE NAME LIKE 'undo%';

-- Check LSN progress
SHOW ENGINE INNODB STATUS\G

-- Configure redo log files
SET GLOBAL innodb_redo_log_capacity = 256M;
</code></pre>
<hr />
<h2 id="10-design-comparisons"><a class="header" href="#10-design-comparisons"><strong>10. Design Comparisons</strong></a></h2>
<div class="table-wrapper"><table><thead><tr><th>Feature</th><th>SQLite WAL</th><th>PostgreSQL WAL</th><th>InnoDB Redo/Undo</th></tr></thead><tbody>
<tr><td>Crash recovery</td><td>Replay WAL</td><td>Replay WAL</td><td>Replay Redo + Rollback Undo</td></tr>
<tr><td>MVCC</td><td>Snapshot via pages</td><td>Undo via tuple versions</td><td>Undo log based</td></tr>
<tr><td>Writers</td><td>Single</td><td>Multiple</td><td>Multiple</td></tr>
<tr><td>Checkpoints</td><td>Manual or auto</td><td>Checkpoints + segments</td><td>Checkpoint + purge</td></tr>
<tr><td>Isolation</td><td>Basic snapshot</td><td>MVCC</td><td>MVCC via undo segments</td></tr>
</tbody></table>
</div>
<hr />
<h2 id="11-advanced-concepts"><a class="header" href="#11-advanced-concepts"><strong>11. Advanced Concepts</strong></a></h2>
<ul>
<li><strong>Doublewrite Buffer</strong> – Prevents partial page writes.</li>
<li><strong>Log Sequence Numbers (LSN)</strong> – Global logical clock for redo positions.</li>
<li><strong>Purge Thread</strong> – Cleans old undo logs after transactions commit.</li>
<li><strong>Group Commit</strong> – Batches fsyncs for higher throughput.</li>
</ul>
<hr />
<h2 id="12-references-1"><a class="header" href="#12-references-1"><strong>12. References</strong></a></h2>
<ul>
<li><a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-redo-log.html">MySQL InnoDB Redo Log Docs</a></li>
<li><a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-undo-tablespaces.html">MySQL InnoDB Undo Tablespaces</a></li>
<li><a href="https://dev.mysql.com/doc/refman/8.0/en/innodb-recovery.html">InnoDB Recovery Process</a></li>
<li><a href="https://github.com/mysql/mysql-server/blob/8.0/storage/innobase/log/log0recv.cc">MySQL Source Code – log0recv.cc</a></li>
<li><a href="https://www.microsoft.com/en-us/research/publication/aries/">ARIES: A Transaction Recovery Method Supporting Fine-Granularity Locking</a></li>
</ul>
<hr />
<h2 id="13-summary-1"><a class="header" href="#13-summary-1"><strong>13. Summary</strong></a></h2>
<p>InnoDB’s dual log system - <strong>redo for durability</strong> and <strong>undo for atomicity and MVCC</strong> - provides a robust foundation for transactional consistency.
While the <strong>redo log</strong> ensures all committed changes can survive a crash, the <strong>undo log</strong> provides rollback capability and snapshot isolation, ensuring correctness in concurrent systems.</p>
<p>This elegant separation allows MySQL to balance <strong>performance, durability, and concurrency</strong>, making InnoDB one of the most advanced storage engines in the open-source ecosystem.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-12-rocksdb-wal-and-recovery"><a class="header" href="#chapter-12-rocksdb-wal-and-recovery">Chapter 12: RocksDB WAL and Recovery</a></h1>
<hr />
<h2 id="1-introduction-2"><a class="header" href="#1-introduction-2"><strong>1. Introduction</strong></a></h2>
<p>RocksDB is a <strong>high-performance, embedded key–value store</strong> developed by Facebook (now Meta), built on top of Google’s LevelDB.
It’s optimized for <strong>SSD storage, write-intensive workloads, and low-latency access</strong>.</p>
<p>At its core, RocksDB uses a <strong>Log-Structured Merge Tree (LSM)</strong> architecture - a write-optimized design where:</p>
<ul>
<li>Writes are appended sequentially (fast!),</li>
<li>Reads use in-memory indexes and Bloom filters,</li>
<li>And background <strong>compaction</strong> merges data to maintain order.</li>
</ul>
<p>Durability in RocksDB is achieved through a <strong>Write-Ahead Log (WAL)</strong>, just like in traditional databases - but adapted to an LSM world.</p>
<blockquote>
<p>Reference: <a href="https://github.com/facebook/rocksdb/wiki/Write-Ahead-Log">RocksDB WAL Docs</a></p>
</blockquote>
<hr />
<h2 id="2-the-core-idea-1"><a class="header" href="#2-the-core-idea-1"><strong>2. The Core Idea</strong></a></h2>
<p>All writes in RocksDB go through two components:</p>
<pre><code>Client Write → WAL (log file) → MemTable → SSTables (via Compaction)
</code></pre>
<ul>
<li>The <strong>WAL</strong> ensures durability.</li>
<li>The <strong>MemTable</strong> (an in-memory skiplist) provides fast access to recent data.</li>
<li>When the MemTable fills up, it’s flushed to disk as an <strong>SSTable</strong> (Sorted String Table).</li>
</ul>
<p>This two-tier approach provides <strong>fast writes + crash recovery</strong>.</p>
<hr />
<h2 id="3-the-write-path"><a class="header" href="#3-the-write-path"><strong>3. The Write Path</strong></a></h2>
<p>Here’s the step-by-step lifecycle of a write in RocksDB:</p>
<pre><code>1. Client issues PUT("key", "value")
2. RocksDB appends the record to the WAL file
3. Write is acknowledged after fsync (durability)
4. The record is inserted into the MemTable
5. Once MemTable is full → flush to disk (SST)
6. Later: Compaction merges SSTs for optimization
</code></pre>
<h3 id="visual-flow"><a class="header" href="#visual-flow">Visual Flow</a></h3>
<pre><code>┌─────────────┐
│ Application │
└──────┬──────┘
       │
       ▼
┌─────────────┐
│ WAL (Log)   │  ← Durable
└──────┬──────┘
       │
       ▼
┌─────────────┐
│ MemTable    │  ← In-memory, fast
└──────┬──────┘
       │ (flush)
       ▼
┌─────────────┐
│ SSTables    │  ← On-disk sorted runs
└─────────────┘
</code></pre>
<hr />
<h2 id="4-wal-file-format-and-management"><a class="header" href="#4-wal-file-format-and-management"><strong>4. WAL File Format and Management</strong></a></h2>
<p>Every RocksDB WAL file contains a sequence of <strong>records</strong>, each representing a batch of writes (WriteBatch).
Each record has:</p>
<div class="table-wrapper"><table><thead><tr><th>Field</th><th>Description</th></tr></thead><tbody>
<tr><td>CRC32C</td><td>Checksum</td></tr>
<tr><td>Size</td><td>Record length</td></tr>
<tr><td>Type</td><td>Record type (full, first, middle, last)</td></tr>
<tr><td>Data</td><td>The serialized write batch</td></tr>
</tbody></table>
</div>
<p><strong>Files:</strong></p>
<pre><code>00001.log
00002.log
...
MANIFEST-000003
</code></pre>
<p>Each WAL file corresponds to a specific <strong>log number</strong>, and RocksDB maintains metadata about active and obsolete logs.</p>
<hr />
<h2 id="5-c-example-writing-to-wal"><a class="header" href="#5-c-example-writing-to-wal"><strong>5. C++ Example: Writing to WAL</strong></a></h2>
<p>RocksDB’s public API automatically handles WAL writes internally.
However, here’s a minimal example:</p>
<pre><code class="language-cpp">#include "rocksdb/db.h"
#include "rocksdb/options.h"

using namespace rocksdb;

int main() {
    DB* db;
    Options options;
    options.create_if_missing = true;

    Status s = DB::Open(options, "/tmp/rocksdb_wal_demo", &amp;db);
    assert(s.ok());

    WriteOptions writeOptions;
    writeOptions.sync = true; // ensures WAL fsync
    db-&gt;Put(writeOptions, "user:1", "Alice");

    delete db;
}
</code></pre>
<p>Here:</p>
<ul>
<li><code>writeOptions.sync = true</code> forces a flush to the WAL before acknowledging the write.</li>
<li>If RocksDB crashes, it replays the WAL to reconstruct the MemTable.</li>
</ul>
<hr />
<h2 id="6-recovery-process-1"><a class="header" href="#6-recovery-process-1"><strong>6. Recovery Process</strong></a></h2>
<p>During startup:</p>
<ol>
<li>RocksDB scans the last checkpoint and log sequence numbers.</li>
<li>Finds any unflushed WAL files.</li>
<li>Replays WAL entries to reconstruct in-memory MemTables.</li>
<li>Resumes normal operation.</li>
</ol>
<p>Simplified from <a href="https://github.com/facebook/rocksdb/blob/main/db/db_impl_open.cc">db_impl/db_impl_open.cc</a>:</p>
<pre><code class="language-cpp">Status DBImpl::RecoverLogFiles() {
    for (auto log : logs_to_recover) {
        SequentialFileReader reader(log.file);
        while (ReadRecord(&amp;reader, &amp;record)) {
            WriteBatch batch(record);
            WriteBatchInternal::InsertInto(&amp;batch, memtable);
        }
    }
}
</code></pre>
<p>This ensures that all committed writes are replayed into the MemTable, exactly like <strong>PostgreSQL’s replay of WAL segments</strong> or <strong>InnoDB’s redo recovery</strong>.</p>
<hr />
<h2 id="7-flushing-and-checkpointing"><a class="header" href="#7-flushing-and-checkpointing"><strong>7. Flushing and Checkpointing</strong></a></h2>
<p>RocksDB performs <strong>flushing</strong> and <strong>checkpoints</strong> differently:</p>
<ul>
<li><strong>Flushing</strong>: Converts MemTable → SSTable.</li>
<li><strong>Checkpointing</strong>: Writes a consistent snapshot of the entire DB (via <code>Checkpoint::CreateCheckpoint()</code>).</li>
</ul>
<p>WAL files are deleted only after their corresponding MemTables are flushed and persisted.</p>
<pre><code class="language-cpp">Status DB::Flush(const FlushOptions&amp; options);
</code></pre>
<p>The process:</p>
<pre><code>WAL File → MemTable → Flush → SSTable → Remove old WAL
</code></pre>
<hr />
<h2 id="8-compaction-and-log-compaction"><a class="header" href="#8-compaction-and-log-compaction"><strong>8. Compaction and Log Compaction</strong></a></h2>
<p>RocksDB’s <strong>compaction</strong> process is a background merge of SST files to maintain sorted order and reclaim space.
It’s conceptually related to “log compaction” in Kafka - rewriting data to keep only the latest versions.</p>
<h3 id="levels-in-rocksdb"><a class="header" href="#levels-in-rocksdb"><strong>Levels in RocksDB:</strong></a></h3>
<pre><code>L0: Recent SSTs (from MemTables)
L1: Older, larger files
L2..Ln: Compact, sorted data
</code></pre>
<p>During compaction:</p>
<ul>
<li>Redundant key versions are dropped.</li>
<li>Deleted keys are purged.</li>
<li>Older SSTs are merged into newer ones.</li>
</ul>
<p>This keeps query performance fast and storage usage efficient.</p>
<hr />
<h2 id="9-performance-considerations"><a class="header" href="#9-performance-considerations"><strong>9. Performance Considerations</strong></a></h2>
<div class="table-wrapper"><table><thead><tr><th>Configuration</th><th>Description</th></tr></thead><tbody>
<tr><td><code>write_buffer_size</code></td><td>Controls MemTable size</td></tr>
<tr><td><code>max_write_buffer_number</code></td><td>Number of MemTables before flush</td></tr>
<tr><td><code>WAL_ttl_seconds</code></td><td>How long to keep WALs before deletion</td></tr>
<tr><td><code>WAL_size_limit_MB</code></td><td>Max total WAL size before force flush</td></tr>
<tr><td><code>max_background_flushes</code></td><td>Number of flush threads</td></tr>
</tbody></table>
</div>
<p><strong>Write latency</strong> in RocksDB is typically determined by:</p>
<ul>
<li>WAL fsync cost</li>
<li>MemTable lock contention</li>
<li>Background flush pressure</li>
</ul>
<p>For most workloads, RocksDB achieves <strong>&lt;1ms durability latency</strong> with async I/O and group commits.</p>
<hr />
<h2 id="10-group-commit-and-batching"><a class="header" href="#10-group-commit-and-batching"><strong>10. Group Commit and Batching</strong></a></h2>
<p>RocksDB batches multiple concurrent writes into a <strong>single WAL record</strong>, improving throughput:</p>
<pre><code class="language-cpp">Status DBImpl::WriteImpl(WriteOptions options, WriteBatch* updates) {
    Writer w(updates);
    writers_.push_back(&amp;w);
    if (writers_.size() &gt; 1) {
        // Wait for the leader writer
        w.wait();
    } else {
        // Leader writes all batches to WAL
        WriteBatch merged = MergePendingWrites();
        wal-&gt;Append(merged);
        fsync_if_needed();
        notify_followers();
    }
}
</code></pre>
<p>This is similar to <strong>InnoDB’s group commit mechanism</strong>.</p>
<hr />
<h2 id="11-wal-recycling-and-ttl"><a class="header" href="#11-wal-recycling-and-ttl"><strong>11. WAL Recycling and TTL</strong></a></h2>
<p>To avoid creating too many log files, RocksDB recycles WAL files:</p>
<pre><code class="language-cpp">options.recycle_log_file_num = 2;
</code></pre>
<p>When enabled, old WALs are truncated and reused, reducing filesystem overhead - a critical optimization for SSD endurance and cloud workloads.</p>
<hr />
<h2 id="12-example-manual-wal-replay"><a class="header" href="#12-example-manual-wal-replay"><strong>12. Example: Manual WAL Replay</strong></a></h2>
<p>RocksDB provides a lower-level API to read WALs manually for diagnostics or replication:</p>
<pre><code class="language-cpp">#include "rocksdb/transaction_log.h"

DB* db;
DB::Open(Options(), "/tmp/rocksdb_demo", &amp;db);

SequenceNumber start_seq = 0;
std::unique_ptr&lt;TransactionLogIterator&gt; it;
db-&gt;GetUpdatesSince(start_seq, &amp;it);

while (it-&gt;Valid()) {
    BatchResult batch = it-&gt;GetBatch();
    std::cout &lt;&lt; "Sequence: " &lt;&lt; batch.sequence &lt;&lt; std::endl;
    it-&gt;Next();
}
</code></pre>
<p>This feature is used by <strong>RocksDB Replication</strong>, <strong>MyRocks</strong>, and <strong>distributed storage systems</strong> to stream changes downstream - similar to PostgreSQL’s <code>WALReceiver</code>.</p>
<hr />
<h2 id="13-recovery-example-timeline"><a class="header" href="#13-recovery-example-timeline"><strong>13. Recovery Example Timeline</strong></a></h2>
<pre><code>Before crash:
  WAL: 00023.log
  MemTable: unflushed writes

After crash:
  On startup:
    → Detects 00023.log
    → Reads each WriteBatch
    → Reconstructs MemTable
    → Continues operation
</code></pre>
<p>This recovery process is extremely fast - often milliseconds - since logs are sequential.</p>
<hr />
<h2 id="14-comparison-with-other-systems"><a class="header" href="#14-comparison-with-other-systems"><strong>14. Comparison with Other Systems</strong></a></h2>
<div class="table-wrapper"><table><thead><tr><th>Feature</th><th>RocksDB</th><th>InnoDB</th><th>PostgreSQL</th></tr></thead><tbody>
<tr><td>Storage model</td><td>LSM Tree</td><td>B+ Tree</td><td>Heap + B+ Index</td></tr>
<tr><td>WAL type</td><td>Append-only key-value logs</td><td>Page-level redo logs</td><td>Segment WAL</td></tr>
<tr><td>Checkpoint</td><td>Flush MemTable</td><td>Flush dirty pages</td><td>Checkpoint LSN</td></tr>
<tr><td>Undo/Redo separation</td><td>None (idempotent updates)</td><td>Both</td><td>Single WAL</td></tr>
<tr><td>Compaction</td><td>Background merge</td><td>Buffer flushing</td><td>None</td></tr>
<tr><td>Concurrency</td><td>Multi-threaded</td><td>Multi-threaded</td><td>Multi-process</td></tr>
<tr><td>Designed for</td><td>Embedded / SSD / Key-value</td><td>OLTP relational</td><td>OLTP relational</td></tr>
</tbody></table>
</div>
<hr />
<h2 id="15-design-insights"><a class="header" href="#15-design-insights"><strong>15. Design Insights</strong></a></h2>
<ul>
<li><strong>Log-structured design</strong> turns random writes into sequential appends - perfect for SSDs.</li>
<li><strong>Compaction replaces checkpointing and vacuuming</strong>, simplifying recovery logic.</li>
<li><strong>Crash safety</strong> is simple - replay WALs, rebuild MemTables.</li>
<li><strong>No undo logs</strong> needed - LSM updates are idempotent.</li>
</ul>
<p>RocksDB’s design favors <strong>high throughput and low write amplification</strong> at the cost of higher background I/O (compaction).</p>
<hr />
<h2 id="16-references"><a class="header" href="#16-references"><strong>16. References</strong></a></h2>
<ul>
<li><a href="https://github.com/facebook/rocksdb/wiki/Write-Ahead-Log">RocksDB Wiki – Write-Ahead Log</a></li>
<li><a href="https://github.com/facebook/rocksdb/blob/main/db/db_impl_open.cc">RocksDB Source Code – db_impl/db_impl_open.cc</a></li>
<li><a href="https://github.com/facebook/rocksdb/wiki/Compaction">RocksDB Compaction Overview</a></li>
<li><a href="https://github.com/google/leveldb/blob/main/db/log_format.h">LevelDB Log Format</a></li>
<li><a href="https://github.com/facebook/mysql-5.6/wiki/MyRocks-Introduction">MyRocks Design Docs</a></li>
</ul>
<hr />
<h2 id="17-summary"><a class="header" href="#17-summary"><strong>17. Summary</strong></a></h2>
<p>RocksDB’s WAL is simple yet powerful - an <strong>append-only, sequential log</strong> that guarantees durability and fast recovery.
It fits naturally into the LSM design philosophy:
<strong>write fast, merge later</strong>.</p>
<p>By combining WAL + MemTable + Compaction, RocksDB achieves:</p>
<ul>
<li>High throughput,</li>
<li>Fast crash recovery,</li>
<li>Tunable durability,</li>
<li>And SSD-optimized performance.</li>
</ul>
<p>RocksDB’s design demonstrates how <strong>WAL principles evolve</strong> from page-based systems to log-structured, distributed storage - a natural evolution of durability mechanisms for modern data systems.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-13-lmdb--embedded-systems"><a class="header" href="#chapter-13-lmdb--embedded-systems">Chapter 13: LMDB &amp; Embedded Systems</a></h1>
<h2 id="131-introduction"><a class="header" href="#131-introduction">13.1 Introduction</a></h2>
<p>LMDB (Lightning Memory-Mapped Database) is a high-performance, ultra-lightweight key-value store designed for reliability and speed. It is particularly well-suited for <strong>embedded systems</strong>, where resource constraints and real-time performance requirements demand efficient storage engines.</p>
<p>In this chapter, we will explore:</p>
<ul>
<li>The design principles of LMDB.</li>
<li>Why LMDB is ideal for embedded environments.</li>
<li>Memory-mapped storage and zero-copy access.</li>
<li>Transactional guarantees (ACID compliance).</li>
<li>Practical examples of using LMDB in embedded applications.</li>
<li>Performance tuning for constrained hardware.</li>
</ul>
<hr />
<h2 id="132-lmdb-fundamentals"><a class="header" href="#132-lmdb-fundamentals">13.2 LMDB Fundamentals</a></h2>
<h3 id="1321-key-features"><a class="header" href="#1321-key-features">13.2.1 Key Features</a></h3>
<ol>
<li><strong>Memory-Mapped Storage</strong>: LMDB maps the database file into the process’s address space. Reads are zero-copy and extremely fast.</li>
<li><strong>ACID Transactions</strong>: LMDB supports fully serializable transactions without locks for readers.</li>
<li><strong>Single-Level B+Tree</strong>: Uses a copy-on-write B+Tree, making writes safe and enabling consistent snapshots.</li>
<li><strong>No Fragmentation</strong>: LMDB avoids internal fragmentation through page-based allocation and reuse.</li>
<li><strong>Read-Optimized</strong>: Multiple readers can access the database simultaneously without blocking.</li>
</ol>
<h3 id="1322-database-structure"><a class="header" href="#1322-database-structure">13.2.2 Database Structure</a></h3>
<ul>
<li>LMDB uses <strong>fixed-size pages</strong>, typically 4 KB.</li>
<li>Keys and values are stored in <strong>B+Tree nodes</strong>.</li>
<li><strong>Copy-on-write</strong> ensures that updates do not overwrite active readers.</li>
</ul>
<hr />
<h2 id="133-why-lmdb-is-ideal-for-embedded-systems"><a class="header" href="#133-why-lmdb-is-ideal-for-embedded-systems">13.3 Why LMDB is Ideal for Embedded Systems</a></h2>
<p>Embedded systems often have limited CPU, memory, and storage. LMDB fits this scenario perfectly because:</p>
<ol>
<li><strong>Minimal Dependencies</strong>: LMDB is a single C library with no external dependencies.</li>
<li><strong>Small Footprint</strong>: It uses a fixed-size memory map and minimal heap allocations.</li>
<li><strong>High Throughput</strong>: Zero-copy reads and sequential writes leverage the OS page cache.</li>
<li><strong>Crash Safety</strong>: Copy-on-write ensures database consistency even on power failure.</li>
<li><strong>Predictable Latency</strong>: Since reads never block, performance is deterministic.</li>
</ol>
<p><strong>Example use cases in embedded systems:</strong></p>
<ul>
<li>IoT devices storing sensor logs.</li>
<li>Routers or network appliances storing configuration and routing tables.</li>
<li>Mobile apps needing reliable local storage.</li>
</ul>
<hr />
<h2 id="134-lmdb-transactions-in-embedded-systems"><a class="header" href="#134-lmdb-transactions-in-embedded-systems">13.4 LMDB Transactions in Embedded Systems</a></h2>
<h3 id="1341-read-only-transactions"><a class="header" href="#1341-read-only-transactions">13.4.1 Read-Only Transactions</a></h3>
<p>Read transactions are <strong>lock-free</strong> and lightweight:</p>
<pre><code class="language-c">MDB_txn *txn;
mdb_txn_begin(env, NULL, MDB_RDONLY, &amp;txn);
MDB_cursor *cursor;
mdb_cursor_open(txn, dbi, &amp;cursor);

MDB_val key, data;
while (mdb_cursor_get(cursor, &amp;key, &amp;data, MDB_NEXT) == 0) {
    printf("Key: %s, Value: %s\n", (char *)key.mv_data, (char *)data.mv_data);
}

mdb_cursor_close(cursor);
mdb_txn_abort(txn); // read-only transactions can be aborted
</code></pre>
<h3 id="1342-read-write-transactions"><a class="header" href="#1342-read-write-transactions">13.4.2 Read-Write Transactions</a></h3>
<p>Write transactions use <strong>copy-on-write</strong>, ensuring no active reader is blocked:</p>
<pre><code class="language-c">MDB_txn *txn;
mdb_txn_begin(env, NULL, 0, &amp;txn);

MDB_val key, data;
key.mv_size = strlen("device_id");
key.mv_data = "device_id";
data.mv_size = strlen("12345");
data.mv_data = "12345";

mdb_put(txn, dbi, &amp;key, &amp;data, 0);
mdb_txn_commit(txn);
</code></pre>
<hr />
<h2 id="135-memory-mapping-considerations"><a class="header" href="#135-memory-mapping-considerations">13.5 Memory Mapping Considerations</a></h2>
<ul>
<li>LMDB maps the entire database into the virtual memory of the process.</li>
<li>Embedded devices often have limited RAM; choose an appropriate <strong>map size</strong>:</li>
</ul>
<pre><code class="language-c">mdb_env_set_mapsize(env, 10 * 1024 * 1024); // 10 MB map
</code></pre>
<ul>
<li>The map size must be larger than your database. Expanding the map requires reopening the environment.</li>
</ul>
<hr />
<h2 id="136-performance-tuning-for-embedded-devices"><a class="header" href="#136-performance-tuning-for-embedded-devices">13.6 Performance Tuning for Embedded Devices</a></h2>
<ol>
<li><strong>Page Size</strong>: Default 4 KB is often ideal; smaller pages reduce RAM usage but may increase fragmentation.</li>
<li><strong>Batch Writes</strong>: Group multiple writes into a single transaction to minimize disk I/O.</li>
<li><strong>Read-Only Transactions</strong>: Use multiple concurrent readers for analytics or monitoring.</li>
<li><strong>Avoid Frequent Map Resizing</strong>: Pre-allocate a map large enough for future growth.</li>
</ol>
<hr />
<h2 id="137-lmdb-vs-other-embedded-databases"><a class="header" href="#137-lmdb-vs-other-embedded-databases">13.7 LMDB vs Other Embedded Databases</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Feature</th><th>LMDB</th><th>SQLite</th><th>RocksDB</th></tr></thead><tbody>
<tr><td>Read Performance</td><td>Excellent</td><td>Good</td><td>Good</td></tr>
<tr><td>Write Performance</td><td>Good</td><td>Moderate</td><td>Excellent</td></tr>
<tr><td>Memory Usage</td><td>Low</td><td>Low</td><td>Moderate</td></tr>
<tr><td>ACID Compliance</td><td>Yes</td><td>Yes</td><td>Yes</td></tr>
<tr><td>Complexity</td><td>Low</td><td>Low</td><td>High</td></tr>
<tr><td>Embedded-Friendly</td><td>Excellent</td><td>Excellent</td><td>Moderate</td></tr>
</tbody></table>
</div>
<p>LMDB is <strong>read-heavy optimized</strong> and excels in environments where reads dominate writes.</p>
<hr />
<h2 id="138-embedded-system-example-iot-device-log-storage"><a class="header" href="#138-embedded-system-example-iot-device-log-storage">13.8 Embedded System Example: IoT Device Log Storage</a></h2>
<p>Suppose we want to store sensor readings in an IoT device:</p>
<pre><code class="language-c">#include &lt;stdio.h&gt;
#include &lt;lmdb.h&gt;

int main() {
    MDB_env *env;
    MDB_dbi dbi;
    MDB_txn *txn;

    mdb_env_create(&amp;env);
    mdb_env_set_mapsize(env, 2 * 1024 * 1024); // 2 MB
    mdb_env_open(env, "./sensor_db", 0, 0664);

    mdb_txn_begin(env, NULL, 0, &amp;txn);
    mdb_dbi_open(txn, NULL, 0, &amp;dbi);

    MDB_val key, data;
    key.mv_size = sizeof(int);
    key.mv_data = &amp;(int){1};
    data.mv_size = sizeof(double);
    double temp = 25.3;
    data.mv_data = &amp;temp;

    mdb_put(txn, dbi, &amp;key, &amp;data, 0);
    mdb_txn_commit(txn);

    mdb_dbi_close(env, dbi);
    mdb_env_close(env);
    return 0;
}
</code></pre>
<p><strong>Explanation:</strong></p>
<ul>
<li>Each sensor reading is stored as a key-value pair.</li>
<li>LMDB ensures writes are atomic and consistent.</li>
<li>Reading the database requires no locks, even while writes happen.</li>
</ul>
<hr />
<h2 id="139-summary"><a class="header" href="#139-summary">13.9 Summary</a></h2>
<ul>
<li>LMDB is a <strong>lightweight, reliable, and fast</strong> key-value store ideal for embedded systems.</li>
<li>Its <strong>memory-mapped architecture</strong> enables zero-copy reads and high throughput.</li>
<li>Copy-on-write and ACID transactions make it <strong>crash-safe</strong>.</li>
<li>Performance tuning is crucial for constrained devices: map size, batch writes, and read optimization.</li>
<li>LMDB is widely used in embedded software, IoT devices, mobile apps, and network appliances.</li>
</ul>
<hr />
<h2 id="1310-lmdb-transactional-design-and-wal-like-mechanism"><a class="header" href="#1310-lmdb-transactional-design-and-wal-like-mechanism">13.10 LMDB Transactional Design and WAL-like Mechanism</a></h2>
<p>Unlike many traditional databases, LMDB does <strong>not use a separate Write-Ahead Log (WAL) file</strong>. Instead, it achieves <strong>atomicity, durability, and crash safety</strong> using <strong>memory-mapped files and a copy-on-write (COW) B+Tree</strong>. Let’s break this down.</p>
<hr />
<h3 id="13101-copy-on-write-cow-btree"><a class="header" href="#13101-copy-on-write-cow-btree">13.10.1 Copy-on-Write (COW) B+Tree</a></h3>
<p>LMDB’s core design revolves around a <strong>COW B+Tree</strong>, which ensures that <strong>writes never overwrite existing data in place</strong>:</p>
<ol>
<li>Each page in the database file is <strong>immutable once written</strong>.</li>
<li>Updates create <strong>new copies of the affected pages</strong>, leaving readers untouched.</li>
<li>Once the transaction commits, LMDB <strong>updates the root pointer</strong> to the new B+Tree root.</li>
<li>Readers always see a <strong>consistent snapshot</strong> of the database, even while writers are active.</li>
</ol>
<p><strong>Advantages of this design:</strong></p>
<ul>
<li><strong>Readers never block writers</strong>.</li>
<li><strong>Atomic commits</strong>: either the root pointer update succeeds or fails; partial writes are never visible.</li>
<li><strong>Crash safety without WAL</strong>: since old pages remain intact until commit, the database can always revert to the previous consistent state.</li>
</ul>
<hr />
<h3 id="13102-pseudo-wal-mechanism"><a class="header" href="#13102-pseudo-wal-mechanism">13.10.2 Pseudo-WAL Mechanism</a></h3>
<p>While LMDB doesn’t have a traditional WAL, it mimics <strong>WAL functionality</strong> in a lightweight way:</p>
<ul>
<li><strong>Transactional updates</strong>: New pages are written to free space in the memory-mapped file.</li>
<li><strong>Durability</strong>: On commit, LMDB updates the <strong>root page number</strong> in the database header and optionally flushes pages to disk (<code>mdb_env_sync</code>).</li>
<li><strong>Atomic root update</strong>: The root pointer acts as a single commit record, similar to the WAL commit record.</li>
<li><strong>Rollback support</strong>: If a crash occurs, uncommitted pages are simply ignored because the root pointer still points to the old tree.</li>
</ul>
<p><strong>Visualization:</strong></p>
<pre><code>[Old Tree Pages]         [New Tree Pages]
      |                        |
      v                        v
   Reader sees             Writer modifies pages
   Old root pointer          (COW)
                              |
                              v
                        Root pointer updated -&gt; commit
</code></pre>
<hr />
<h3 id="13103-transaction-commit-process"><a class="header" href="#13103-transaction-commit-process">13.10.3 Transaction Commit Process</a></h3>
<ol>
<li>
<p><strong>Begin transaction</strong>: Allocate new pages for updates.</p>
</li>
<li>
<p><strong>Write data to pages</strong>: Updates occur in private copies.</p>
</li>
<li>
<p><strong>Commit transaction</strong>:</p>
<ul>
<li>Flush modified pages to disk.</li>
<li>Atomically update root page pointer in DB header.</li>
</ul>
</li>
<li>
<p><strong>Readers continue using old root</strong> until next transaction.</p>
</li>
<li>
<p><strong>Free old pages</strong>: Pages from previous transactions that are no longer referenced are recycled.</p>
</li>
</ol>
<p>This ensures <strong>ACID compliance</strong>:</p>
<div class="table-wrapper"><table><thead><tr><th>Property</th><th>How LMDB Implements It</th></tr></thead><tbody>
<tr><td>Atomicity</td><td>Root page pointer updated atomically; either old or new tree visible</td></tr>
<tr><td>Consistency</td><td>B+Tree structure maintained; page-level integrity checks</td></tr>
<tr><td>Isolation</td><td>Readers see snapshot of database at transaction start</td></tr>
<tr><td>Durability</td><td>Pages written to disk and root pointer updated atomically</td></tr>
</tbody></table>
</div>
<hr />
<h3 id="13104-page-management--garbage-collection"><a class="header" href="#13104-page-management--garbage-collection">13.10.4 Page Management &amp; Garbage Collection</a></h3>
<p>LMDB uses a <strong>freelist system</strong> instead of log compaction:</p>
<ul>
<li>Modified pages are tracked during write transactions.</li>
<li>After a transaction commits, <strong>unused pages</strong> are added to a free list.</li>
<li>Next transaction can reuse pages without expanding the file.</li>
<li>This avoids <strong>fragmentation</strong> and ensures predictable memory usage.</li>
</ul>
<hr />
<h3 id="13105-comparison-to-traditional-wal"><a class="header" href="#13105-comparison-to-traditional-wal">13.10.5 Comparison to Traditional WAL</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Feature</th><th>Traditional WAL</th><th>LMDB (COW)</th></tr></thead><tbody>
<tr><td>Separate log file</td><td>Yes</td><td>No</td></tr>
<tr><td>Write amplification</td><td>Higher</td><td>Low</td></tr>
<tr><td>Crash recovery</td><td>Replay WAL</td><td>Root pointer rollback</td></tr>
<tr><td>Read/write concurrency</td><td>Often blocked</td><td>Readers never blocked</td></tr>
<tr><td>Disk space overhead</td><td>Extra log file needed</td><td>Only old pages (freelist)</td></tr>
</tbody></table>
</div>
<hr />
<h3 id="13106-performance-implications"><a class="header" href="#13106-performance-implications">13.10.6 Performance Implications</a></h3>
<ol>
<li>
<p><strong>Reads are extremely fast</strong>:</p>
<ul>
<li>Memory-mapped pages allow direct access; no WAL replay required.</li>
</ul>
</li>
<li>
<p><strong>Writes are sequential and append-only</strong>:</p>
<ul>
<li>Copy-on-write writes to free pages; minimal disk seek.</li>
</ul>
</li>
<li>
<p><strong>Crash recovery is instant</strong>:</p>
<ul>
<li>Database can be reopened immediately; no log replay.</li>
</ul>
</li>
</ol>
<hr />
<h3 id="13107-practical-tips-for-embedded-systems"><a class="header" href="#13107-practical-tips-for-embedded-systems">13.10.7 Practical Tips for Embedded Systems</a></h3>
<ul>
<li><strong>Map size</strong>: Ensure the memory-mapped file is large enough to accommodate growth and avoid frequent remapping.</li>
<li><strong>Commit frequency</strong>: Frequent small transactions can be slower due to flushing overhead; batch updates if possible.</li>
<li><strong>Read-heavy applications</strong>: LMDB excels because readers are non-blocking.</li>
<li><strong>Write-heavy embedded workloads</strong>: Optimize page size and map size; consider using multiple databases for partitioning.</li>
</ul>
<hr />
<h3 id="1311-summary-of-lmdb-wal-design"><a class="header" href="#1311-summary-of-lmdb-wal-design">13.11 Summary of LMDB WAL Design</a></h3>
<ul>
<li>LMDB <strong>does not need a separate WAL</strong>; copy-on-write B+Tree + atomic root pointer ensures crash-safe commits.</li>
<li>Embedded systems benefit from <strong>low memory usage, predictable performance, and high read concurrency</strong>.</li>
<li>LMDB’s approach eliminates the complexity of WAL management while providing <strong>full ACID guarantees</strong>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="part-iv-wal-in-distributed-systems--introduction"><a class="header" href="#part-iv-wal-in-distributed-systems--introduction">Part IV: WAL in Distributed Systems – Introduction</a></h1>
<p>As systems scale beyond a single machine, the challenges of <strong>data consistency, fault tolerance, and availability</strong> become central to database design. While WAL (Write-Ahead Logging) ensures <strong>durable and atomic updates</strong> in single-node databases, distributed systems introduce new complexities: multiple nodes, network partitions, and concurrent updates.</p>
<p>In this part, we explore how WAL principles extend to <strong>distributed environments</strong>, enabling <strong>replication, consensus, and high availability</strong>. We will also examine how modern distributed databases implement WAL-like mechanisms to maintain consistency across nodes while tolerating failures.</p>
<p><strong>Key Themes of Part IV:</strong></p>
<ol>
<li>
<p><strong>Replication &amp; Consensus</strong> (Chapter 14)
Distributed systems require coordinated updates across multiple nodes. This chapter will cover how WAL interacts with <strong>replication protocols</strong> and <strong>consensus algorithms</strong> (like Raft and Paxos) to guarantee consistency, even in the presence of node failures or network partitions. You will learn about:</p>
<ul>
<li>Synchronous vs. asynchronous replication.</li>
<li>How WAL logs are transmitted and applied across replicas.</li>
<li>Commit strategies and consistency guarantees in distributed setups.</li>
</ul>
</li>
<li>
<p><strong>Distributed Databases</strong> (Chapter 15)
Large-scale distributed databases rely on WAL-inspired mechanisms to ensure data durability and correctness. This chapter will explore:</p>
<ul>
<li>How distributed databases handle write ordering, conflict resolution, and crash recovery.</li>
<li>Design patterns for WAL in sharded or partitioned data environments.</li>
<li>Real-world examples of distributed WAL usage in systems like <strong>Cassandra, CockroachDB, and Spanner</strong>.</li>
</ul>
</li>
</ol>
<p>By the end of this part, you will have a <strong>deep understanding of WAL beyond single-node systems</strong>, learning how its principles scale to maintain <strong>reliability, durability, and consistency</strong> in distributed architectures.</p>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h2 id="chapter-14-replication--consensus"><a class="header" href="#chapter-14-replication--consensus">Chapter 14: Replication &amp; Consensus</a></h2>
<h3 id="introduction"><a class="header" href="#introduction"><strong>Introduction</strong></a></h3>
<p>When databases grow beyond a single machine, <strong>durability and correctness</strong> become a team sport.
A single crash can be recovered using WAL, but what happens when your database runs across <strong>multiple machines</strong>?</p>
<p>How do all replicas agree on <em>which transactions were committed</em>?
How do we ensure that a follower node doesn’t replay half-written data?
And how do we make sure that every replica applies the same log entries in the same order?</p>
<p>These questions are answered by the combination of <strong>Write-Ahead Logs (WAL)</strong> and <strong>Consensus Protocols</strong> like <strong>Raft</strong>, <strong>Paxos</strong>, or <strong>ZAB</strong> (used by ZooKeeper).</p>
<p>Replication + Consensus = <em>Durability &amp; Availability across machines</em>.</p>
<hr />
<h2 id="1-the-journey-from-single-node-to-distributed-wal"><a class="header" href="#1-the-journey-from-single-node-to-distributed-wal"><strong>1. The Journey from Single Node to Distributed WAL</strong></a></h2>
<p>Let’s recall how WAL works in a single node:</p>
<pre><code>+--------------------+
|  User Transaction  |
+--------------------+
          |
          v
  +------------------+
  | Write to WAL Log |   (sequential disk write)
  +------------------+
          |
          v
  +------------------+
  | Update Data Page |
  +------------------+
</code></pre>
<p>When power fails, we recover from WAL by replaying committed transactions.</p>
<p>But in a <strong>distributed setup</strong>, we want <em>multiple replicas</em> to have the same log - so that if one node dies, another can take over.</p>
<p>This requires <strong>replication of WAL entries</strong>.</p>
<hr />
<h2 id="2-replicating-the-wal"><a class="header" href="#2-replicating-the-wal"><strong>2. Replicating the WAL</strong></a></h2>
<p>In distributed databases like <strong>PostgreSQL</strong>, <strong>MongoDB</strong>, <strong>CockroachDB</strong>, and <strong>Etcd</strong>, replication happens at the <strong>log level</strong>.</p>
<p>Instead of sending the final data pages, the system sends <strong>WAL entries</strong> - the smallest, most atomic units of change.</p>
<p>This makes replication:</p>
<ul>
<li><strong>Lightweight</strong> (logs are small)</li>
<li><strong>Deterministic</strong> (everyone replays the same sequence)</li>
<li><strong>Efficient</strong> (sequential write + sequential replay)</li>
</ul>
<h3 id="example-log-replication-timeline"><a class="header" href="#example-log-replication-timeline">Example: Log Replication Timeline</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Time</th><th>Leader (Node A)</th><th>Follower (Node B)</th></tr></thead><tbody>
<tr><td>T1</td><td>Receives <code>UPDATE x=5</code></td><td></td></tr>
<tr><td>T2</td><td>Appends to WAL: <code>log[1] = "x=5"</code></td><td></td></tr>
<tr><td>T3</td><td>Sends log[1] to follower</td><td>Receives log[1]</td></tr>
<tr><td>T4</td><td>Both mark log[1] as committed</td><td>Replays to data</td></tr>
</tbody></table>
</div>
<p>After this, both nodes have:</p>
<pre><code>WAL = [log1: x=5]
Data = x=5
</code></pre>
<p>Thus, replication ensures <em>consistency</em> and <em>fault tolerance</em>.</p>
<hr />
<h2 id="3-synchronous-vs-asynchronous-replication"><a class="header" href="#3-synchronous-vs-asynchronous-replication"><strong>3. Synchronous vs Asynchronous Replication</strong></a></h2>
<p>Replication can occur in different modes:</p>
<div class="table-wrapper"><table><thead><tr><th>Type</th><th>Description</th><th>Tradeoff</th></tr></thead><tbody>
<tr><td><strong>Synchronous</strong></td><td>Leader waits until all replicas confirm write</td><td>Safer but slower</td></tr>
<tr><td><strong>Asynchronous</strong></td><td>Leader commits locally, sends later</td><td>Faster but may lose data if leader crashes</td></tr>
<tr><td><strong>Semi-sync (Hybrid)</strong></td><td>Wait for at least one replica</td><td>Middle ground</td></tr>
</tbody></table>
</div>
<p>Example in PostgreSQL:</p>
<pre><code class="language-sql">synchronous_commit = on
</code></pre>
<p>ensures the transaction waits until at least one standby writes WAL to disk.</p>
<hr />
<h3 id="diagram-wal-replication-modes"><a class="header" href="#diagram-wal-replication-modes"><strong>Diagram: WAL Replication Modes</strong></a></h3>
<pre><code>User ----&gt; Leader WAL ----&gt; Follower WAL
            |                    |
         Commit wait         Commit async
</code></pre>
<p>Synchronous = strong consistency
Asynchronous = eventual consistency</p>
<hr />
<h2 id="4-the-need-for-consensus"><a class="header" href="#4-the-need-for-consensus"><strong>4. The Need for Consensus</strong></a></h2>
<p>Simple replication works well - until the leader crashes.</p>
<p>Imagine three nodes: A, B, C.
A is the leader. It appends a new WAL entry (<code>log[5]</code>), sends it to B and C, but crashes before committing.</p>
<p>Now who decides if <code>log[5]</code> is valid?</p>
<p>If B and C disagree, we risk <strong>split-brain</strong> - where two leaders think they are correct.</p>
<p>That’s where <strong>consensus protocols</strong> like <strong>Raft</strong> and <strong>Paxos</strong> come in.</p>
<p>They ensure:</p>
<ol>
<li>All nodes agree on the same WAL order.</li>
<li>No committed log entry is lost.</li>
<li>Only one leader can exist at a time.</li>
</ol>
<hr />
<h2 id="5-wal--raft-a-simple-analogy"><a class="header" href="#5-wal--raft-a-simple-analogy"><strong>5. WAL + Raft: A Simple Analogy</strong></a></h2>
<p>Think of Raft as a <strong>distributed WAL controller</strong>.</p>
<p>Each Raft node maintains a <strong>replicated log</strong>.
Every client command becomes a <strong>log entry</strong>, replicated to all nodes before it’s committed.</p>
<pre><code>Client Command -&gt; Log Entry -&gt; Append to WAL on all replicas -&gt; Commit
</code></pre>
<h3 id="raft-log-example"><a class="header" href="#raft-log-example"><strong>Raft Log Example</strong></a></h3>
<div class="table-wrapper"><table><thead><tr><th>Index</th><th>Term</th><th>Command</th><th>State</th></tr></thead><tbody>
<tr><td>1</td><td>1</td><td><code>SET x=5</code></td><td>Committed</td></tr>
<tr><td>2</td><td>1</td><td><code>SET y=10</code></td><td>Committed</td></tr>
<tr><td>3</td><td>2</td><td><code>DELETE y</code></td><td>Pending</td></tr>
</tbody></table>
</div>
<p>Each node’s WAL might look like:</p>
<pre><code>Node A WAL: [1, 2, 3]
Node B WAL: [1, 2]
Node C WAL: [1, 2, 3]
</code></pre>
<p>Once a majority (2/3) of nodes acknowledge entry 3 → it’s <strong>committed</strong>.</p>
<p>Then all replicas replay it in order.</p>
<hr />
<h3 id="diagram-raft-commit-flow"><a class="header" href="#diagram-raft-commit-flow"><strong>Diagram: Raft Commit Flow</strong></a></h3>
<pre><code>Client
   |
   v
+----------+
|  Leader  |---- AppendEntries ---&gt; [Follower 1]
| (WAL Log)|---- AppendEntries ---&gt; [Follower 2]
+----------+
       |
       +--&gt; Commit if majority ack
</code></pre>
<hr />
<h2 id="6-log-matching--term-rules"><a class="header" href="#6-log-matching--term-rules"><strong>6. Log Matching &amp; Term Rules</strong></a></h2>
<p>Raft ensures safety using <strong>two key rules</strong>:</p>
<ol>
<li>
<p><strong>Log Matching Property</strong>
If two logs have the same index and term, their entire prefix is identical.
→ Prevents divergent history.</p>
</li>
<li>
<p><strong>Leader Completeness Property</strong>
A newly elected leader must contain all committed entries.</p>
</li>
</ol>
<p>These rules ensure that WALs never “fork” or contain partial transactions.</p>
<hr />
<h2 id="7-leader-election"><a class="header" href="#7-leader-election"><strong>7. Leader Election</strong></a></h2>
<p>When a node crashes, others detect a timeout and hold an <strong>election</strong>.</p>
<p>Each node:</p>
<ol>
<li>Increments its term</li>
<li>Votes for itself</li>
<li>Requests votes from others</li>
<li>First to reach majority becomes the <strong>leader</strong></li>
</ol>
<p>Once a leader is elected, it resumes WAL replication from the last known index.</p>
<h3 id="diagram-election-timeline"><a class="header" href="#diagram-election-timeline"><strong>Diagram: Election Timeline</strong></a></h3>
<pre><code>[A] --crash--&gt; [B,C] start election
[B] term=3 votes=2 --&gt; new leader
[C] term=3 follows B
</code></pre>
<p>Then replication continues from <code>lastLogIndex</code>.</p>
<hr />
<h2 id="8-checkpoints-and-snapshots-in-distributed-wal"><a class="header" href="#8-checkpoints-and-snapshots-in-distributed-wal"><strong>8. Checkpoints and Snapshots in Distributed WAL</strong></a></h2>
<p>As the WAL grows, keeping all entries forever is expensive.</p>
<p>To solve this, systems periodically take <strong>snapshots</strong> or <strong>checkpoints</strong>, truncating old logs that are already applied everywhere.</p>
<p>Example:</p>
<pre><code>WAL: [1..10000]
Snapshot: State after log 10000
Truncate logs &lt;= 10000
</code></pre>
<p>On recovery, a node loads the latest snapshot, then replays newer WAL entries.</p>
<hr />
<h2 id="9-how-consensus-prevents-split-brain"><a class="header" href="#9-how-consensus-prevents-split-brain"><strong>9. How Consensus Prevents Split-Brain</strong></a></h2>
<p>Let’s revisit the crash scenario:</p>
<ul>
<li>Node A (leader) had WAL up to entry #5</li>
<li>Node B, C had entries up to #4</li>
</ul>
<p>A crashes before committing #5.</p>
<p>Without consensus, B might become leader and commit different #5.
Now, A and B have divergent WALs → <strong>split-brain</strong>.</p>
<p>Raft prevents this:</p>
<ul>
<li>New leader election requires <strong>majority agreement</strong>.</li>
<li>Majority never acknowledged #5 → new leader’s log excludes it.</li>
<li>When A rejoins, it <strong>rolls back</strong> entry #5 to match majority.</li>
</ul>
<p>Thus, WALs converge to a single, agreed-upon sequence.</p>
<hr />
<h2 id="10-real-world-implementations"><a class="header" href="#10-real-world-implementations"><strong>10. Real-World Implementations</strong></a></h2>
<div class="table-wrapper"><table><thead><tr><th>System</th><th>Consensus</th><th>WAL Layer</th><th>Notes</th></tr></thead><tbody>
<tr><td><strong>Etcd</strong></td><td>Raft</td><td>BoltDB WAL</td><td>Key-value store for k8s</td></tr>
<tr><td><strong>CockroachDB</strong></td><td>Raft</td><td>RocksDB WAL</td><td>Distributed SQL database</td></tr>
<tr><td><strong>TiDB</strong></td><td>Raft</td><td>Custom WAL</td><td>Multi-raft per region</td></tr>
<tr><td><strong>PostgreSQL</strong></td><td>None (primary/standby)</td><td>Physical WAL shipping</td><td>Optional synchronous mode</td></tr>
<tr><td><strong>MongoDB</strong></td><td>Custom (Raft-like)</td><td>Oplog (logical WAL)</td><td>High availability</td></tr>
</tbody></table>
</div>
<hr />
<h3 id="example-postgresql-streaming-replication"><a class="header" href="#example-postgresql-streaming-replication">Example: PostgreSQL Streaming Replication</a></h3>
<p>Postgres replicates <em>WAL segments</em> over TCP using a background process (<code>walsender</code> → <code>walreceiver</code>).</p>
<pre><code>Primary (WAL Writer) ---&gt; Standby (WAL Receiver)
      write WAL                store and replay
</code></pre>
<p>Command to check replication status:</p>
<pre><code class="language-sql">SELECT * FROM pg_stat_replication;
</code></pre>
<p>This form of replication is <em>physical</em> - byte-for-byte log shipping.
Systems like MongoDB use <em>logical replication</em> (replicating operations instead).</p>
<hr />
<h2 id="11-summary-wal-as-the-foundation-of-distributed-consensus"><a class="header" href="#11-summary-wal-as-the-foundation-of-distributed-consensus"><strong>11. Summary: WAL as the Foundation of Distributed Consensus</strong></a></h2>
<p>At this point, we can summarize the key takeaways:</p>
<div class="table-wrapper"><table><thead><tr><th>Concept</th><th>Description</th></tr></thead><tbody>
<tr><td>WAL</td><td>Source of truth for state changes</td></tr>
<tr><td>Replication</td><td>Copies WAL to other nodes</td></tr>
<tr><td>Consensus</td><td>Ensures all replicas agree on WAL order</td></tr>
<tr><td>Checkpoint</td><td>Compact form of stable state</td></tr>
<tr><td>Recovery</td><td>Reapply committed WAL entries after crash</td></tr>
</tbody></table>
</div>
<p>Together, these ensure that no matter which node fails, the system can <strong>recover</strong>, <strong>agree</strong>, and <strong>continue</strong>.</p>
<hr />
<h2 id="12-mini-example-a-raft-like-wal-simulation"><a class="header" href="#12-mini-example-a-raft-like-wal-simulation"><strong>12. Mini Example: A Raft-Like WAL Simulation</strong></a></h2>
<p>Let’s simulate a mini system in pseudocode:</p>
<pre><code class="language-python">class Node:
    def __init__(self):
        self.log = []
        self.commit_index = 0

    def append(self, entry):
        self.log.append(entry)

    def replicate_to(self, peers):
        for p in peers:
            p.log = self.log.copy()

    def commit(self):
        self.commit_index = len(self.log)

# Leader appends and replicates
leader = Node()
f1, f2 = Node(), Node()

leader.append("SET x=10")
leader.replicate_to([f1, f2])
leader.commit()

print(f1.log, f2.log)
</code></pre>
<p>Output:</p>
<pre><code>['SET x=10'] ['SET x=10']
</code></pre>
<p>This is the essence of replication - distributed agreement on log entries.</p>
<hr />
<h2 id="13-visual-recap-wal--consensus--reliability"><a class="header" href="#13-visual-recap-wal--consensus--reliability"><strong>13. Visual Recap: WAL + Consensus = Reliability</strong></a></h2>
<pre><code>[ Client ]
    |
    v
[ Leader ]
   |
   +--&gt; Append to WAL
   +--&gt; Replicate to followers
   +--&gt; Wait for majority ACK
   |
   +--&gt; Commit
   |
   +--&gt; Apply to state machine
</code></pre>
<p>When any node crashes:</p>
<ul>
<li>WAL ensures <em>no local data is lost</em></li>
<li>Consensus ensures <em>no global order is broken</em></li>
</ul>
<hr />
<h2 id="14-closing-thoughts"><a class="header" href="#14-closing-thoughts"><strong>14. Closing Thoughts</strong></a></h2>
<p>In single-node databases, WAL protects against <strong>crashes</strong>.
In distributed systems, it protects against <strong>disagreement</strong>.</p>
<p>Replication spreads the WAL.
Consensus ensures <em>everyone sees it the same way</em>.</p>
<p>In the next chapter, we’ll explore how distributed databases like <strong>CockroachDB</strong>, <strong>Spanner</strong>, and <strong>Etcd</strong> extend these ideas - combining WAL, replication, and consensus into <strong>globally consistent transactional systems</strong>.</p>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h2 id="chapter-15-distributed-databases"><a class="header" href="#chapter-15-distributed-databases">Chapter 15: Distributed Databases</a></h2>
<hr />
<h3 id="introduction-when-one-wal-isnt-enough"><a class="header" href="#introduction-when-one-wal-isnt-enough"><strong>Introduction: When One WAL Isn’t Enough</strong></a></h3>
<p>In a single-node system, the <strong>Write-Ahead Log (WAL)</strong> protects us from crashes.
In a replicated cluster, it ensures all nodes agree on what happened.</p>
<p>But what happens when your database isn’t just <em>replicated</em> - it’s <em>distributed</em> across multiple <strong>regions, continents, and data centers</strong>?</p>
<p>This is the world of <strong>distributed databases</strong>, where WAL evolves into a <strong>global coordination mechanism</strong> that guarantees consistency even across thousands of machines.</p>
<p>Let’s start with a simple idea.</p>
<hr />
<h3 id="1-the-big-picture-from-local-logs-to-global-logs"><a class="header" href="#1-the-big-picture-from-local-logs-to-global-logs"><strong>1. The Big Picture: From Local Logs to Global Logs</strong></a></h3>
<p>A distributed database is essentially a <strong>network of smaller databases</strong>, called <em>nodes</em> or <em>replicas</em>, that coordinate through <strong>logs</strong>.</p>
<p>Each node has:</p>
<ul>
<li>Its own <strong>local WAL</strong> (for crash recovery)</li>
<li>And participates in a <strong>distributed WAL</strong> (for global order)</li>
</ul>
<pre><code>        ┌──────────┐
        │ Client   │
        └────┬─────┘
             │
             ▼
   ┌───────────────┐
   │ Distributed   │   (Global WAL / Consensus)
   │   Log Layer   │
   └────┬────┬─────┘
        │    │
        ▼    ▼
 ┌────────┐ ┌────────┐
 │ Node A │ │ Node B │
 │ Local  │ │ Local  │
 │  WAL   │ │  WAL   │
 └────────┘ └────────┘
</code></pre>
<p>Every write first lands in a <strong>per-node WAL</strong>, but then must be <strong>agreed upon globally</strong> through replication and consensus.</p>
<p>This ensures that:</p>
<ul>
<li>Every replica applies changes in the same order</li>
<li>No node sees “phantom” data or missed writes</li>
<li>The system recovers even if entire regions fail</li>
</ul>
<hr />
<h3 id="2-key-principles-of-distributed-wal"><a class="header" href="#2-key-principles-of-distributed-wal"><strong>2. Key Principles of Distributed WAL</strong></a></h3>
<div class="table-wrapper"><table><thead><tr><th>Concept</th><th>Description</th></tr></thead><tbody>
<tr><td><strong>Local Durability</strong></td><td>Each node logs changes to its own WAL before applying them</td></tr>
<tr><td><strong>Replication</strong></td><td>WAL entries are copied to followers</td></tr>
<tr><td><strong>Consensus</strong></td><td>A group of nodes agrees on the order of commits</td></tr>
<tr><td><strong>Sharding</strong></td><td>The keyspace is split into ranges or partitions, each with its own WAL</td></tr>
<tr><td><strong>Global Coordination</strong></td><td>Clock or timestamp protocols maintain ordering across shards</td></tr>
</tbody></table>
</div>
<p>Distributed databases integrate these layers into a single cohesive system.</p>
<hr />
<h3 id="3-wal-at-the-shard-level"><a class="header" href="#3-wal-at-the-shard-level"><strong>3. WAL at the Shard Level</strong></a></h3>
<p>Distributed databases don’t use one giant log for the whole system - that would be impossible to scale.</p>
<p>Instead, they split the data into <strong>shards</strong> or <strong>ranges</strong>, each maintaining its <strong>own independent WAL</strong>.</p>
<p>Example:</p>
<div class="table-wrapper"><table><thead><tr><th>Shard</th><th>Data Range</th><th>WAL File</th><th>Leader Node</th></tr></thead><tbody>
<tr><td>1</td><td>keys [0-1000)</td><td>wal_shard_1.log</td><td>Node A</td></tr>
<tr><td>2</td><td>keys [1000-2000)</td><td>wal_shard_2.log</td><td>Node B</td></tr>
<tr><td>3</td><td>keys [2000-3000)</td><td>wal_shard_3.log</td><td>Node C</td></tr>
</tbody></table>
</div>
<p>Each WAL is replicated and managed by its own mini-consensus group (like Raft).</p>
<p>This design is called <strong>Multi-Raft</strong> - and is used in <strong>CockroachDB</strong>, <strong>TiDB</strong>, and <strong>YugabyteDB</strong>.</p>
<hr />
<h3 id="4-example-multi-raft-in-action"><a class="header" href="#4-example-multi-raft-in-action"><strong>4. Example: Multi-Raft in Action</strong></a></h3>
<p>Let’s visualize it:</p>
<pre><code>Shard 1: Keys 0–1000
   A(Leader)
   /     \
  B       C

Shard 2: Keys 1000–2000
   B(Leader)
   /     \
  A       C

Shard 3: Keys 2000–3000
   C(Leader)
   /     \
  A       B
</code></pre>
<p>Each shard’s WAL operates independently:</p>
<ul>
<li>Writes to shard 1 only affect <code>wal_1</code></li>
<li>Writes to shard 2 affect <code>wal_2</code></li>
</ul>
<p>This allows <strong>parallel replication and recovery</strong>.</p>
<p>If Node B fails, only the shards it leads are impacted.</p>
<hr />
<h3 id="5-global-transactions-and-distributed-wal-coordination"><a class="header" href="#5-global-transactions-and-distributed-wal-coordination"><strong>5. Global Transactions and Distributed WAL Coordination</strong></a></h3>
<p>Now comes the tricky part - what if a transaction touches <strong>multiple shards</strong>?</p>
<p>Example:</p>
<pre><code class="language-sql">BEGIN;
UPDATE accounts SET balance = balance - 100 WHERE id = 42;   -- Shard 1
UPDATE accounts SET balance = balance + 100 WHERE id = 900;   -- Shard 2
COMMIT;
</code></pre>
<p>This is a <strong>distributed transaction</strong> - and the WAL needs to ensure atomicity across multiple logs.</p>
<p>That’s where <strong>two-phase commit (2PC)</strong> and <strong>timestamp ordering</strong> protocols come in.</p>
<hr />
<h3 id="6-two-phase-commit-2pc-and-wal"><a class="header" href="#6-two-phase-commit-2pc-and-wal"><strong>6. Two-Phase Commit (2PC) and WAL</strong></a></h3>
<p>2PC ensures that either <em>all</em> shards commit or <em>none</em> do.
It works as follows:</p>
<h4 id="step-1-prepare-phase"><a class="header" href="#step-1-prepare-phase">Step 1: Prepare Phase</a></h4>
<ul>
<li>Coordinator sends “prepare” to all shards.</li>
<li>Each shard appends a “prepare” record to its WAL and replies “ready”.</li>
</ul>
<h4 id="step-2-commit-phase"><a class="header" href="#step-2-commit-phase">Step 2: Commit Phase</a></h4>
<ul>
<li>If all shards are ready, coordinator sends “commit”.</li>
<li>Each shard appends a “commit” record to WAL and applies the transaction.</li>
</ul>
<p><strong>Example WAL Entries:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Shard</th><th>WAL Entries</th></tr></thead><tbody>
<tr><td>1</td><td>prepare(tx42), commit(tx42)</td></tr>
<tr><td>2</td><td>prepare(tx42), commit(tx42)</td></tr>
</tbody></table>
</div>
<p>If a crash happens mid-way, the WAL tells each shard exactly what phase it was in - ensuring recovery consistency.</p>
<hr />
<h3 id="7-global-timestamps-and-wal-ordering"><a class="header" href="#7-global-timestamps-and-wal-ordering"><strong>7. Global Timestamps and WAL Ordering</strong></a></h3>
<p>Some modern databases, like <strong>Google Spanner</strong>, avoid traditional 2PC by assigning <strong>globally synchronized timestamps</strong> using atomic clocks and GPS.</p>
<p>Each transaction gets a unique commit timestamp <code>Tcommit</code>, and WAL entries are ordered accordingly.</p>
<p><strong>Spanner WAL Structure (simplified):</strong></p>
<pre><code>Tcommit=10001: UPDATE accounts SET balance=...
Tcommit=10002: INSERT INTO transfers ...
</code></pre>
<p>Because every node’s clock is synchronized within microseconds, Spanner can ensure that:</p>
<ul>
<li>All replicas agree on commit order</li>
<li>Reads at timestamp T see a consistent snapshot</li>
</ul>
<p>This turns WAL into a <strong>time-ordered global log</strong>.</p>
<hr />
<h3 id="8-wal-and-snapshot-isolation"><a class="header" href="#8-wal-and-snapshot-isolation"><strong>8. WAL and Snapshot Isolation</strong></a></h3>
<p>Many distributed databases provide <strong>Snapshot Isolation (SI)</strong> or <strong>Serializable Isolation</strong>.</p>
<p>They use WAL + MVCC (multi-version concurrency control):</p>
<ul>
<li>Each write appends a new version to WAL</li>
<li>Readers can choose a consistent timestamp</li>
<li>No global locking is needed</li>
</ul>
<p><strong>Example:</strong></p>
<div class="table-wrapper"><table><thead><tr><th>Version</th><th>Key</th><th>Value</th><th>CommitTS</th></tr></thead><tbody>
<tr><td>v1</td><td>x</td><td>10</td><td>1001</td></tr>
<tr><td>v2</td><td>x</td><td>12</td><td>1005</td></tr>
</tbody></table>
</div>
<p>A read at T=1002 sees v1.
A read at T=1006 sees v2.</p>
<p>WAL entries store both value and version metadata.</p>
<hr />
<h3 id="9-recovery-across-nodes"><a class="header" href="#9-recovery-across-nodes"><strong>9. Recovery Across Nodes</strong></a></h3>
<p>In distributed systems, recovery means two things:</p>
<ol>
<li><strong>Local recovery:</strong> replay local WAL to restore node’s last known state.</li>
<li><strong>Global recovery:</strong> reconcile with cluster to ensure no stale commits.</li>
</ol>
<p>Steps during node restart:</p>
<ol>
<li>Node replays its WAL → rebuilds local store.</li>
<li>Node contacts peers → requests missing entries (log catch-up).</li>
<li>Consensus layer ensures log alignment.</li>
<li>Node rejoins replication group safely.</li>
</ol>
<p>This process guarantees both <strong>durability</strong> and <strong>consistency</strong>.</p>
<hr />
<h3 id="10-checkpoints-and-compaction-in-distributed-systems"><a class="header" href="#10-checkpoints-and-compaction-in-distributed-systems"><strong>10. Checkpoints and Compaction in Distributed Systems</strong></a></h3>
<p>Just like single-node WALs, distributed logs grow indefinitely.</p>
<p>Hence, each shard periodically:</p>
<ul>
<li>Takes a <strong>snapshot</strong></li>
<li>Stores the latest applied index</li>
<li>Truncates logs up to that point</li>
</ul>
<p>Example:</p>
<pre><code>snapshot_at_index=5000
truncate WAL &lt;= 5000
</code></pre>
<p>For distributed consistency, all nodes in the group must agree on truncation boundaries.</p>
<p>In Raft:</p>
<pre><code class="language-text">InstallSnapshot RPC
</code></pre>
<p>transfers the snapshot to lagging followers to catch them up efficiently.</p>
<hr />
<h3 id="11-real-systems-how-they-use-distributed-wals"><a class="header" href="#11-real-systems-how-they-use-distributed-wals"><strong>11. Real Systems: How They Use Distributed WALs</strong></a></h3>
<div class="table-wrapper"><table><thead><tr><th>System</th><th>WAL Type</th><th>Consensus</th><th>Notes</th></tr></thead><tbody>
<tr><td><strong>Google Spanner</strong></td><td>Global WAL (timestamp ordered)</td><td>Paxos</td><td>True global consistency</td></tr>
<tr><td><strong>CockroachDB</strong></td><td>Multi-Raft WALs</td><td>Raft</td><td>SQL on key-value store</td></tr>
<tr><td><strong>TiDB</strong></td><td>Region-based WAL</td><td>Raft</td><td>Each “region” has its own log</td></tr>
<tr><td><strong>YugabyteDB</strong></td><td>Tablet WAL</td><td>Raft</td><td>Compatible with PostgreSQL</td></tr>
<tr><td><strong>MongoDB</strong></td><td>Oplog (logical WAL)</td><td>Raft-like</td><td>Eventual or strong consistency</td></tr>
<tr><td><strong>FoundationDB</strong></td><td>Deterministic Global Log</td><td>Custom</td><td>Centralized log replication</td></tr>
</tbody></table>
</div>
<p>Let’s look at two examples in more depth.</p>
<hr />
<h3 id="12-example-1-cockroachdbs-distributed-wal-design"><a class="header" href="#12-example-1-cockroachdbs-distributed-wal-design"><strong>12. Example 1: CockroachDB’s Distributed WAL Design</strong></a></h3>
<p>CockroachDB runs a Raft consensus group per range.</p>
<ul>
<li>Each range = independent WAL</li>
<li>Range leaders replicate logs to followers</li>
<li>The KV store (RocksDB) persists them locally</li>
</ul>
<p>When you run:</p>
<pre><code class="language-sql">INSERT INTO users VALUES (1, 'Alice');
</code></pre>
<p>It becomes a Raft log entry in a specific range’s WAL.</p>
<p>CockroachDB guarantees <strong>linearizable consistency</strong> because each Raft log represents a serial history of operations for its range.</p>
<p><strong>Recovery:</strong> If a leader crashes, followers’ WALs elect a new leader and resume from last committed index.</p>
<hr />
<h3 id="13-example-2-google-spanners-truetime--wal"><a class="header" href="#13-example-2-google-spanners-truetime--wal"><strong>13. Example 2: Google Spanner’s TrueTime + WAL</strong></a></h3>
<p>Spanner maintains a <strong>globally consistent WAL</strong> across continents using atomic clocks.</p>
<p>Each mutation:</p>
<ol>
<li>Is assigned a timestamp <code>t</code></li>
<li>Replicated via Paxos</li>
<li>Committed when majority acknowledges</li>
<li>Applied in timestamp order</li>
</ol>
<p>Diagram:</p>
<pre><code>Clients ---&gt; Leaders ---&gt; Paxos Replicas ---&gt; Global WAL
         (assign t)       (commit)           (apply in order)
</code></pre>
<p>This gives <strong>serializability</strong> with <strong>no central coordinator</strong>, thanks to globally synchronized clocks.</p>
<hr />
<h3 id="14-challenges-in-distributed-wals"><a class="header" href="#14-challenges-in-distributed-wals"><strong>14. Challenges in Distributed WALs</strong></a></h3>
<p>Distributed WALs are complex due to:</p>
<div class="table-wrapper"><table><thead><tr><th>Challenge</th><th>Description</th></tr></thead><tbody>
<tr><td><strong>Network Partition</strong></td><td>Some nodes unreachable - consensus required</td></tr>
<tr><td><strong>Clock Skew</strong></td><td>Causes misordered commits without proper synchronization</td></tr>
<tr><td><strong>Log Divergence</strong></td><td>Crashed nodes may have partial logs</td></tr>
<tr><td><strong>Rebalancing</strong></td><td>Moving shards = moving WAL ownership</td></tr>
<tr><td><strong>Latency vs Consistency</strong></td><td>Global ordering can slow writes</td></tr>
</tbody></table>
</div>
<p>Most systems balance these via tunable consistency levels.</p>
<hr />
<h3 id="15-visual-summary-layers-of-a-distributed-database"><a class="header" href="#15-visual-summary-layers-of-a-distributed-database"><strong>15. Visual Summary: Layers of a Distributed Database</strong></a></h3>
<pre><code>          ┌───────────────────────────┐
          │ Application / SQL Layer   │
          └────────────┬──────────────┘
                       │
          ┌────────────▼──────────────┐
          │ Transaction Coordinator   │
          └────────────┬──────────────┘
                       │
          ┌────────────▼──────────────┐
          │ Consensus / Raft Groups   │  → distributed WALs
          └────────────┬──────────────┘
                       │
          ┌────────────▼──────────────┐
          │ Local Storage Engines     │  → local WAL
          └───────────────────────────┘
</code></pre>
<p>Each layer extends the previous one - from local WAL to global durability.</p>
<hr />
<h3 id="16-the-future-global-logs-beyond-databases"><a class="header" href="#16-the-future-global-logs-beyond-databases"><strong>16. The Future: Global Logs Beyond Databases</strong></a></h3>
<p>The concept of WAL-based consensus isn’t limited to databases.
Modern systems like <strong>Kafka</strong>, <strong>Pulsar</strong>, and <strong>Redpanda</strong> use distributed logs as their <em>primary abstraction</em>.</p>
<ul>
<li>Kafka topics = distributed WALs for event streaming</li>
<li>Etcd = distributed WAL for configuration</li>
<li>Temporal.io = distributed WAL for workflow state</li>
</ul>
<p>WAL has evolved from a recovery mechanism into a <strong>core synchronization primitive</strong> for the distributed world.</p>
<hr />
<h3 id="17-recap-the-journey-of-wal"><a class="header" href="#17-recap-the-journey-of-wal"><strong>17. Recap: The Journey of WAL</strong></a></h3>
<div class="table-wrapper"><table><thead><tr><th>Stage</th><th>Use of WAL</th></tr></thead><tbody>
<tr><td><strong>Single Node</strong></td><td>Crash recovery</td></tr>
<tr><td><strong>Replicated Cluster</strong></td><td>Log-based replication</td></tr>
<tr><td><strong>Consensus System</strong></td><td>Agreement on log order</td></tr>
<tr><td><strong>Distributed Database</strong></td><td>Coordinated global commit</td></tr>
<tr><td><strong>Global Database</strong></td><td>Time-synchronized WAL ordering</td></tr>
</tbody></table>
</div>
<p>From a humble recovery tool, WAL has become the <strong>heartbeat of distributed consistency</strong>.</p>
<hr />
<h3 id="18-closing-thoughts"><a class="header" href="#18-closing-thoughts"><strong>18. Closing Thoughts</strong></a></h3>
<p>In distributed databases, WAL isn’t just a <em>local crash log</em> - it’s the <strong>unifying thread</strong> connecting:</p>
<ul>
<li>Replication</li>
<li>Consensus</li>
<li>Transactions</li>
<li>Global ordering</li>
</ul>
<p>Each layer builds upon it to deliver <strong>durability, atomicity, and consistency</strong> across space and time.</p>
<p>When you look at a distributed system, beneath the complex machinery of shards, leaders, clocks, and consensus, there is always one quiet hero -
the <strong>Write-Ahead Log</strong>,
faithfully recording the world, one entry at a time.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="part-v-advanced-topics"><a class="header" href="#part-v-advanced-topics">Part V: Advanced Topics</a></h1>
<p>Up to this point, we’ve explored how Write-Ahead Logging (WAL) forms the backbone of reliable data systems - from basic persistence and crash recovery to distributed replication and consensus.
But databases in the real world don’t just need to <em>work</em> - they need to be <strong>fast</strong>, <strong>observable</strong>, and <strong>evolution-ready</strong>.</p>
<p>This part, <strong>Advanced Topics</strong>, moves beyond correctness into the realms of <strong>performance, introspection, and innovation</strong>. It focuses on how modern storage engines optimize, debug, and visualize the internals of WAL-driven systems.</p>
<hr />
<h3 id="chapter-16-performance-optimizations"><a class="header" href="#chapter-16-performance-optimizations"><strong>Chapter 16: Performance Optimizations</strong></a></h3>
<p>We start by examining how real-world systems squeeze every bit of throughput and latency improvement from their WAL pipeline. You’ll learn techniques like group commit, parallel log writers, batched fsyncs, compression, log segmentation, and memory-mapped I/O.
We’ll compare the optimizations used in PostgreSQL, RocksDB, and modern cloud databases - showing how small implementation details can lead to massive performance differences at scale.</p>
<hr />
<h3 id="chapter-17-debugging--visualization"><a class="header" href="#chapter-17-debugging--visualization"><strong>Chapter 17: Debugging &amp; Visualization</strong></a></h3>
<p>Once a WAL system is running in production, visibility becomes critical. In this chapter, we focus on <strong>how to debug, trace, and visualize</strong> WAL internals - from LSN tracking and checkpoint metrics to visualizing replication lag and transaction graphs.
You’ll learn how engineers trace log flows, use metrics to diagnose I/O stalls, and design visualization tools that make log internals intuitive.
This section bridges low-level storage debugging with human-friendly observability.</p>
<hr />
<h3 id="chapter-18-modern-storage-engines"><a class="header" href="#chapter-18-modern-storage-engines"><strong>Chapter 18: Modern Storage Engines</strong></a></h3>
<p>Finally, we step into the frontier of <strong>modern storage engine design</strong> - where WAL meets emerging technologies. We’ll explore hybrid engines that blend in-memory data with persistent logs, log-structured merge (LSM) optimizations, columnar WALs, and cloud-native designs that separate compute and storage.
From embedded databases like LMDB to distributed storage systems, this chapter showcases the <strong>evolution of WAL thinking</strong> in modern architectures.</p>
<hr />
<p>By the end of this part, you’ll not only understand how to make WAL-based systems reliable - but how to make them <em>shine</em> under pressure.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-16-performance-optimizations-1"><a class="header" href="#chapter-16-performance-optimizations-1">Chapter 16: Performance Optimizations</a></h1>
<p>When we first learned about Write-Ahead Logging (WAL), the idea sounded simple:</p>
<blockquote>
<p>“Write changes to a log first, then apply them to the database.”</p>
</blockquote>
<p>That’s true - but simplicity hides a trap.
In large systems, <strong>logging every operation</strong> can become <strong>a performance bottleneck</strong>. Disks are slow, fsyncs are expensive, and concurrency makes coordination tricky.</p>
<p>So, how do modern databases like <strong>PostgreSQL</strong>, <strong>RocksDB</strong>, or <strong>MySQL InnoDB</strong> write millions of transactions per second <em>while still ensuring durability and consistency</em>?</p>
<p>That’s where <strong>WAL performance optimizations</strong> come in.</p>
<p>This chapter explains how databases squeeze the best performance out of the WAL - by reducing disk I/O, batching writes, using smart buffering, and parallelizing work - without compromising data safety.</p>
<hr />
<h2 id="1-the-bottleneck-why-wal-can-be-slow"><a class="header" href="#1-the-bottleneck-why-wal-can-be-slow"><strong>1. The Bottleneck: Why WAL Can Be Slow</strong></a></h2>
<p>Every time a transaction commits, it must ensure that its WAL entry is <strong>safely written to disk</strong> before reporting success.
This means calling <code>fsync()</code> or its equivalent - a system call that flushes all buffered data to physical storage.</p>
<p>Here’s the problem:</p>
<ul>
<li><code>fsync()</code> can take <strong>milliseconds</strong>.</li>
<li>Modern CPUs can process <strong>millions</strong> of operations per second.</li>
<li>If every commit waits for disk I/O, the system slows to a crawl.</li>
</ul>
<p>This leads to the central tension in database design:</p>
<blockquote>
<p>How do we maintain durability without waiting for the disk every time?</p>
</blockquote>
<p>Let’s explore the key techniques databases use to solve this.</p>
<hr />
<h2 id="2-group-commit"><a class="header" href="#2-group-commit"><strong>2. Group Commit</strong></a></h2>
<p>Imagine a busy restaurant:</p>
<ul>
<li>Each customer (transaction) places an order.</li>
<li>Instead of sending one waiter per customer to the kitchen (disk), the restaurant waits a moment and <strong>batches several orders together</strong>.</li>
</ul>
<p>That’s exactly what <strong>group commit</strong> does.</p>
<h3 id="how-it-works"><a class="header" href="#how-it-works">How it works:</a></h3>
<ol>
<li>Multiple transactions write their WAL records into a shared buffer in memory.</li>
<li>A background process periodically flushes this buffer to disk.</li>
<li>All transactions in that batch are considered “committed” once the flush completes.</li>
</ol>
<h3 id="benefits"><a class="header" href="#benefits">Benefits:</a></h3>
<ul>
<li>Reduces the number of <code>fsync()</code> calls drastically.</li>
<li>Improves throughput by 10x or more in write-heavy workloads.</li>
</ul>
<h3 id="example-simplified-pseudo-code"><a class="header" href="#example-simplified-pseudo-code">Example (simplified pseudo-code):</a></h3>
<pre><code class="language-python">pending_transactions = []
while True:
    txn = wait_for_new_transaction()
    append_to_wal_buffer(txn.log)
    pending_transactions.append(txn)
    
    if len(pending_transactions) &gt;= BATCH_SIZE or timeout():
        flush_to_disk(wal_buffer)
        mark_all_committed(pending_transactions)
        pending_transactions.clear()
</code></pre>
<p><strong>PostgreSQL</strong>, <strong>MySQL</strong>, and <strong>Kafka</strong> all rely heavily on this mechanism.</p>
<hr />
<h2 id="3-wal-buffering-and-memory-management"><a class="header" href="#3-wal-buffering-and-memory-management"><strong>3. WAL Buffering and Memory Management</strong></a></h2>
<p>Every WAL entry first lands in memory - the <strong>WAL buffer</strong> - before being written to disk.</p>
<h3 id="why-buffer"><a class="header" href="#why-buffer">Why buffer?</a></h3>
<ul>
<li>It allows combining many small writes into one large write.</li>
<li>It enables <strong>sequential I/O</strong>, which disks handle efficiently.</li>
<li>It gives room for optimizations like compression or reordering.</li>
</ul>
<p>Databases tune the size of this buffer:</p>
<ul>
<li>Too small → frequent flushes.</li>
<li>Too large → risk of data loss on crash (before fsync).</li>
</ul>
<p>PostgreSQL, for instance, has a parameter <code>wal_buffers</code> to configure this size.</p>
<hr />
<h2 id="4-batched-and-asynchronous-fsync"><a class="header" href="#4-batched-and-asynchronous-fsync"><strong>4. Batched and Asynchronous fsync</strong></a></h2>
<p>Calling <code>fsync()</code> after every write blocks the main thread.
Modern systems avoid this using <strong>batched</strong> or <strong>async fsync</strong>.</p>
<h3 id="example-asynchronous-fsync"><a class="header" href="#example-asynchronous-fsync">Example: Asynchronous fsync</a></h3>
<p>Instead of waiting for disk confirmation, the system marks a batch as “pending” and continues accepting new transactions.
A background thread performs the actual flush asynchronously.</p>
<p>When the disk write completes, it marks all those transactions as durable.</p>
<p>This allows <strong>concurrent log generation</strong> while the disk works in parallel.</p>
<hr />
<h2 id="5-compression-and-log-segmentation"><a class="header" href="#5-compression-and-log-segmentation"><strong>5. Compression and Log Segmentation</strong></a></h2>
<p>Logs grow rapidly - especially when every change is recorded.
To manage disk space and speed up I/O, databases use:</p>
<h3 id="a-compression"><a class="header" href="#a-compression">a. <strong>Compression</strong></a></h3>
<p>Compressing WAL segments before writing reduces:</p>
<ul>
<li>Disk writes (fewer bytes)</li>
<li>I/O time</li>
<li>Storage footprint</li>
</ul>
<p>However, compression adds <strong>CPU overhead</strong>, so it’s used selectively (e.g., for large batch logs or archival).</p>
<h3 id="b-segmentation"><a class="header" href="#b-segmentation">b. <strong>Segmentation</strong></a></h3>
<p>Instead of keeping a single growing log file, WAL is split into <strong>segments</strong> (e.g., 16 MB in PostgreSQL).</p>
<ul>
<li>Segments can be rotated and reused.</li>
<li>Easier to archive or replicate.</li>
<li>Avoids fragmentation and large-file penalties.</li>
</ul>
<hr />
<h2 id="6-parallel-log-writers"><a class="header" href="#6-parallel-log-writers"><strong>6. Parallel Log Writers</strong></a></h2>
<p>As CPUs became multi-core, databases evolved to exploit concurrency in WAL writing.</p>
<h3 id="idea"><a class="header" href="#idea">Idea:</a></h3>
<ul>
<li>Multiple threads can generate WAL records concurrently.</li>
<li>One or more dedicated I/O threads handle actual disk writes.</li>
</ul>
<p>This allows separation between:</p>
<ul>
<li><strong>Log generation (CPU-bound)</strong></li>
<li><strong>Log flushing (I/O-bound)</strong></li>
</ul>
<p>For example:</p>
<ul>
<li><strong>RocksDB</strong> uses multiple threads to prepare WAL batches.</li>
<li><strong>Aurora</strong> (AWS) parallelizes commit log writes across multiple nodes for ultra-low latency.</li>
</ul>
<hr />
<h2 id="7-direct-io-and-memory-mapping"><a class="header" href="#7-direct-io-and-memory-mapping"><strong>7. Direct I/O and Memory Mapping</strong></a></h2>
<p>Normally, the OS buffers all writes in its <strong>page cache</strong>.
But databases often want tighter control.</p>
<h3 id="options"><a class="header" href="#options">Options:</a></h3>
<ul>
<li><strong>Direct I/O:</strong> Write directly to disk, bypassing OS cache.
→ Reduces double-buffering and unpredictability.</li>
<li><strong>Memory-mapped I/O (mmap):</strong> Treats files as memory, enabling fast reads/writes with less system call overhead.</li>
</ul>
<p>LMDB and WiredTiger (MongoDB’s engine) are strong examples of efficient mmap-based WAL handling.</p>
<hr />
<h2 id="8-adaptive-checkpointing"><a class="header" href="#8-adaptive-checkpointing"><strong>8. Adaptive Checkpointing</strong></a></h2>
<p>Remember checkpoints? They flush all modified pages from memory to disk so that the database can recover quickly using the WAL.</p>
<p>However, frequent checkpoints cause write spikes and I/O contention.</p>
<h3 id="solution-adaptive-or-incremental-checkpointing"><a class="header" href="#solution-adaptive-or-incremental-checkpointing">Solution: Adaptive or Incremental Checkpointing</a></h3>
<ul>
<li>Spread out checkpoint writes gradually.</li>
<li>Trigger checkpoints based on activity or WAL size thresholds.</li>
</ul>
<p>This ensures <strong>steady I/O load</strong> and avoids sudden performance drops.</p>
<hr />
<h2 id="9-hardware-aware-optimizations"><a class="header" href="#9-hardware-aware-optimizations"><strong>9. Hardware-Aware Optimizations</strong></a></h2>
<p>Databases also adapt their WAL designs to match hardware characteristics:</p>
<ul>
<li><strong>NVMe SSDs:</strong> Allow parallel I/O and deep queues → WAL can issue multiple flushes concurrently.</li>
<li><strong>NVRAM / Persistent Memory:</strong> Can make WAL writes almost as fast as RAM.</li>
<li><strong>Battery-backed DRAM:</strong> Lets databases delay fsync safely, knowing data won’t be lost in power failure.</li>
</ul>
<p>In cloud databases, WAL durability may be implemented by <strong>replicating log entries across nodes</strong> instead of writing to local disk - trading I/O latency for network redundancy.</p>
<hr />
<h2 id="10-real-world-example-postgresql-group-commit-timeline"><a class="header" href="#10-real-world-example-postgresql-group-commit-timeline"><strong>10. Real-World Example: PostgreSQL Group Commit Timeline</strong></a></h2>
<p>Let’s visualize a typical commit scenario with group commit:</p>
<pre><code>Time →
|----------------------------------------------------------&gt;

Txn A arrives ---+
Txn B arrives -----+--------+  (batched together)
Txn C arrives --------+     |
                      |     |
   [ WAL buffer fill ]|     |
                      v     v
             +-----------------------+
             |   fsync() once        |
             +-----------------------+
                      |
              Txn A, B, C committed
</code></pre>
<p>Even though 3 transactions arrived at different times, they shared the same <code>fsync()</code>.
Result: <strong>1 disk flush instead of 3</strong> - 3x performance improvement.</p>
<hr />
<h2 id="11-putting-it-all-together"><a class="header" href="#11-putting-it-all-together"><strong>11. Putting It All Together</strong></a></h2>
<div class="table-wrapper"><table><thead><tr><th>Optimization</th><th>Goal</th><th>Example</th></tr></thead><tbody>
<tr><td>Group Commit</td><td>Batch fsyncs</td><td>PostgreSQL, MySQL</td></tr>
<tr><td>WAL Buffering</td><td>Merge small writes</td><td>All major DBs</td></tr>
<tr><td>Async fsync</td><td>Overlap I/O &amp; CPU</td><td>RocksDB</td></tr>
<tr><td>Compression</td><td>Reduce I/O volume</td><td>CockroachDB</td></tr>
<tr><td>Log Segmentation</td><td>Manage rotation</td><td>PostgreSQL</td></tr>
<tr><td>Parallel Writers</td><td>Exploit multi-core</td><td>RocksDB</td></tr>
<tr><td>mmap / Direct I/O</td><td>Control OS caching</td><td>LMDB, WiredTiger</td></tr>
<tr><td>Adaptive Checkpoint</td><td>Smooth I/O</td><td>InnoDB</td></tr>
</tbody></table>
</div>
<hr />
<h2 id="12-closing-thoughts"><a class="header" href="#12-closing-thoughts"><strong>12. Closing Thoughts</strong></a></h2>
<p>Performance optimization in WAL systems is a careful balancing act between <strong>safety</strong> and <strong>speed</strong>.</p>
<p>Every optimization we discussed - batching, buffering, or parallelism - aims to reduce latency <strong>without ever losing data</strong>.</p>
<p>WAL isn’t just a log - it’s the <strong>heartbeat of the database</strong>.
And like any heartbeat, it must be both <strong>steady</strong> and <strong>fast</strong>.</p>
<p>Understanding these techniques will help you design, tune, and debug real-world systems that don’t just recover from crashes - they <strong>thrive under load</strong>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-17-debugging--visualization-1"><a class="header" href="#chapter-17-debugging--visualization-1">Chapter 17: Debugging &amp; Visualization</a></h1>
<blockquote>
<p>“It’s not what your system <em>does</em>, it’s what you can <em>see it doing</em> that makes it reliable.”</p>
</blockquote>
<p>Up to now, we’ve learned how to make Write-Ahead Logging (WAL) <strong>fast</strong> and <strong>durable</strong>.
But what happens when something goes wrong - a commit stalls, replication lags, or recovery takes forever?</p>
<p>At that moment, logs and metrics become your only window into the database’s inner life.</p>
<p>This chapter focuses on <strong>how to debug, trace, and visualize</strong> WAL internals.
You’ll learn how to understand the flow of data inside the log, interpret checkpoints and sequence numbers, and even design visualization tools that make the invisible parts of the database <em>visible and intuitive</em>.</p>
<hr />
<h2 id="1-why-debugging-wal-matters"><a class="header" href="#1-why-debugging-wal-matters"><strong>1. Why Debugging WAL Matters</strong></a></h2>
<p>Debugging WAL is different from debugging application code.
You’re not just looking at stack traces - you’re examining <strong>how data moves through time</strong>:</p>
<ul>
<li>From memory to disk,</li>
<li>From one node to another,</li>
<li>From one checkpoint to the next.</li>
</ul>
<p>Failures in this pipeline cause:</p>
<ul>
<li><strong>Data loss or corruption</strong> (if WAL isn’t replayed correctly)</li>
<li><strong>Replication lag</strong> (if followers can’t keep up)</li>
<li><strong>High latency</strong> (if fsyncs block)</li>
<li><strong>Long recovery time</strong> (if checkpoints are sparse)</li>
</ul>
<p>Understanding WAL internals helps you <em>predict</em>, <em>diagnose</em>, and <em>prevent</em> such issues.</p>
<hr />
<h2 id="2-the-key-metrics-what-to-observe"><a class="header" href="#2-the-key-metrics-what-to-observe"><strong>2. The Key Metrics: What to Observe</strong></a></h2>
<p>Let’s start by listing the key things to monitor and visualize in a WAL system:</p>
<div class="table-wrapper"><table><thead><tr><th>Metric</th><th>Description</th><th>Why It Matters</th></tr></thead><tbody>
<tr><td><strong>LSN (Log Sequence Number)</strong></td><td>A monotonically increasing byte offset in the WAL</td><td>Tracks progress of writes</td></tr>
<tr><td><strong>WAL write rate</strong></td><td>Bytes written per second</td><td>Indicates I/O throughput</td></tr>
<tr><td><strong>Flush latency</strong></td><td>Time taken for fsync</td><td>Detects slow disks</td></tr>
<tr><td><strong>Replication lag</strong></td><td>Difference between primary and replica LSN</td><td>Shows sync delay</td></tr>
<tr><td><strong>Checkpoint distance</strong></td><td>WAL bytes since last checkpoint</td><td>Determines recovery workload</td></tr>
<tr><td><strong>Pending transactions</strong></td><td>Unflushed or uncommitted transactions</td><td>Useful for debugging stalls</td></tr>
</tbody></table>
</div>
<p>Most production databases expose these metrics via logs, system tables, or monitoring APIs.</p>
<hr />
<h2 id="3-reading-the-wal-the-human-way"><a class="header" href="#3-reading-the-wal-the-human-way"><strong>3. Reading the WAL: The Human Way</strong></a></h2>
<p>Let’s make this practical.
Imagine you’re debugging PostgreSQL.</p>
<p>You can actually read the WAL files using:</p>
<pre><code class="language-bash">pg_waldump /var/lib/postgresql/data/pg_wal/000000010000000A000000C5
</code></pre>
<p>This command prints each record with its <strong>LSN</strong>, <strong>transaction ID</strong>, <strong>type of operation</strong>, and <strong>affected relation</strong>.</p>
<p>Example output:</p>
<pre><code>rmgr: Heap      len (rec/tot): 56/64, tx: 127, lsn: 1/A/C5A40010, desc: INSERT off 5
</code></pre>
<p>This single line tells you:</p>
<ul>
<li>The WAL record is from the <strong>heap</strong> (table storage layer)</li>
<li>It represents an <strong>INSERT</strong></li>
<li>It belongs to transaction <strong>127</strong></li>
<li>Its LSN is <strong>1/A/C5A40010</strong></li>
</ul>
<p>By reading WAL entries, you can reconstruct what happened just before a crash - like an airplane black box.</p>
<hr />
<h2 id="4-visualizing-wal-internals"><a class="header" href="#4-visualizing-wal-internals"><strong>4. Visualizing WAL Internals</strong></a></h2>
<h3 id="a-the-flow-of-a-transaction"><a class="header" href="#a-the-flow-of-a-transaction"><strong>a. The Flow of a Transaction</strong></a></h3>
<p>Let’s visualize how a single transaction moves through WAL stages:</p>
<pre><code>[Transaction Start]
       |
       v
[Generate WAL Record] -- (in memory buffer)
       |
       v
[Write to WAL file] -- (sequential write)
       |
       v
[fsync to disk] -- (durability point)
       |
       v
[Apply changes to data pages]
</code></pre>
<p>Each stage emits measurable signals:</p>
<ul>
<li>Time spent in each stage</li>
<li>Queue depth (number of waiting commits)</li>
<li>Size of buffered log</li>
</ul>
<p>A good visualization tool (like a Grafana dashboard or custom timeline UI) can show these flows in real time.</p>
<hr />
<h3 id="b-example-visualization-wal-throughput"><a class="header" href="#b-example-visualization-wal-throughput"><strong>b. Example Visualization: WAL Throughput</strong></a></h3>
<p>A simple time-series graph can reveal much:</p>
<pre><code>WAL Write Rate (MB/s)
│
│         ┌─────────────┐
│         │             │
│     ┌───┘             └───┐
│ ────┘                       └───────▶ Time
        (checkpoint starts) 
</code></pre>
<p>You can immediately spot spikes - often caused by checkpoints or bursts of commits.</p>
<hr />
<h3 id="c-example-visualization-replication-lag"><a class="header" href="#c-example-visualization-replication-lag"><strong>c. Example Visualization: Replication Lag</strong></a></h3>
<p>For replicated databases, plot <strong>Primary LSN – Replica LSN</strong> over time.</p>
<pre><code>Replication Lag (bytes)
│
│         ┌─────────┐
│         │         │
│     ┌───┘         └───┐
│ ────┘                   └────▶ Time
       (network slowdown)
</code></pre>
<p>A steady lag increase signals that replicas aren’t catching up - possibly due to I/O or network issues.</p>
<hr />
<h2 id="5-common-debugging-scenarios"><a class="header" href="#5-common-debugging-scenarios"><strong>5. Common Debugging Scenarios</strong></a></h2>
<h3 id="scenario-1-slow-commits"><a class="header" href="#scenario-1-slow-commits"><strong>Scenario 1: Slow Commits</strong></a></h3>
<p>Symptoms:</p>
<ul>
<li><code>COMMIT</code> takes longer than expected.</li>
</ul>
<p>Debug Steps:</p>
<ol>
<li>Check <code>wal_write_time</code> vs <code>wal_sync_time</code> metrics.</li>
<li>If <code>sync_time</code> is high → disk I/O issue.</li>
<li>Enable <code>track_io_timing</code> (PostgreSQL) or use I/O tracing tools.</li>
<li>Consider enabling group commit or async fsync.</li>
</ol>
<hr />
<h3 id="scenario-2-wal-growing-too-fast"><a class="header" href="#scenario-2-wal-growing-too-fast"><strong>Scenario 2: WAL Growing Too Fast</strong></a></h3>
<p>Symptoms:</p>
<ul>
<li>Disk usage increases rapidly.</li>
<li>Checkpoints are infrequent.</li>
</ul>
<p>Possible Causes:</p>
<ul>
<li>Long-running transactions prevent cleanup.</li>
<li>Checkpoint interval too large.</li>
<li>Replication lag keeps old segments from being recycled.</li>
</ul>
<p>Debugging Tools:</p>
<ul>
<li>Monitor <code>checkpoint_distance</code>.</li>
<li>Use commands like <code>pg_stat_bgwriter</code> or <code>SHOW wal_keep_size;</code>.</li>
</ul>
<hr />
<h3 id="scenario-3-replication-lag"><a class="header" href="#scenario-3-replication-lag"><strong>Scenario 3: Replication Lag</strong></a></h3>
<p>Symptoms:</p>
<ul>
<li>Replica constantly behind primary.</li>
</ul>
<p>Debugging Steps:</p>
<ol>
<li>Compare primary and replica <code>current_lsn</code>.</li>
<li>Monitor network latency and disk speed on replicas.</li>
<li>Enable WAL compression to reduce transfer volume.</li>
<li>Visualize lag over time - patterns often reveal root causes.</li>
</ol>
<hr />
<h2 id="6-building-wal-visualizers"><a class="header" href="#6-building-wal-visualizers"><strong>6. Building WAL Visualizers</strong></a></h2>
<p>You don’t need complex enterprise tools to visualize WAL.</p>
<p>Here’s a simple approach you can build:</p>
<ol>
<li>
<p><strong>Collect Metrics</strong>
Use APIs or log hooks to capture:</p>
<ul>
<li>LSNs</li>
<li>Write latency</li>
<li>WAL size growth</li>
</ul>
</li>
<li>
<p><strong>Store in Time-Series DB</strong>
e.g., Prometheus or InfluxDB.</p>
</li>
<li>
<p><strong>Visualize</strong>
Use Grafana, Plotly, or a custom dashboard.</p>
</li>
</ol>
<h3 id="example-rust--prometheus"><a class="header" href="#example-rust--prometheus">Example (Rust + Prometheus):</a></h3>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use prometheus::{IntGauge, Encoder, TextEncoder};

let wal_write_bytes = IntGauge::new("wal_write_bytes", "Total WAL bytes written").unwrap();

// In WAL write loop
wal_write_bytes.add(batch_size as i64);
<span class="boring">}</span></code></pre></pre>
<p>A small exporter like this can feed live metrics into your visualization tool.</p>
<hr />
<h2 id="7-timeline-visualization-for-recovery--replay"><a class="header" href="#7-timeline-visualization-for-recovery--replay"><strong>7. Timeline Visualization (For Recovery &amp; Replay)</strong></a></h2>
<p>When recovering from crashes or replays, it’s helpful to see a <strong>timeline view</strong> of the WAL:</p>
<pre><code>|----------------------|----------------------|----------------------|
Checkpoint 1           Checkpoint 2           Checkpoint 3
      ↑
   crash here
</code></pre>
<p>The recovery process replays from the last checkpoint to the crash point.
A timeline visualization helps operators know how far replay has progressed - similar to a progress bar during recovery.</p>
<p>Some databases even visualize this internally. For example:</p>
<ul>
<li><strong>PostgreSQL</strong> exposes <code>pg_stat_recovery_prefetch</code> metrics.</li>
<li><strong>CockroachDB</strong>’s web UI shows per-node WAL replay progress.</li>
</ul>
<hr />
<h2 id="8-real-world-tools"><a class="header" href="#8-real-world-tools"><strong>8. Real-World Tools</strong></a></h2>
<div class="table-wrapper"><table><thead><tr><th>Tool</th><th>Database</th><th>Purpose</th></tr></thead><tbody>
<tr><td><code>pg_waldump</code></td><td>PostgreSQL</td><td>View raw WAL records</td></tr>
<tr><td><code>pg_stat_wal</code></td><td>PostgreSQL</td><td>Monitor WAL write rates</td></tr>
<tr><td><code>SHOW ENGINE INNODB STATUS</code></td><td>MySQL</td><td>Inspect log buffer and I/O</td></tr>
<tr><td><code>db_bench</code> + perf tools</td><td>RocksDB</td><td>Trace WAL performance</td></tr>
<tr><td><code>Grafana + Prometheus</code></td><td>Generic</td><td>Visualization</td></tr>
<tr><td><code>strace / iostat / dstat</code></td><td>System-level</td><td>Observe I/O and fsync latency</td></tr>
</tbody></table>
</div>
<p>These tools help bridge the gap between <em>storage layer behavior</em> and <em>system observability</em>.</p>
<hr />
<h2 id="9-wal-debugging-checklist"><a class="header" href="#9-wal-debugging-checklist"><strong>9. WAL Debugging Checklist</strong></a></h2>
<p>When diagnosing WAL issues, use this simple checklist:</p>
<p><strong>Is WAL being flushed regularly?</strong>
Check for stalled or large buffers.</p>
<p><strong>Are checkpoints configured correctly?</strong>
Too frequent = overhead, too rare = long recovery.</p>
<p><strong>Is fsync latency stable?</strong>
Spikes indicate hardware or OS buffering problems.</p>
<p><strong>Is replication lag stable?</strong>
If not, network or disk I/O might be limiting performance.</p>
<p><strong>Do WAL files recycle properly?</strong>
Old segments piling up can signal retention or recovery issues.</p>
<hr />
<h2 id="10-closing-thoughts"><a class="header" href="#10-closing-thoughts"><strong>10. Closing Thoughts</strong></a></h2>
<p>Debugging WAL is like listening to a heartbeat monitor - it tells you how healthy your database really is.</p>
<p>Visualization transforms low-level log mechanics into clear insights:</p>
<ul>
<li>You see <em>where</em> time is spent.</li>
<li>You detect <em>when</em> something is off.</li>
<li>You understand <em>why</em> performance changes.</li>
</ul>
<p>Ultimately, observability completes the story of WAL:</p>
<blockquote>
<p>It starts with <strong>durability</strong>, evolves with <strong>performance</strong>, and matures with <strong>visibility</strong>.</p>
</blockquote>
<p>When you can visualize your WAL - from the first byte written to the last byte replayed -
you no longer just operate a database.
You <em>understand</em> it.</p>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="chapter-18-modern-storage-engines-1"><a class="header" href="#chapter-18-modern-storage-engines-1">Chapter 18: Modern Storage Engines</a></h1>
<blockquote>
<p>“Every generation of databases reinvents the log - but smarter.”</p>
</blockquote>
<p>In the early days of databases, storage engines were simple.
You had a few data files, a write-ahead log, and a recovery routine.
Today, storage engines are the <em>nervous system</em> of distributed data infrastructure - balancing durability, speed, and scalability across disks, memory, and the cloud.</p>
<p>This chapter explores <strong>how modern storage engines use and extend WAL concepts</strong>, how new designs like <strong>LSM trees</strong>, <strong>columnar formats</strong>, and <strong>cloud-native architectures</strong> evolved from the same core principle:</p>
<blockquote>
<p><em>Log first, organize later.</em></p>
</blockquote>
<hr />
<h2 id="1-from-pages-to-streams"><a class="header" href="#1-from-pages-to-streams"><strong>1. From Pages to Streams</strong></a></h2>
<p>Traditional databases like PostgreSQL and MySQL use <strong>page-based storage</strong>.
Each data page (e.g., 8 KB) is updated directly on disk, and the WAL records every modification.</p>
<p>This works well for transactional workloads, but struggles when:</p>
<ul>
<li>Writes are random and frequent.</li>
<li>Datasets exceed memory size.</li>
<li>You need high ingest rates (e.g., analytics or time-series).</li>
</ul>
<p>So modern systems flipped the model:</p>
<blockquote>
<p>Instead of updating in place, <strong>append changes</strong> and <strong>merge later</strong>.</p>
</blockquote>
<p>This philosophy led to <strong>Log-Structured Storage Engines</strong> - the foundation for RocksDB, LevelDB, Cassandra, and even some cloud systems.</p>
<hr />
<h2 id="2-log-structured-merge-trees-lsm-trees"><a class="header" href="#2-log-structured-merge-trees-lsm-trees"><strong>2. Log-Structured Merge Trees (LSM Trees)</strong></a></h2>
<h3 id="the-core-idea"><a class="header" href="#the-core-idea">The Core Idea</a></h3>
<p>Instead of updating disk pages directly, write everything sequentially to a <strong>log</strong> and a <strong>memtable</strong> (in-memory index).
When the memtable fills, flush it to disk as an <strong>SSTable</strong> (Sorted String Table).</p>
<p>Over time, multiple SSTables accumulate and are <strong>compacted</strong> into fewer, larger ones.</p>
<p>This keeps writes <strong>sequential</strong> (fast) and uses background threads to manage organization (compaction).</p>
<h3 id="wals-role-in-lsm-engines"><a class="header" href="#wals-role-in-lsm-engines">WAL’s Role in LSM Engines</a></h3>
<p>Even LSM engines still need WAL:</p>
<ul>
<li>Every write is appended to a WAL file for durability.</li>
<li>If the process crashes, the WAL is replayed into the memtable.</li>
</ul>
<p>So the flow looks like:</p>
<pre><code>[Write Request]
      ↓
[Append to WAL]  ---&gt; [Durability]
      ↓
[Insert into MemTable]  ---&gt; [In-memory performance]
      ↓
[Flush to SSTable] ---&gt; [Persistence + Compaction]
</code></pre>
<p>The LSM approach <strong>maximizes write throughput</strong> and <strong>minimizes random I/O</strong> - making it ideal for workloads like logs, analytics, or key-value stores.</p>
<h3 id="real-world-examples"><a class="header" href="#real-world-examples">Real-World Examples</a></h3>
<ul>
<li><strong>LevelDB</strong> – Simple key-value store from Google using LSM + WAL.</li>
<li><strong>RocksDB</strong> – Facebook’s evolution of LevelDB, optimized for SSDs, with parallel WAL writes.</li>
<li><strong>Cassandra</strong> – Distributed LSM engine where each node maintains a commit log (WAL) and memtable.</li>
</ul>
<hr />
<h2 id="3-columnar-storage-and-wal"><a class="header" href="#3-columnar-storage-and-wal"><strong>3. Columnar Storage and WAL</strong></a></h2>
<p>Columnar databases (like <strong>ClickHouse</strong>, <strong>Parquet</strong>, or <strong>DuckDB</strong>) store data by <strong>columns</strong> instead of rows.</p>
<p>That design is fantastic for analytics - but what about durability?</p>
<p>Columnar systems also rely on WAL, but in a slightly different way:</p>
<ul>
<li>Instead of logging every cell change, they <strong>batch record changes</strong> to entire column segments.</li>
<li>The WAL acts as a <em>staging area</em> for compressed columnar blocks.</li>
<li>Once a block is finalized and flushed to disk, the WAL entries can be discarded.</li>
</ul>
<h3 id="visualization"><a class="header" href="#visualization">Visualization</a></h3>
<pre><code>[Query Insert]
   ↓
[Write to Columnar WAL]
   ↓
[Compress Columns + Encode]
   ↓
[Write Final Column Files (.parquet/.orc)]
</code></pre>
<p>Columnar WALs emphasize <strong>batch durability</strong> - balancing the need for crash safety with high analytical throughput.</p>
<hr />
<h2 id="4-hybrid-engines-mixing-row-and-columnar-with-wal"><a class="header" href="#4-hybrid-engines-mixing-row-and-columnar-with-wal"><strong>4. Hybrid Engines: Mixing Row and Columnar with WAL</strong></a></h2>
<p>Modern workloads often need both:</p>
<ul>
<li>Fast transactional updates (row-oriented)</li>
<li>Fast analytics (column-oriented)</li>
</ul>
<p>Hybrid engines like <strong>MariaDB ColumnStore</strong>, <strong>SingleStore</strong>, and <strong>DuckDB</strong> bridge this gap.</p>
<p>They maintain:</p>
<ul>
<li>A <strong>row-based WAL</strong> for real-time inserts and updates.</li>
<li>Periodic <strong>columnar compaction</strong> that merges WAL data into analytical blocks.</li>
</ul>
<p>This approach combines <strong>low-latency writes</strong> with <strong>OLAP performance</strong> - both powered by WAL as the unifying persistence layer.</p>
<hr />
<h2 id="5-cloud-native-wal-and-shared-storage"><a class="header" href="#5-cloud-native-wal-and-shared-storage"><strong>5. Cloud-Native WAL and Shared Storage</strong></a></h2>
<p>In cloud systems, local disks are no longer the only persistence layer.
Storage engines are now built to work with <strong>object storage (S3, GCS, Azure Blob)</strong> and <strong>distributed WALs</strong>.</p>
<h3 id="example-auroras-log-first-design"><a class="header" href="#example-auroras-log-first-design">Example: Aurora’s Log-First Design</a></h3>
<p>Amazon Aurora flipped traditional architecture on its head:</p>
<ul>
<li>Instead of storing data pages directly on disk, each database node sends WAL records to a <strong>distributed log service</strong>.</li>
<li>The log service persists and replicates the WAL across storage nodes.</li>
<li>Data pages are <em>reconstructed</em> on demand from these logs.</li>
</ul>
<p>This effectively turns WAL into a <strong>control plane for data durability</strong>.</p>
<h3 id="benefits-1"><a class="header" href="#benefits-1">Benefits</a></h3>
<ul>
<li>Near-instant crash recovery (since logs are distributed).</li>
<li>Storage layer is stateless and shared.</li>
<li>Read replicas can rebuild data independently from WAL streams.</li>
</ul>
<p>The same philosophy powers <strong>CockroachDB</strong>, <strong>TiDB</strong>, and <strong>FoundationDB</strong> - all of which rely on some form of <strong>log shipping</strong> as the backbone of durability and replication.</p>
<hr />
<h2 id="6-wal-in-embedded-systems"><a class="header" href="#6-wal-in-embedded-systems"><strong>6. WAL in Embedded Systems</strong></a></h2>
<p>Even tiny databases embedded in devices use WAL - though optimized for constraints.</p>
<h3 id="example-lmdb"><a class="header" href="#example-lmdb">Example: LMDB</a></h3>
<ul>
<li>Uses <strong>memory-mapped I/O</strong> instead of writing log segments manually.</li>
<li>The “log” is implicit in the <strong>copy-on-write B+Tree</strong> structure.</li>
<li>Commits atomically swap root pointers, ensuring durability without explicit log replay.</li>
</ul>
<h3 id="example-sqlite-wal-mode"><a class="header" href="#example-sqlite-wal-mode">Example: SQLite (WAL Mode)</a></h3>
<ul>
<li>Uses a rolling WAL file to stage changes.</li>
<li>On checkpoint, merges WAL content into the main database.</li>
<li>Provides atomicity even on mobile or IoT devices.</li>
</ul>
<p>In embedded systems, the focus is on <strong>simplicity, crash safety, and minimal overhead</strong>, not high throughput.</p>
<hr />
<h2 id="7-advanced-wal-techniques-in-modern-engines"><a class="header" href="#7-advanced-wal-techniques-in-modern-engines"><strong>7. Advanced WAL Techniques in Modern Engines</strong></a></h2>
<div class="table-wrapper"><table><thead><tr><th>Technique</th><th>Description</th><th>Used In</th></tr></thead><tbody>
<tr><td><strong>Tiered WAL Storage</strong></td><td>Hot log segments on SSD, cold on HDD or S3</td><td>RocksDB, CockroachDB</td></tr>
<tr><td><strong>Vectorized WAL Records</strong></td><td>Log multiple column changes as vectorized batches</td><td>ClickHouse, DuckDB</td></tr>
<tr><td><strong>Delta WAL</strong></td><td>Only log changed values, not full pages</td><td>InnoDB, WiredTiger</td></tr>
<tr><td><strong>Logical WAL</strong></td><td>Store changes as logical events (SQL-like) for replication</td><td>PostgreSQL Logical Decoding</td></tr>
<tr><td><strong>Network-Replicated WAL</strong></td><td>Stream WAL directly over network for fault tolerance</td><td>Aurora, TiDB</td></tr>
</tbody></table>
</div>
<p>Each technique reflects a design trade-off between <strong>write latency</strong>, <strong>replay speed</strong>, and <strong>data granularity</strong>.</p>
<hr />
<h2 id="8-visualization-evolution-of-wal-architecture"><a class="header" href="#8-visualization-evolution-of-wal-architecture"><strong>8. Visualization: Evolution of WAL Architecture</strong></a></h2>
<pre><code>Traditional Engine (PostgreSQL, MySQL)
--------------------------------------
   [WAL] → [Data Pages] → [Disk]

Log-Structured Engine (RocksDB, Cassandra)
------------------------------------------
   [WAL] → [MemTable] → [SSTables] → [Compaction]

Cloud-Native Engine (Aurora, CockroachDB)
-----------------------------------------
   [WAL Stream] → [Distributed Log Service] → [Object Storage]
</code></pre>
<p>Each evolution pushes the boundary of performance, durability, and scalability -
but the <strong>WAL remains the beating heart</strong> of all of them.</p>
<hr />
<h2 id="9-debugging-and-observing-modern-wals"><a class="header" href="#9-debugging-and-observing-modern-wals"><strong>9. Debugging and Observing Modern WALs</strong></a></h2>
<p>As WALs become distributed and layered, debugging tools have evolved too:</p>
<ul>
<li><strong>Aurora Log Inspector</strong> shows per-node WAL latency.</li>
<li><strong>RocksDB’s db_bench + perf</strong> visualize compaction and WAL I/O patterns.</li>
<li><strong>ClickHouse system tables</strong> expose WAL flush timings and segment sizes.</li>
<li><strong>FoundationDB trace logs</strong> reveal commit log latencies across the cluster.</li>
</ul>
<p>The trend is clear - <strong>WAL visibility</strong> is now a first-class citizen in modern observability stacks.</p>
<hr />
<h2 id="10-the-future-of-wal-beyond-disks"><a class="header" href="#10-the-future-of-wal-beyond-disks"><strong>10. The Future of WAL: Beyond Disks</strong></a></h2>
<p>Emerging technologies are changing what “durability” means:</p>
<ul>
<li><strong>Persistent Memory (PMEM)</strong> allows direct, byte-addressable writes.</li>
<li><strong>Battery-backed NVRAM</strong> eliminates fsync latency.</li>
<li><strong>Transactional file systems</strong> integrate WAL-like journaling directly into the OS.</li>
</ul>
<p>In such systems, WAL may shift from being an <em>explicit file</em> to a <em>memory region</em> or <em>replicated stream</em> - but its purpose remains unchanged:</p>
<blockquote>
<p><em>A safe, ordered record of the truth.</em></p>
</blockquote>
<hr />
<h2 id="11-closing-thoughts"><a class="header" href="#11-closing-thoughts"><strong>11. Closing Thoughts</strong></a></h2>
<p>From the earliest relational engines to today’s distributed cloud systems,
the Write-Ahead Log has evolved - but never disappeared.</p>
<p>It has become:</p>
<ul>
<li>A <strong>buffer</strong> for performance,</li>
<li>A <strong>timeline</strong> for recovery,</li>
<li>A <strong>replication medium</strong> for scale,</li>
<li>And a <strong>control plane</strong> for cloud data integrity.</li>
</ul>
<p>Modern storage engines might differ wildly in form - page stores, LSMs, column stores, or shared logs -
but they all whisper the same story:</p>
<blockquote>
<p><em>“Durability begins with a log.”</em></p>
</blockquote>
<p>And understanding that story is what transforms a developer into a database engineer.</p>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h1 id="appendix-advanced-concepts--further-exploration"><a class="header" href="#appendix-advanced-concepts--further-exploration">Appendix: Advanced Concepts &amp; Further Exploration</a></h1>
<p>While the main chapters focused on the design, implementation, and evolution of Write-Ahead Logging across systems, many underlying mechanisms and optimizations deserve a deeper look.
This appendix offers short summaries of key topics-ranging from OS-level I/O internals to cutting-edge hardware technologies-that underpin or enhance WAL performance and reliability.</p>
<hr />
<h2 id="a-filesystem--os-internals"><a class="header" href="#a-filesystem--os-internals"><strong>A. Filesystem &amp; OS Internals</strong></a></h2>
<h3 id="1-fsync-and-durability-guarantees"><a class="header" href="#1-fsync-and-durability-guarantees"><strong>1. <code>fsync</code> and Durability Guarantees</strong></a></h3>
<p>A deep dive into how <code>fsync</code> ensures data persistence on disk, the subtleties of write ordering, and differences across filesystems and platforms. Understanding when data <em>really</em> hits the disk is crucial for reliable WAL design.</p>
<h3 id="2-zero-copy-io-mechanisms"><a class="header" href="#2-zero-copy-io-mechanisms"><strong>2. Zero-Copy I/O Mechanisms</strong></a></h3>
<p>Explores techniques like <code>sendfile</code>, <code>mmap</code>, and <code>splice</code> that allow data movement without CPU copying. These are often leveraged in high-performance log shipping or checkpoint operations.</p>
<h3 id="3-page-cache--writeback-internals"><a class="header" href="#3-page-cache--writeback-internals"><strong>3. Page Cache &amp; Writeback Internals</strong></a></h3>
<p>Explains how operating systems buffer WAL writes in memory before flushing to disk, including how dirty pages, writeback throttling, and flush daemons work behind the scenes.</p>
<h3 id="4-direct-io-vs-buffered-io"><a class="header" href="#4-direct-io-vs-buffered-io"><strong>4. Direct I/O vs Buffered I/O</strong></a></h3>
<p>Compares <code>O_DIRECT</code> access (used by some databases) to bypass kernel caching with traditional buffered I/O, and discusses when each approach offers performance or consistency advantages.</p>
<h3 id="5-filesystem-journaling-ext4-xfs-ntfs"><a class="header" href="#5-filesystem-journaling-ext4-xfs-ntfs"><strong>5. Filesystem Journaling (ext4, XFS, NTFS)</strong></a></h3>
<p>Unpacks how filesystem-level journaling interacts with WAL-sometimes duplicating effort-and how to tune or avoid double-write penalties.</p>
<hr />
<h2 id="b-hardware--storage"><a class="header" href="#b-hardware--storage"><strong>B. Hardware &amp; Storage</strong></a></h2>
<h3 id="6-nvme-ssds-and-write-amplification"><a class="header" href="#6-nvme-ssds-and-write-amplification"><strong>6. NVMe, SSDs, and Write Amplification</strong></a></h3>
<p>Describes the internal Flash Translation Layer (FTL) behavior, block erasure, and write amplification effects that influence WAL performance on solid-state drives.</p>
<h3 id="7-battery-backed-write-caches--nvram"><a class="header" href="#7-battery-backed-write-caches--nvram"><strong>7. Battery-backed Write Caches &amp; NVRAM</strong></a></h3>
<p>Shows how enterprise storage devices ensure data durability without immediate flushing, allowing WAL commits to complete faster.</p>
<h3 id="8-persistent-memory-pmem--wal-design"><a class="header" href="#8-persistent-memory-pmem--wal-design"><strong>8. Persistent Memory (PMEM) &amp; WAL Design</strong></a></h3>
<p>Introduces byte-addressable non-volatile memory and how it enables sub-microsecond logging paths-changing the role of WAL entirely.</p>
<h3 id="9-cpu-cache-coherency-and-memory-barriers"><a class="header" href="#9-cpu-cache-coherency-and-memory-barriers"><strong>9. CPU Cache Coherency and Memory Barriers</strong></a></h3>
<p>Explains when WAL writes are actually visible to other cores or hardware, how fences and cache flushes maintain correctness, and their performance cost.</p>
<hr />
<h2 id="c-concurrency-locks--consistency"><a class="header" href="#c-concurrency-locks--consistency"><strong>C. Concurrency, Locks &amp; Consistency</strong></a></h2>
<h3 id="10-spinlocks-mutexes-and-latches-in-wal-paths"><a class="header" href="#10-spinlocks-mutexes-and-latches-in-wal-paths"><strong>10. Spinlocks, Mutexes, and Latches in WAL Paths</strong></a></h3>
<p>Covers concurrency primitives used to protect shared log buffers or sequence numbers in high-performance systems.</p>
<h3 id="11-group-commit-mechanisms"><a class="header" href="#11-group-commit-mechanisms"><strong>11. Group Commit Mechanisms</strong></a></h3>
<p>Describes how databases batch multiple commits before a single flush, improving throughput while slightly increasing latency.</p>
<h3 id="12-atomic-writes--checksums"><a class="header" href="#12-atomic-writes--checksums"><strong>12. Atomic Writes &amp; Checksums</strong></a></h3>
<p>Explains how systems guard against torn writes by aligning WAL records to sector boundaries and using CRCs to detect corruption.</p>
<hr />
<h2 id="d-replication--distributed-systems"><a class="header" href="#d-replication--distributed-systems"><strong>D. Replication &amp; Distributed Systems</strong></a></h2>
<h3 id="13-wal-shipping--log-based-replication"><a class="header" href="#13-wal-shipping--log-based-replication"><strong>13. WAL Shipping &amp; Log-based Replication</strong></a></h3>
<p>Discusses streaming WAL to replicas, log shipping intervals, and how it forms the basis for asynchronous replication.</p>
<h3 id="14-raft-and-paxos-log-internals"><a class="header" href="#14-raft-and-paxos-log-internals"><strong>14. Raft and Paxos Log Internals</strong></a></h3>
<p>Connects WAL to distributed consensus-each Raft or Paxos log is essentially a replicated WAL ensuring consistent ordering across nodes.</p>
<h3 id="15-logical-vs-physical-replication"><a class="header" href="#15-logical-vs-physical-replication"><strong>15. Logical vs Physical Replication</strong></a></h3>
<p>Explores how systems parse and interpret WAL for logical replication or change data capture (CDC) pipelines.</p>
<hr />
<h2 id="e-instrumentation--visualization"><a class="header" href="#e-instrumentation--visualization"><strong>E. Instrumentation &amp; Visualization</strong></a></h2>
<h3 id="16-wal-performance-profiling-tools"><a class="header" href="#16-wal-performance-profiling-tools"><strong>16. WAL Performance Profiling Tools</strong></a></h3>
<p>Introduces tools like <code>perf</code>, <code>iostat</code>, <code>fio</code>, <code>blktrace</code>, and <code>strace</code> to measure I/O latency, syscall overheads, and disk flush patterns.</p>
<h3 id="17-visualizing-wal-io-latency"><a class="header" href="#17-visualizing-wal-io-latency"><strong>17. Visualizing WAL I/O Latency</strong></a></h3>
<p>Shows how to use flame graphs, latency histograms, and queue depth plots to identify performance bottlenecks in the write path.</p>
<h3 id="18-metrics-and-telemetry"><a class="header" href="#18-metrics-and-telemetry"><strong>18. Metrics and Telemetry</strong></a></h3>
<p>Outlines how to collect and monitor metrics such as LSN progression, flush frequency, and log size growth for observability and alerting.</p>
<hr />
<h2 id="f-database-specific-extensions"><a class="header" href="#f-database-specific-extensions"><strong>F. Database-specific Extensions</strong></a></h2>
<h3 id="19-innodb-doublewrite-buffer-deep-dive"><a class="header" href="#19-innodb-doublewrite-buffer-deep-dive"><strong>19. InnoDB Doublewrite Buffer Deep Dive</strong></a></h3>
<p>Explains MySQL’s safeguard against torn pages-how it writes data twice for safety and its interaction with WAL and fsync.</p>
<h3 id="20-postgresql-wal-archiving-internals"><a class="header" href="#20-postgresql-wal-archiving-internals"><strong>20. PostgreSQL WAL Archiving Internals</strong></a></h3>
<p>Covers how PostgreSQL archives and replays WAL segments for backup, point-in-time recovery, and streaming replication.</p>
<h3 id="21-rocksdb-logwriter--ratelimiter-behavior"><a class="header" href="#21-rocksdb-logwriter--ratelimiter-behavior"><strong>21. RocksDB LogWriter &amp; RateLimiter Behavior</strong></a></h3>
<p>Examines how RocksDB’s log writer handles durability and how the rate limiter throttles WAL I/O to prevent write stalls.</p>
<h3 id="22-sqlite-wal-index-file-format"><a class="header" href="#22-sqlite-wal-index-file-format"><strong>22. SQLite WAL Index File Format</strong></a></h3>
<p>Details SQLite’s unique shared-memory-based WAL index file that coordinates concurrent readers and writers efficiently.</p>
<hr />
<h2 id="g-experimental--emerging-topics"><a class="header" href="#g-experimental--emerging-topics"><strong>G. Experimental &amp; Emerging Topics</strong></a></h2>
<h3 id="23-zns-zoned-namespace-ssds-and-wal"><a class="header" href="#23-zns-zoned-namespace-ssds-and-wal"><strong>23. ZNS (Zoned Namespace) SSDs and WAL</strong></a></h3>
<p>Describes how modern zoned SSDs use append-only zones, perfectly suited for WAL-like sequential write patterns.</p>
<h3 id="24-log-structured-storage-without-wal"><a class="header" href="#24-log-structured-storage-without-wal"><strong>24. Log-Structured Storage without WAL</strong></a></h3>
<p>Explores how pure log-structured designs (like LSM trees) incorporate WAL concepts internally, sometimes eliminating separate logs altogether.</p>
<h3 id="25-async-io-apis-io_uring-aio"><a class="header" href="#25-async-io-apis-io_uring-aio"><strong>25. Async I/O APIs (io_uring, AIO)</strong></a></h3>
<p>Introduces modern kernel APIs that enable high-throughput, non-blocking WAL writes without dedicated writer threads.</p>
<h3 id="26-compression-and-encryption-in-wal"><a class="header" href="#26-compression-and-encryption-in-wal"><strong>26. Compression and Encryption in WAL</strong></a></h3>
<p>Outlines how systems secure and compress WAL segments, balancing performance with storage efficiency and confidentiality.</p>
<h3 id="27-simulating-crashes-and-recovery-testing"><a class="header" href="#27-simulating-crashes-and-recovery-testing"><strong>27. Simulating Crashes and Recovery Testing</strong></a></h3>
<p>Covers methodologies for fault injection and recovery validation to ensure WAL correctness under power failures or crashes.</p>
<hr />
<h3 id="closing-note"><a class="header" href="#closing-note"><strong>Closing Note</strong></a></h3>
<p>These topics form the deep substrate beneath modern database reliability. Exploring them not only clarifies why WAL works, but also illuminates how storage, hardware, and operating systems have evolved together to make durability both fast and reliable.</p>
<hr />
<div style="break-before: page; page-break-before: always;"></div><h2 id="authors-note"><a class="header" href="#authors-note">Author’s Note</a></h2>
<p>When I first started exploring system programming, it often felt like staring into a maze of complexity-dense documentation, scattered blog posts, and code that seemed to explain itself to no one. Over time, I realized that most of these ideas aren’t <em>impossible</em> but complex and they need to be a bit more beginner friendly.</p>
<p>This book grew out of my own journey to understand how real systems ensure reliability and durability through <strong>Write-Ahead Logging (WAL)</strong>. I’m still learning, every single day. Writing this book was as much a way for me to understand these concepts deeply as it was to share them with others who are on the same path.</p>
<p>I wanted to make something that feels practical and human-something that shows that even the “hard parts” of system design can be approachable if explained step by step.</p>
<p>And yes, this book has been <strong>polished with the help of Large Language Models (LLMs)</strong> - not to replace understanding, but to accelerate writing, refine explanations, and make the text clearer. The ideas, however, were all learned, struggled with, coded, and built from scratch.</p>
<p>If this book helped you <em>see under the hood</em> or sparked curiosity about how systems really work, I’d love to hear from you.
Let’s keep learning together and make system programming accessible to everyone who’s curious enough to look deeper.</p>
<p><strong>Connect with me:</strong></p>
<ul>
<li>
<p>Website:  <a href="https://sidy.me">sidy.me</a></p>
</li>
<li>
<p>LinkedIn:  <a href="https://linkedin.com/in/siddharthsingh89">linkedin.com/in/siddharthsingh89</a></p>
</li>
<li>
<p>Github:  <a href="https://github.com/siddharthsingh89">github.com/siddharthsingh89</a></p>
</li>
<li>
<p>Email:  <a href="mailto:Siddharth.Singh89@live.com">Siddharth.Singh89@live.com</a></p>
</li>
</ul>
<h3 id="how-you-can-contribute"><a class="header" href="#how-you-can-contribute">How You Can Contribute</a></h3>
<ul>
<li><strong>Rebuild the examples.</strong> Modify the code, experiment, and share what you find.</li>
<li><strong>Contribute or translate.</strong> Help expand this into an open, global resource.</li>
<li><strong>Teach others.</strong> Use these ideas in classrooms, workshops, or meetups.</li>
</ul>
<p>This is just the beginning. I plan to continue exploring other components of the modern databases, <strong>storage engines</strong>, <strong>OS internals</strong>, more <strong>distributed systems</strong>, and <strong>performance tuning</strong>-and make them understandable to everyone, not just experts.</p>
<p>Thank you for reading, experimenting, and joining me on this journey of learning-by-building.</p>
<blockquote>
</blockquote>
<p>Siddharth</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
